[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2025 Coriell Bioinformatics Internship",
    "section": "",
    "text": "Welcome!\nWe’re excited to have you as an intern and we hope that you’ll learn a lot with us this summer. We’ll use this book as a kind of reference manual for the things we think you should have a little background knowledge about before starting on an interesting project.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "data-best-practices.html",
    "href": "data-best-practices.html",
    "title": "‘Best’ practices for data projects",
    "section": "",
    "text": "Quick note (TODO)\nScience requires reproducibility, not only to ensure that your results are generalizable but also to make your life easier. One overlooked aspect of of learning data science is creating consistent and clear organization of your projects and learning how to keep your data safe and easily searchable.\nCitations incoming. Please note that much of this document does not contain original material. References are listed at the end of the chapter (I still need to get my .bib files in order to give proper attribution).",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#project-organization",
    "href": "data-best-practices.html#project-organization",
    "title": "1  Organizing projects",
    "section": "1.1 Project organization",
    "text": "1.1 Project organization\nScience requires reproducibility. not only to ensure that your results are generalizable but also to make your own life easier. One overlooked aspect of of learning data science is creating consistent and clear organization of your projects."
  },
  {
    "objectID": "data-best-practices.html#why-care-about-data-management",
    "href": "data-best-practices.html#why-care-about-data-management",
    "title": "‘Best’ practices for data projects",
    "section": "Why care about data management?",
    "text": "Why care about data management?\nComputing is now an essential part of research. This is outlined beautifully in the paper, “All biology is computational biology” by Florian Markowetz. Data is getting bigger and bigger and we need to be equipped with the tools for storing, manipulating, and communicating insights derived from it. However, most researchers are never taught good computational practices. Computational best practices are imperitive. Implementing best (or good enough) practices can improve reproducibility, ensure correctness, and increase efficiency.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#topics",
    "href": "data-best-practices.html#topics",
    "title": "1  Organizing projects",
    "section": "1.2 Topics",
    "text": "1.2 Topics\n\nData management\nProject organization\nTracking changes\nSoftware\nManuscripts"
  },
  {
    "objectID": "data-best-practices.html#save-and-lock-all-raw-data",
    "href": "data-best-practices.html#save-and-lock-all-raw-data",
    "title": "‘Best’ practices for data projects",
    "section": "Save and lock all raw data",
    "text": "Save and lock all raw data\nKeep raw data in its unedited form. This includes not making changes to filenames. In bioinformatics, it’s common to get data from a sequencing facility with incomprehensible filenames. Don’t fall victim to the temptation of changing these filenames! Instead, it’s much better to keep the filenames exactly how they were sent to you and simply create a spreadsheet that maps the files to their metadata. In the case of a sample mix-up, it’s much easier to make a change to a row in a spreadsheet then to track down all of the filenames that you changed and ensure they’re correctly modified.\nOnce you have your raw data, you don’t want the raw data to change in any way that is not documented by code. To ensure this, you can consider changing file permissions to make the file immutable (unchangable). Using bash, you can change file permissions with:\nchattr +i myfile.txt\nIf you’re using Excel for data analysis, lock the spreadsheet with the raw data and only make references to this sheet when performing calculations.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#backups",
    "href": "data-best-practices.html#backups",
    "title": "‘Best’ practices for data projects",
    "section": "Backups",
    "text": "Backups\n\nThere are two types of people, those who do backups and those who will do backups.\n\nThe following are NOT backup solutions:\n\nCopies of data on the same disk\nDropbox/Google Drive\nRAID arrays\n\nAll of these solutions mirror the data. Corruption or ransomware will propagate. For example, if you corrupt a file on your local computer and then push that change to DropBox then the file on DropBox is now also corrupted. I’m sure some of these cloud providers have version controlled files but it’s better to just avoid the problem entirely by keeping good backups.\n\nUse the 3-2-1 rule:\n\nKeep 3 copies of any important file: 1 primary and 2 backups.\nKeep the files on 2 different media types to protect against different types of hazards.\nStore 1 copy offsite (e.g., outside your home or business facility).\n\nA backup is only a backup if you can restore the files!",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#files",
    "href": "data-best-practices.html#files",
    "title": "1  Organizing projects",
    "section": "2.4 Files",
    "text": "2.4 Files\n\nStore data in standard, machine readable formats\n\nTXT, CSV, TSV, JSON, YAML, HDF5\nLarge text files should be compressed\n\nxz is better than gzip. We should all use xz\n\n\nEnsure filenames are computer friendly\n\nNo spaces\nUse only letters, numbers, and “-” “_” and “.” as delimiters\n\n\n# Bad\ndata for jozef.txt\n\n# Okay\ndata-for-jozef.txt\n\n# Best\n2023-11-09_repetitive-element-counts.txt\nQuiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone"
  },
  {
    "objectID": "data-best-practices.html#quiz",
    "href": "data-best-practices.html#quiz",
    "title": "1  Organizing projects",
    "section": "2.4 Quiz",
    "text": "2.4 Quiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone"
  },
  {
    "objectID": "data-best-practices.html#more-on-filenames",
    "href": "data-best-practices.html#more-on-filenames",
    "title": "‘Best’ practices for data projects",
    "section": "More on filenames",
    "text": "More on filenames\nIt cannot be stressed enough how important filenames can be for an analysis. To get the most out of your files and to avoid catastrpohic failures, you should stick to some basic principles for naming files. First, use consistent and unique identifiers across all files that you generate for an experiment. For example, if you’re conducting a study that has both RNA-seq and ATAC-seq data performed on the same subjects, don’t name the files from the RNA-seq experiment subject1.fq.fz and the files from the ATAC-seq experiment control_subject1.fq.gz if they refer to the same sample. For small projects, it’s fairly easy to create consistent and unique IDs for each subject. For large projects unique random IDs can be used.\nFor example, the following filenames would be bad:\nsubject1-control_at_96hr1.txt\ns1_ctl-at-4days_2.txt\ns2TRT4d1.txt\nsbj2_Treatment_4_Days_Replicate_2.txt\nThese are better. Why are they better? They are consistent. The delimiter is consistent between the words (“_“) and each of the words represents something meaningful about the sample. These filenames also do not contain any spaces and can easily be parsed automatically.\nsubject1_control_4days_rep1.txt\nsubject1_control_4days_rep2.txt\nsubject2_treatment_4days_rep1.txt\nsubject2_treatment_4days_rep2.txt\nFile naming best practices also apply to naming executable scripts. The name of the file should describe the function of the script. For example,\n01_align_with_STAR.sh\nis better than simply naming the file\n01_script.sh\nPro-tip\nOne easy way to create unique random IDs for a large project is to concatenate descriptions and take the SHA/MDA5 hashsum.\necho \"subject1_control_4days_rep1\" | sha256\n# 57f458a294542b2ed6ac14ca64d3c8e4599eed7a\n\necho \"subject1_control_4days_rep2\" | shasum\n# b6ea9d729e57cce68b37de390d56c542bc17dea6",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#create-analysis-freindly-data---tidy-data",
    "href": "data-best-practices.html#create-analysis-freindly-data---tidy-data",
    "title": "‘Best’ practices for data projects",
    "section": "Create analysis freindly data - tidy data",
    "text": "Create analysis freindly data - tidy data\nThe term tidy data was defined by Hadley Wickham to describe data which is amenable to downstream analysis. Most people are familiar with performing a quick and dirty data analysis in a program like Excel. You may have also used some of Excel’s fancy features for coloring cells, adding bold and underlines to text, and formatting cells with other decorations. All of this tends to just be extra fluff. If you format you data properly then it will be much easier to perform downstream analysis on and will not require the use of extra decorations. This is true even in Excel!\nTo conform to the requirements of being tidy, the data should follow some simple principles:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nDon’t cram two variables into one value. e.g. “male_treated”.\nDon’t embed units into data. e.g. “3.4kg”. Instead, put in column name e.g. “weight_kg” = 3.4\n\n\n\nOnce your data is in this format, it can easily be read into downstream programs like R, or parsed with command line text editing programs like sed.\nThe iris dataset in R provides a classic example of this format\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nPro-tip\ndata.frame like objects can be stored and retrieved efficiently using the Apache Arrow format instead of CSV files:\ndf &lt;- arrow::read_parquet(\"path/to/file.parquet\")\narrow::write_parquet(df, \"path/to/different_file.parquet\")\nsee R arrow for more details. There’s also the nanoparquet package which provides a light weight reader/writer for parquet files.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#untidy-data",
    "href": "data-best-practices.html#untidy-data",
    "title": "‘Best’ practices for data projects",
    "section": "Untidy(?) data",
    "text": "Untidy(?) data\nSome data formats are not amenable to the ‘tidy’ structure, i.e. they’re just not best represented as long tables. For example, large/sparse matrices, geo-spatial data, R objects, etc.the lesson here is to store data in the format that is most appropriate for the data. For example, don’t convert a matrix to a long format and save as a tsv file! Save it as an .rds file instead. Large matrices can also be efficiently stored as HDF5 files. Sparse matrices can be saved and accessed eficiently using the Matrix package in R. And if you are accessing the same data often, consider storing as a SQLite database and accessing with dbplyr or sqlalchemy in Python. The main point is don’t force data into a format that you’re familiar with only because you’re familiar with that format. This will often lead to large file sizes and inefficient performance.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#record-all-steps-used-to-generate-the-data",
    "href": "data-best-practices.html#record-all-steps-used-to-generate-the-data",
    "title": "‘Best’ practices for data projects",
    "section": "Record all steps used to generate the data",
    "text": "Record all steps used to generate the data\nAlways document all steps you used to generate the data that’s present in your projects. This can be as simple as a README with some comments and a wget command or as complex as a snakemake workflow. The point is, be sure you can track down the exact source of every file that you created or downloaded.\nFor example, a README documenting the creation of the files needed to generate a reference index might look like:\nTranscripts:\nwget http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/gencode.v38.transcripts.fa.gz\n\nPrimary Assembly:\nwget http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/GRCh38.primary_assembly.genome.fa.gz\n\nCreate concatenated transcripts + genome for salmon (i.e. gentrome):\ncat gencode.v38.transcripts.fa.gz GRCh38.primary_assembly.genome.fa.gz &gt; gentrome.fa.gz\n\nCreate decoys file for salmon:\ngrep \"&gt;\" &lt;(gunzip -c GRCh38.primary_assembly.genome.fa.gz) | cut -d \" \" -f 1 &gt; decoys.txt\nsed -i.bak -e 's/&gt;//g' decoys.txt\nFor more complicated steps include a script. e.g. creating a new genome index, subsetting BAM files, accessing data from NCBI, etc.\nPro-tip\nA simple way to build a data pipeline that is surprisingly robust is just to create scripts for each step and number them in the order that they should be executed.\n01_download.sh\n02_process.py\n03_makeFigures.R\nYou can also include a runner script that will execute all of the above. Or, for more consistent workflows, use a workflow manager like Nextflow, Snakemake, WDL, or good ole’ GNU Make",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#look-familiar",
    "href": "data-best-practices.html#look-familiar",
    "title": "‘Best’ practices for data projects",
    "section": "Look familiar?",
    "text": "Look familiar?",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#project-structure",
    "href": "data-best-practices.html#project-structure",
    "title": "‘Best’ practices for data projects",
    "section": "Project structure",
    "text": "Project structure\nOne of the most useful changes that you can make to your workflow is the create a consistent folder structure for all of your analyses and stick with it. Coming up with a consistent and generalizable structure can be challenging at first but some general guidelines are presented here and here\nFirst of all, when beginning a new project, you should have some way of naming your projects. One good way of naming projects is to each project a descriptive name and append the date the project was started. For example, brca_rnaseq_2023-11-09/ is better than rnaseq_data/. In six months when a collaborator wants their old BRCA data re-analyzed you’ll thank yourself for timestamping the project folder and giving it a descriptive name.\nMy personal structure for every project looks like:\nyyyymmdd_project-name/\n├── data\n├── doc\n├── README\n├── results\n│   ├── data-files\n│   ├── figures\n│   └── rds-files\n└── scripts\n\n\nPrefixing the project directory with the ISO date allows for easy sorting by date\na README text file is present at the top level of the directory with a short description about the project and any notes or updates\ndata/ should contain soft links to any raw data or the results of downloading data from an external source\ndoc/ contains metadata documents about the samples or other metadata information about the experiment\nresults/ contains only data generated within the project. It has sub-directories for figures/, data-files/ and rds-files/. If you have a longer or more complicated analysis then add sub-directories indicating which script generated the results.\nscripts/ contains all analysis scripts numbered in their order of execution. Synchronize the script names with the results they produce.\n\n\nA more complex example\nyyyymmdd_project-name/\n├── data\n│   ├── 00_fastq\n│   ├── 01_fastqc\n│   ├── 02_trimmed\n│   └── 03_quants\n├── delivered\n│   └── 2022-09-11_analysis-results.zip\n├── doc\n│   └── sample-metadata.txt\n├── project.Rproj\n├── README\n├── results\n│   ├── data-files\n│   │   ├── 04\n│   │   │   └── differential-expression.tsv.gz\n│   │   └── 05\n│   │       └── gsea-results.tsv.gz\n│   ├── figures\n│   │   ├── 04\n│   │   │   ├── ma-plots.png\n│   │   │   └── volcano-plots.png\n│   │   └── 05\n│   │       └── enrichment-plots.png\n│   └── rds-files\n│       └── 04\n│           └── SummarizedExperiment.rds\n└── scripts\n    ├── 01_fastqc.sh\n    ├── 02_trim.sh\n    ├── 03_quant.sh\n    ├── 04_differential_expression.Qmd\n    └── 05_gsea.Qmd\nPro-tip\nIf using Rstudio, include an .Rproj file at the top level of your directory. Doing this enables you to use the here package to reference data within your project in a relative fashion. For example, you can more easily save data with:\nplot_volcano(...)\nggsave(here(\"data\", \"results\", \"figures\", \"04\", \"volcano-plots.png\"))",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#project-structure-1",
    "href": "data-best-practices.html#project-structure-1",
    "title": "1  Some data best practices",
    "section": "3.3 Project structure",
    "text": "3.3 Project structure\nA fuller example might look more like:\nyyyymmdd_project-name/\n├── data\n│   ├── 00_fastq\n│   ├── 01_fastqc\n│   ├── 02_trimmed\n│   └── 03_quants\n├── delivered\n│   └── 2022-09-11_analysis-results.zip\n├── doc\n│   └── sample-metadata.txt\n├── project.Rproj\n├── README\n├── results\n│   ├── data-files\n│   │   ├── 04\n│   │   │   └── differential-expression.tsv.gz\n│   │   └── 05\n│   │       └── gsea-results.tsv.gz\n│   ├── figures\n│   │   ├── 04\n│   │   │   ├── ma-plots.png\n│   │   │   └── volcano-plots.png\n│   │   └── 05\n│   │       └── enrichment-plots.png\n│   └── rds-files\n│       └── 04\n│           └── SummarizedExperiment.rds\n└── scripts\n    ├── 01_fastqc.sh\n    ├── 02_trim.sh\n    ├── 03_quant.sh\n    ├── 04_differential_expression.Qmd\n    └── 05_gsea.Qmd\nPro-tip\nIf using Rstudio, include an .Rproj file at the top level of your directory. Doing this enables you to use the here package to reference data within your project in a relative fashion. For example, you can more easily save data with:\nplot_volcano(...)\nggsave(here(\"data\", \"results\", \"figures\", \"04\", \"volcano-plots.png\"))"
  },
  {
    "objectID": "data-best-practices.html#manual-version-control",
    "href": "data-best-practices.html#manual-version-control",
    "title": "‘Best’ practices for data projects",
    "section": "Manual version control",
    "text": "Manual version control\nVersion control refers to the practice of tracking changes in files and data over their lifetime. You should always track any changes made to your project over the entire life of the project. This can be done either manually or using a dedicated version control system. If doing this manually, add a file called “CHANGELOG.md” in your docs/ directory and add detailed notes in reverse chronological order.\nFor example:\n## 2016-04-08\n\n* Switched to cubic interpolation as default.\n* Moved question about family's TB history to end of questionnaire.\n\n## 2016-04-06\n\n* Added option for cubic interpolation.\n* Removed question about staph exposure (can be inferred from blood test results).\n\nIf you make a significant change to the project, copy the whole directory, date it, and store it such that it will no longer be modified. Copies of these old projects can be compressed and saved with tar + xz compression\ntar -cJvf old.20231109_myproject.tar.xz myproject/`",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#version-control-with-git",
    "href": "data-best-practices.html#version-control-with-git",
    "title": "‘Best’ practices for data projects",
    "section": "Version control with git",
    "text": "Version control with git\n\ngit is probably the de facto version control system in use today for tracking changes across software projects. You should strive to learn and use git to track your projects. Version control systems allow you to track all changes, comment on why changes were made, create parallel branches, and merge existing ones.\ngit is primarily used for source code files. Microsoft Office files and PDFs can be stored with Github but it’s hard to track changes. Rely on Microsoft’s “Track Changes” instead and save frequently.\nIt’s not necessary to version control raw data (back it up!) since it shouldn’t change. Likewise, backup intermediate data and version control the scripts that made it.\nFor a quick primer on Git and GitHub check out the book Happy Git with R or The Official GitHub Training Manual Anyone in the lab can join the coriell-research organization on Github and start tracking their code\nBe careful committing sensitive information to GitHub",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts",
    "title": "‘Best’ practices for data projects",
    "section": "Quick tips to improve your scripts",
    "text": "Quick tips to improve your scripts\n\nPlace a description at the top of every script\nThe description should indicate who the author is. When the code was created. A short description of what the expected inputs and outputs are along with how to use the code. You three months from now will appreciate it when you need to revisit your analysis\nFor example:\n#!/usr/bin/env python3\n# Gennaro Calendo\n# 2023-11-09\n# \n# This scripts performs background correction of all images in the \n#  user supplied directory\n#\n# Usage ./correct-bg.py --input images/ --out_dir out_directory\n#\nfrom image_correction import background_correct\n\nfor img in images:\n  img = background_correct(img)\n  save_image(img, \"out_directory/corrected.png\")\n  \n\n\nDecompose programs into functions\nFunctions make it easier to reason about your code, spot errors, and make changes. This also follows the Don’t Repeat Yourself principle aimed at reducing repetition by replacing it with abstractions that are more stable\nCompare this chunk of code that rescales values using a min-max function (0-1)\ndf &lt;- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\nto this function which does the same thing\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf &lt;- lapply(df, rescale01)\nWhich is easier to read? Which is easier to debug? Which is more efficient?\n\n\nGive functions and variables meaningful names\n\nPrograms are written for people and then computers\nUse variable and function names that are meaningful and correct\nKeep names consistent. Use either snake_case or camelCase but try not to mix both\n\nBad:\nlol &lt;- 1:100\n\nmydata &lt;- data.frame(x = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nf &lt;- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\nBetter:\nages &lt;- 1:100\n\nbioinfo_names &lt;- data.frame(Name = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nplotScatterPlot &lt;- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\n\n\nDo not control program flow with comments\n\nthis is error prone and makes it difficult or impossible to automate\nUse if/else statements instead\n\nBad:\n# Download the file\n#filename &lt;- \"data.tsv\"\n#url &lt;- \"http:://example.com/data.tsv\"\n#download.file(url, filename)\n\n# Read in to a data.frame\ndf &lt;- read.delim(\"data.tsv\", sep=\"\\t\")\nGood:\nfilename &lt;- \"data.tsv\"\nurl &lt;- \"http:://example.com/data.tsv\"\n\nif (!file.exists(filename)) {\n  download.file(url, filename)\n}\ndf &lt;- read.delim(filename)\n\n\nUse a consistent style\nPick a style guide and stick with it. If using R, the styler package can automatically clean up poorly formatted code. If using Python, black is a highly opinionated formatter that is pretty popular. Although, I think ruff is currently all the rage with the Pythonistas these days.\nBad:\nflights|&gt;group_by(dest)|&gt; summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |&gt; \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-&gt; flight_plot\nGood:\nflight_plot &lt;- flights |&gt; \n  group_by(dest) |&gt; \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |&gt; \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()\n\n\nDon’t use right hand assignment\nThis is R specific. I’ve seen this pop up with folks who are strong tidyverse adherents. I get it, that’s the direction of the piping operator. However, this right-hand assignment flies in the face of basically every other programming language, and since code is primarily read rather than executed, it’s much harder to scan a codebase and understand the variable assignment when the assignments can be anywhere in the pipe!\nDon’t do this\ndata |&gt; \n  select(...) |&gt; \n  filter(...) |&gt; \n  group_by(...) |&gt; \n  summarize(...) -&gt; by_group\nIt’s much easier to look down a script and see that by_group is created by all of the piped operations when assigned normally.\nby_group &lt;- data |&gt; \n  select(...) |&gt; \n  filter(...) |&gt; \n  group_by(...) |&gt; \n  summarize(...)",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-1",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-1",
    "title": "1  Some data best practices",
    "section": "5.2 Quick tips to improve your scripts",
    "text": "5.2 Quick tips to improve your scripts\n\n5.2.1 Give functions and variables meaningful names\n\nPrograms are written for people and then computers\nUse variable and function names that are meaningful and correct\nKeep names consistent. Use either snake_case or camelCase but try not to mix both\n\nBad:\nlol <- 1:100\n\nmydata <- data.frame(x = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nf <- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\nBetter:\nages <- 1:100\n\nbioinfo_names <- data.frame(Name = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nplotScatterPlot <- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-2",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-2",
    "title": "1  Some data best practices",
    "section": "5.3 Quick tips to improve your scripts",
    "text": "5.3 Quick tips to improve your scripts\n\n5.3.1 Do not control program flow with comments\n\nthis is error prone and makes it difficult or impossible to automate\nUse if/else statements instead\n\nBad:\n# Download the file\n#filename <- \"data.tsv\"\n#url <- \"http:://example.com/data.tsv\"\n#download.file(url, filename)\n\n# Read in to a data.frame\ndf <- read.delim(\"data.tsv\", sep=\"\\t\")\nGood:\nfilename <- \"data.tsv\"\nurl <- \"http:://example.com/data.tsv\"\n\nif (!file.exists(filename)) {\n  download.file(url, filename)\n}\ndf <- read.delim(filename)"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-3",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-3",
    "title": "1  Some data best practices",
    "section": "5.4 Quick tips to improve your scripts",
    "text": "5.4 Quick tips to improve your scripts\n\n5.4.1 Use a consistent style\n\nPick a style guide and stick with it\nIf using R, the styler package can automatically cleanup poorly formatted code\nIf using Python, black is a highly opinionated formatter\nDon’t use right hand assignment\n\nBad:\nflights|>group_by(dest)|> summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |> \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-> flight_plot\nGood:\nflight_plot <- flights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-4",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-4",
    "title": "1  Some data best practices",
    "section": "5.5 Quick tips to improve your scripts",
    "text": "5.5 Quick tips to improve your scripts\n\n5.5.1 Use a consistent style\n\nPick a style guide and stick with it\nIf using R, the styler package can automatically cleanup poorly formatted code\nIf using Python, black is a highly opinionated formatter\nDon’t use right hand assignment\n\nBad:\nflights|>group_by(dest)|> summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |> \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-> flight_plot\nGood:\nflight_plot <- flights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()"
  },
  {
    "objectID": "data-best-practices.html#a-typical-workflow",
    "href": "data-best-practices.html#a-typical-workflow",
    "title": "1  Some data best practices",
    "section": "6.1 A typical workflow",
    "text": "6.1 A typical workflow\n\nA common practice in academic writing is for the lead author to send successive versions of a manuscript to coauthors to collect feedback, which is returned as changes to the document, comments on the document, plain text in email, or a mix of all 3. This allows coauthors to use familiar tools but results in a lot of files to keep track of and a lot of tedious manual labor to merge comments to create the next master version.\n\nInstead of an email based workflow, we should aim to mirror best practices developed for data management and software development\nWe want to:\n\nEnsure text is accessible to yourself and others now and in the future by making a single accessible master document\nMake it easy to track and combine changes from multiple authors\nAvoid duplication and manual entry of references, figures, tables, etc.\nMake it easy to regenerate the final, most updated version and share with collaborators and submit\nMore important than the type of workflow is getting everyone to agree on the workflow chosen and how changes will be tracked"
  },
  {
    "objectID": "data-best-practices.html#workflow-1-single-online-master-document",
    "href": "data-best-practices.html#workflow-1-single-online-master-document",
    "title": "1  Some data best practices",
    "section": "6.2 Workflow 1: single online master document",
    "text": "6.2 Workflow 1: single online master document\n\nKeep a single online master document that all authors can collaborate on\nThe tool should be able to track changes, automatically manage references, track version changes.\nTwo candidates are Google Docs and Microsoft Office online\n\nHere’s a good blog post about using Google Docs for scientific writing\n\nOf course, some collaborators will insist on using familiar desktop based tools which will require saving these files in a versioned doc/ directory and manually merging the changes\n\nPandoc can be useful for these kinds of document merging"
  },
  {
    "objectID": "data-best-practices.html#workflow-2-text-documents-under-version-control",
    "href": "data-best-practices.html#workflow-2-text-documents-under-version-control",
    "title": "1  Some data best practices",
    "section": "6.3 Workflow 2: Text documents under version control",
    "text": "6.3 Workflow 2: Text documents under version control\n\nWrite papers in a text based format like \\(\\LaTeX\\) or markdown managing references in a .bib file and tracking changes via git\nThen convert these plain text documents to a pdf or Word doc for final submission with Pandoc\nMathematics, physics, and astronomy have been doing it this way for decades\nThis of course requires everyone to learn a typsetting language and be able to use version control tools…\n\nWhat is the difference between plain text and word processing?\nText editors?\nWhat does it mean to compile a document?\nBIBTex?\nGit/ Github?\n\n\nthis is all probably a bit too much"
  },
  {
    "objectID": "data-best-practices.html#summary-1",
    "href": "data-best-practices.html#summary-1",
    "title": "1  Some data best practices",
    "section": "7.1 Summary",
    "text": "7.1 Summary\n\nData management\n\nSave all raw data and don’t modify it\nKeep good backups and make sure they work. 3-2-1 rule\nUse consistent, meaningful filenames that make sense to computers and reflect their content or function\nCreate analysis friendly data\nWork with/ save data in the format that it is best suited to\n\nProject Organization\n\nUse a consistent, well-defined project structure across all projects\nGive each project a consistent and meaningful name\nUse the structure of the project to organize where files go\n\nTracking Changes\n\nKeep changes small and save changes frequently\nIf manually tracking changes do so in a logical location in a plain text document\nUse a version control system\n\nSoftware\n\nWrite a short description at the top of every script about what the script does and how to use it\nDecompose programs into functions. Don’t Repeat Yourself\nGive functions and variables meaningful names\nUse statements for control flow instead of comments\nUse a consistent style of coding. Use a code styler\n\nManuscripts\n\nPick a workflow that everyone agrees on and keep a single, active collaborative document using either Microsoft Online or Google Docs"
  },
  {
    "objectID": "data-best-practices.html#resources-1",
    "href": "data-best-practices.html#resources-1",
    "title": "1  Some data best practices",
    "section": "8.1 Resources",
    "text": "8.1 Resources\n\n8.1.1 Best practices\n\nGood Enough Practices in Scientific Computing: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510#sec004\nBackups: https://www.cisa.gov/sites/default/files/publications/data_backup_options.pdf\nFile naming: https://library.si.edu/sites/default/files/filenamingorganizing_20210609.pdf\nTidy data: https://www.jstatsoft.org/article/view/v059i10\nFrank Harrell’s R Workflow for Reproducible Analysis: https://hbiostat.org/rflow/\n\nFH blog is also really good for statistical concepts: https://www.fharrell.com/\n\n\n\n\n8.1.2 Bioinformatics\n\nBioinformatics Data Skills: https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/\nBioStars for bioinfo questions: https://www.biostars.org/\nBioconductor common workflows: https://bioconductor.org/packages/release/BiocViews.html#___Workflow\n\n\n\n8.1.3 Proficiency with computational tools\n\nMIT Missing Semester: https://missing.csail.mit.edu/\n\nReally, check this one out\n\n\n\n\n8.1.4 R\n\nR for Data Science: https://r4ds.hadley.nz/\nAdvanced R: https://adv-r.hadley.nz/\nfasteR (base): https://github.com/matloff/fasteR\nEfficient R Programming: https://bookdown.org/csgillespie/efficientR/\nR performance tips: https://peerj.com/preprints/26605.pdf\nR Inferno: https://www.burns-stat.com/documents/books/the-r-inferno/\nIntroduction to data science: https://rafalab.github.io/dsbook-part-1/\nAdvanced data science: https://rafalab.github.io/dsbook-part-2/"
  },
  {
    "objectID": "r-basics.html#introduction-to-programming-in-r",
    "href": "r-basics.html#introduction-to-programming-in-r",
    "title": "2  R bootcamp",
    "section": "2.1 Introduction to programming in R",
    "text": "2.1 Introduction to programming in R\nThere are tons of great resources for learning R. R for Data Science is probably the most popular resource for new useRs to get up to speed with slicing and dicing data in R. The R for Data Science book, however, is taught from the perspective of the Tidyverse. The Tidyverse is an opinionated set of packages and functions that help users perform data manipulations primarily on data.frames. While these packages and functions can be great for experienced users by providing ergonomic and consistent interfaces for data.frame manipulation, it is my personal belief that new users should first learn the base language, especially if their goal is to perform bioinformatics analysis.\nBioinformatics tools rely heavily on subsetting and matrix manipulations. In my experience, users who start learning R using only function from the Tidyverse have a difficult time understanding matrix manipulations and subsetting operations common in bioinformatics workflows. This becomes especially important when using SummarizedExperiments - the backbone of many bioinformatics data structures in R.\nFor this reason, we’re going to focus on learning R from the ground up using functions that exist primarily in the base language. A great resource for learning base R quickly is Norm Matloff’s fasteR which can be found here."
  },
  {
    "objectID": "data-best-practices.html#large-files",
    "href": "data-best-practices.html#large-files",
    "title": "‘Best’ practices for data projects",
    "section": "Large files",
    "text": "Large files\nYou’ll probably be dealing with files on the order of 10s of GBs. You do not want to be copying these files from one place to another. This increases confusion and runs the risk of introducing errors. Instead avoid making copies of large local files or persistent databases and simply link to the files.\nYou can use use soft links. A powerful way of finding an linking files can be done with find\n# Link all fastq files to a local directory\nfind /path/to/fq/files -name \"*.fq.gz\") -exec ln -s {} . \\;\nIf using R, you can also sometimes specify a URL in place of a file path for certain functions.\n# Avoid downloading a large GTF file - reads GTF directly into memory\nurl &lt;- \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/gencode.v44.annotation.gtf.gz\"\ngtf &lt;- rtracklayer::import(url)",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#file-types-and-file-names",
    "href": "data-best-practices.html#file-types-and-file-names",
    "title": "‘Best’ practices for data projects",
    "section": "File types and file names",
    "text": "File types and file names\nAs a data scientist you’ll be dealing with a lot of files but have you ever considered what a file is? Files come in all shapes and formats. Some are very application specific and require specialized programs to open. For example, consider DICOM files that are used to store and manipulate radiology data. Luckily, in bioinformatics we tend to deal mainly with simple, plain text files, most often. Plain text files are typically designed to be both human and machine-readable. If you have the choice of saving any data, you should know that some formats will make your life easier. Certain file formats like TXT, CSV, TSV, JSON, and YAML are standard plain text file formats that are easy to share and easy to open and manipulate. Because of this, you should prefer to store your data in machine-readable formats. Avoid .xlsx files for storing data. Prefer TXT, CSV, TSV, JSON, YAML, and HDF5.\nIf you have very large text files then you can use compression utilities to save space. Most bioinformatics software is designed to work well with gzip compressed data. gzip is a relatively old compression format. You could also consider using xz as a means to compress your data - just know that xz compression is less supported across tools.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#file-naming",
    "href": "data-best-practices.html#file-naming",
    "title": "‘Best’ practices for data projects",
    "section": "File naming",
    "text": "File naming\nFile naming is important but often overlooked. You want your files to be named logically and communicate their contents. You also want your files to be named in a way that a computer can easily read. For example, spaces in filenames are a royal pain when manipulating files on the command line.\nTo ensure filenames are computer friendly, don’t use spaces in filenames. Use only letters, numbers, and “-” “_” and “.” as delimiters. For example:\n# Bad\ndata for jozef.txt\n\n# Okay\ndata-for-jozef.txt\n\n# Best\n2023-11-09_repetitive-element-counts.txt\nQuiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#best-practices",
    "href": "data-best-practices.html#best-practices",
    "title": "‘Best’ practices for data projects",
    "section": "Best practices",
    "text": "Best practices\n\nGood Enough Practices in Scientific Computing: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510#sec004\nBackups: https://www.cisa.gov/sites/default/files/publications/data_backup_options.pdf\nFile naming: https://library.si.edu/sites/default/files/filenamingorganizing_20210609.pdf\nTidy data: https://www.jstatsoft.org/article/view/v059i10\nFrank Harrell’s R Workflow for Reproducible Analysis: https://hbiostat.org/rflow/\n\nFH blog is also really good for statistical concepts: https://www.fharrell.com/",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#bioinformatics",
    "href": "data-best-practices.html#bioinformatics",
    "title": "‘Best’ practices for data projects",
    "section": "Bioinformatics",
    "text": "Bioinformatics\n\nBioinformatics Data Skills: https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/\nBioStars for bioinfo questions: https://www.biostars.org/\nBioconductor common workflows: https://bioconductor.org/packages/release/BiocViews.html#___Workflow",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#proficiency-with-computational-tools",
    "href": "data-best-practices.html#proficiency-with-computational-tools",
    "title": "‘Best’ practices for data projects",
    "section": "Proficiency with computational tools",
    "text": "Proficiency with computational tools\n\nMIT Missing Semester: https://missing.csail.mit.edu/\n\nReally, check this one out",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#r",
    "href": "data-best-practices.html#r",
    "title": "‘Best’ practices for data projects",
    "section": "R",
    "text": "R\n\nR for Data Science: https://r4ds.hadley.nz/\nAdvanced R: https://adv-r.hadley.nz/\nfasteR (base): https://github.com/matloff/fasteR\nEfficient R Programming: https://bookdown.org/csgillespie/efficientR/\nR performance tips: https://peerj.com/preprints/26605.pdf\nR Inferno: https://www.burns-stat.com/documents/books/the-r-inferno/\nIntroduction to data science: https://rafalab.github.io/dsbook-part-1/\nAdvanced data science: https://rafalab.github.io/dsbook-part-2/",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#quick-note-todo",
    "href": "data-best-practices.html#quick-note-todo",
    "title": "1  Some data best practices",
    "section": "1.1 Quick note (TODO)",
    "text": "1.1 Quick note (TODO)\nCitations incoming. Please note that much of this document does not contain original material. References are listed at the end of the chapter (I still need to get my .bib files in order to give proper attribution)."
  },
  {
    "objectID": "r-basics.html#subsetting-in-r",
    "href": "r-basics.html#subsetting-in-r",
    "title": "R programming basics",
    "section": "Subsetting in R",
    "text": "Subsetting in R\nYou may have only ever encountered R from the perspective of the tidyverse. tidyverse functions provide useful abstractions for munging tidy data however, most genomics data is often best represented and operated on as matrices. Keeping your data in matrix format can provide many benefits as far as speed and code clarity, which in turn helps to ensure correctness. You can think of matrices as just fancy 2D versions of vectors. So what are vectors?\nVectors are the main building blocks of most R analyses. Whenever you use the c() function, like: x &lt;- c('a', 'b', 'c') you’re creating a vector. You can do all kinds of cool things with vectors which will prove useful when working with SummarizedExperiments.\nNOTE: the following is heavily inspired by Norm Matloff’s excellent fasteR tutorial. Take a look there to get a brief and concise overview base R. You should also check out the first few chapters of Hadley Wickham’s amazing book Advanced R. The first edition contains some more information on base R.\n\nSubsetting vectors\nBelow, we’ll use the built-in R constant called LETTERS. The LETTERS vector is simply a ‘list’ of all uppercase letters in the Roman alphabet.\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\nWe can subset the vector by position. For example, to get the 3rd letter we use the [ operator and the position we want to extract.\n\nLETTERS[3]\n\n[1] \"C\"\n\n\nWe can also use a range of positions.\n\nLETTERS[3:7]\n\n[1] \"C\" \"D\" \"E\" \"F\" \"G\"\n\n\nWe don’t have to select sequential elements either. We can extract elements by using another vector of positions.\n\nLETTERS[c(7, 5, 14, 14, 1, 18, 15)]\n\n[1] \"G\" \"E\" \"N\" \"N\" \"A\" \"R\" \"O\"\n\n\nVectors become really powerful when we start combining them with logical operations.\n\nmy_favorite_letters &lt;- c(\"A\", \"B\", \"C\")\n\n# See that this produces a logical vector of (TRUE/FALSE) values\n# TRUE when LETTERS is one of my_favorite_letters and FALSE otherwise\nLETTERS %in% my_favorite_letters\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE\n\n# We can use that same expression to filter the vector\nLETTERS[LETTERS %in% my_favorite_letters]\n\n[1] \"A\" \"B\" \"C\"\n\n\nThis same kind of subsetting works on vectors that contain numeric data as well. For example, we can filter the measurements of annual flow of water through the Nile river like so:\nNile is another built-in dataset\n\n# Any values strictly greater than 1200\nNile[Nile &gt; 1200]\n\n[1] 1210 1230 1370 1210 1250 1260 1220\n\n# Any even number\nNile[Nile %% 2 == 0]\n\n [1] 1120 1160 1210 1160 1160 1230 1370 1140 1110  994 1020  960 1180  958 1140\n[16] 1100 1210 1150 1250 1260 1220 1030 1100  774  840  874  694  940  916  692\n[31] 1020 1050  726  456  824  702 1120 1100  832  764  768  864  862  698  744\n[46]  796 1040  944  984  822 1010  676  846  812  742 1040  860  874  848  890\n[61]  744  838 1050  918  986 1020  906 1170  912  746  718  714  740\n\n\n\n\nSubsetting data.frames\nBut these are just one dimensional vectors. In R we usually deal with data.frames (tibbles for you tidyverse) and matrices. Lucky for us, the subsetting operations we learned for vectors work the same way for data.frames and matrices.\nLet’s take a look at the built-in ToothGrowth dataset. The data consists of the length of odontoblasts in 60 guinea pigs receiving one of three levels of vitamin C by one of two delivery methods.\n\nhead(ToothGrowth)\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5\n\n\nThe dollar sign $ is used to extract an individual column from the data.frame, which is just a vector.\n\nhead(ToothGrowth$len)\n\n[1]  4.2 11.5  7.3  5.8  6.4 10.0\n\n\nWe can also use the [[ to get the same thing. Double-brackets come in handy when your columns are not valid R names since $ only works when columns are valid names\n\nhead(ToothGrowth[[\"len\"]])\n\n[1]  4.2 11.5  7.3  5.8  6.4 10.0\n\n\nWhen subsetting a data.frame in base R the general scheme is:\ndf[the rows you want, the columns you want]\nSo in order to get the 5th row of the first column we could do:\n\nToothGrowth[5, 1]\n\n[1] 6.4\n\n\nAgain, we can combine this kind of thinking to extract rows and columns matching logical conditions. For example, if we want to get all of the animals administered orange juice (‘OJ’)\n\nToothGrowth[ToothGrowth$supp == \"OJ\", ]\n\n    len supp dose\n31 15.2   OJ  0.5\n32 21.5   OJ  0.5\n33 17.6   OJ  0.5\n34  9.7   OJ  0.5\n35 14.5   OJ  0.5\n36 10.0   OJ  0.5\n37  8.2   OJ  0.5\n38  9.4   OJ  0.5\n39 16.5   OJ  0.5\n40  9.7   OJ  0.5\n41 19.7   OJ  1.0\n42 23.3   OJ  1.0\n43 23.6   OJ  1.0\n44 26.4   OJ  1.0\n45 20.0   OJ  1.0\n46 25.2   OJ  1.0\n47 25.8   OJ  1.0\n48 21.2   OJ  1.0\n49 14.5   OJ  1.0\n50 27.3   OJ  1.0\n51 25.5   OJ  2.0\n52 26.4   OJ  2.0\n53 22.4   OJ  2.0\n54 24.5   OJ  2.0\n55 24.8   OJ  2.0\n56 30.9   OJ  2.0\n57 26.4   OJ  2.0\n58 27.3   OJ  2.0\n59 29.4   OJ  2.0\n60 23.0   OJ  2.0\n\n\nWe can also combine logical statements. For example, to get all of the rows for animals administered orange juice and with odontoblast length (‘len’) less than 10.\n\nToothGrowth[ToothGrowth$supp == \"OJ\" & ToothGrowth$len &lt; 10, ]\n\n   len supp dose\n34 9.7   OJ  0.5\n37 8.2   OJ  0.5\n38 9.4   OJ  0.5\n40 9.7   OJ  0.5\n\n# We can also use the bracket notation to select rows and columns at the same time\nToothGrowth[ToothGrowth$supp == \"OJ\" & ToothGrowth$len &lt; 10, c(\"len\", \"supp\")]\n\n   len supp\n34 9.7   OJ\n37 8.2   OJ\n38 9.4   OJ\n40 9.7   OJ\n\n\nIt gets annoying typing ToothGrowth every time we want to subset the data.frame. Base R has a very useful function called subset() that can help us type less. subset() essentially ‘looks inside’ the data.frame that you give it for the given columns and evaluates the expression without having to explicitly tell R where to find the columns. Think of it like dplyr::filter().\n\nsubset(ToothGrowth, supp == \"OJ\" & len &lt; 10)\n\n   len supp dose\n34 9.7   OJ  0.5\n37 8.2   OJ  0.5\n38 9.4   OJ  0.5\n40 9.7   OJ  0.5\n\n\n\n\nSubsetting matrices\nMatrices behave much like data.frames but unlike data.frames matrices can only contain one type of data. This might sound like a limitation at first but you’ll soon come to realize that matrices are very powerful (and fast) to work with in R.\n\nset.seed(123)\n\n# Create some random data that looks like methylation values\n(m &lt;- matrix(\n  data = runif(6 * 10),\n  ncol = 6,\n  dimnames = list(\n    paste0(\"CpG.\", 1:10),\n    paste0(\"Sample\", 1:6)\n  )\n))\n\n         Sample1    Sample2   Sample3    Sample4   Sample5    Sample6\nCpG.1  0.2875775 0.95683335 0.8895393 0.96302423 0.1428000 0.04583117\nCpG.2  0.7883051 0.45333416 0.6928034 0.90229905 0.4145463 0.44220007\nCpG.3  0.4089769 0.67757064 0.6405068 0.69070528 0.4137243 0.79892485\nCpG.4  0.8830174 0.57263340 0.9942698 0.79546742 0.3688455 0.12189926\nCpG.5  0.9404673 0.10292468 0.6557058 0.02461368 0.1524447 0.56094798\nCpG.6  0.0455565 0.89982497 0.7085305 0.47779597 0.1388061 0.20653139\nCpG.7  0.5281055 0.24608773 0.5440660 0.75845954 0.2330341 0.12753165\nCpG.8  0.8924190 0.04205953 0.5941420 0.21640794 0.4659625 0.75330786\nCpG.9  0.5514350 0.32792072 0.2891597 0.31818101 0.2659726 0.89504536\nCpG.10 0.4566147 0.95450365 0.1471136 0.23162579 0.8578277 0.37446278\n\n\nIf we want to extract the value for CpG.3 for Sample3\n\nm[3, 3]\n\n[1] 0.6405068\n\n\nOr all values of CpG.3 for every sample\n\nm[3, ]\n\n  Sample1   Sample2   Sample3   Sample4   Sample5   Sample6 \n0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248 \n\n# Or refer to the row by it's name\nm[\"CpG.3\", ]\n\n  Sample1   Sample2   Sample3   Sample4   Sample5   Sample6 \n0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248 \n\n\nOr all CpGs for Sample3\n\nm[, 3]\n\n    CpG.1     CpG.2     CpG.3     CpG.4     CpG.5     CpG.6     CpG.7     CpG.8 \n0.8895393 0.6928034 0.6405068 0.9942698 0.6557058 0.7085305 0.5440660 0.5941420 \n    CpG.9    CpG.10 \n0.2891597 0.1471136 \n\n# Or refer to the column by it's name\nm[, \"Sample3\"]\n\n    CpG.1     CpG.2     CpG.3     CpG.4     CpG.5     CpG.6     CpG.7     CpG.8 \n0.8895393 0.6928034 0.6405068 0.9942698 0.6557058 0.7085305 0.5440660 0.5941420 \n    CpG.9    CpG.10 \n0.2891597 0.1471136 \n\n\nWe can also apply a mask to the entire matrix at once. For example, the following will mark any value that is greater than 0.5 with TRUE\n\nm &gt; 0.5\n\n       Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nCpG.1    FALSE    TRUE    TRUE    TRUE   FALSE   FALSE\nCpG.2     TRUE   FALSE    TRUE    TRUE   FALSE   FALSE\nCpG.3    FALSE    TRUE    TRUE    TRUE   FALSE    TRUE\nCpG.4     TRUE    TRUE    TRUE    TRUE   FALSE   FALSE\nCpG.5     TRUE   FALSE    TRUE   FALSE   FALSE    TRUE\nCpG.6    FALSE    TRUE    TRUE   FALSE   FALSE   FALSE\nCpG.7     TRUE   FALSE    TRUE    TRUE   FALSE   FALSE\nCpG.8     TRUE   FALSE    TRUE   FALSE   FALSE    TRUE\nCpG.9     TRUE   FALSE   FALSE   FALSE   FALSE    TRUE\nCpG.10   FALSE    TRUE   FALSE   FALSE    TRUE   FALSE\n\n\nWe can use this kind of masking to filter rows of the matrix using some very helpful base R functions that operate on matrices. For example, to get only those CpGs where 3 or more samples have a value &gt; 0.5 we can use the rowSums() like so:\n\nm[rowSums(m &gt; 0.5) &gt; 3, ]\n\n        Sample1   Sample2   Sample3   Sample4   Sample5   Sample6\nCpG.3 0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248\nCpG.4 0.8830174 0.5726334 0.9942698 0.7954674 0.3688455 0.1218993\n\n\nThis pattern is very common when dealing with sequencing data. Base R functions like rowSums() and colMeans() are specialized to operate over matrices and are the most efficient way to summarize matrix data. The R package matrixStats also contains highly optimized functions for operating on matrices.\nCompare the above to the tidy solution given the same matrix.\n\ntidyr::as_tibble(m, rownames = \"CpG\") |&gt;\n  tidyr::pivot_longer(!CpG, names_to = \"SampleName\", values_to = \"beta\") |&gt;\n  dplyr::group_by(CpG) |&gt;\n  dplyr::mutate(n = sum(beta &gt; 0.5)) |&gt;\n  dplyr::filter(n &gt; 3) |&gt;\n  tidyr::pivot_wider(id_cols = CpG, names_from = \"SampleName\", values_from = \"beta\") |&gt;\n  tibble::column_to_rownames(var = \"CpG\") |&gt;\n  data.matrix()\n\n        Sample1   Sample2   Sample3   Sample4   Sample5   Sample6\nCpG.3 0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248\nCpG.4 0.8830174 0.5726334 0.9942698 0.7954674 0.3688455 0.1218993\n\n\nThere’s probably some kind of tidy solution using across() that I’m missing but this is how most of the tidy code in the wild that I have seen looks\nHopefully these examples are enough to get you started understanding how subsetting works in R and appreciate how useful it is. Now that you have some familiarity with the using R functions for subsetting objects, you’re ready to start working with SummarizedExperiments.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#what-are-summarizedexperiments",
    "href": "r-basics.html#what-are-summarizedexperiments",
    "title": "R programming basics",
    "section": "What are SummarizedExperiments",
    "text": "What are SummarizedExperiments\nSummarizedExperiments are R objects meant for organizing and manipulating rectangular matrices that are typically produced by arrays or high-throughput sequencing. If you are doing any kind of analysis that requires associating feature-level data (RNA-seq gene counts, methylation array loci, ATAC-seq regions, etc.) with the genomic coordinates of those features and the sample-level metadata with which those features were measured, then you should be using a SummarizedExperiment to organize, manipulate, and store your results.\nPlease take a moment to read through the first 2 sections (at least) of the SummarizedExperiment vignette in order to familiarize yourself with what SummarizedExperiments are and their structure. I will demonstrate how you can use SummarizedExperiments below.\nFrom the SummarizedExperiment vignette:\n\nThe SummarizedExperiment object coordinates four main parts:\n\nassay(), assays(): A matrix-like or list of matrix-like objects of identical dimension\n\nmatrix-like: implements dim(), dimnames(), and 2-dimensional [, [&lt;- methods.\nrows: genes, genomic coordinates, etc.\ncolumns: samples, cells, etc.\n\ncolData(): Annotations on each column, as a DataFrame.\n\nE.g., description of each sample\n\nrowData() and / or rowRanges(): Annotations on each row.\n\nrowRanges(): coordinates of gene / exons in transcripts / etc.\nrowData(): P-values and log-fold change of each gene after differential expression analysis.\n\nmetadata(): List of unstructured metadata describing the overall content of the object.\n\nIn order to better understand how they work, let’s construct a SummarizedExperiment from scratch.\n\nConstructing a SummarizedExperiment\nHopefully you’ll already be working with data that is in a SummarizedExperiment or some other class that derives from one. But just in case you don’t have data structured as a SummarizedExperiment it’s useful and instructive to understand how to create one from scratch.\nTo be most useful, a SummarizedExperiment should have at least:\n\nA matrix of data with features in rows and samples in columns\nA metadata data.frame with samples as rownames and columns describing their properties\n\nAnother really useful object to add to SummarizedExperiments is a GRanges object describing the genomic locations of each feature in the matrix. Adding this to the SummarizedExperiment creates what is called a RangedSummarizedExperiment that acts just like a regular SummarizedExperiment with some extra features.\nTo construct our basic SummarizedExperiment:\n\nWe’ll create a ‘counts’ matrix with gene IDs as rows and Samples in columns\nWe’ll add some metadata describing the Samples\nWe’ll add on GRanges() describing the genomic location of the genes\n\n\nConstruct the counts matrix\n\nsuppressPackageStartupMessages(library(SummarizedExperiment))\n\n\ncounts &lt;- matrix(\n  data = rnbinom(n = 200 * 6, mu = 100, size = 1 / 0.5),\n  nrow = 200,\n  dimnames = list(paste0(\"Gene\", 1:200), paste0(\"Sample\", 1:6))\n)\n\n# Take a peek at what this looks like\ncounts[1:5, 1:5]\n\n      Sample1 Sample2 Sample3 Sample4 Sample5\nGene1     100     210     186      14      95\nGene2      74      17      62      48      27\nGene3     129      72     105     203      73\nGene4      73      80      84      81      59\nGene5      17     242      32      21      58\n\n\n\n\nConstruct the sample metadata\nIt is important that the sample metadata be either a data.frame or a DataFrame object because SummarizedExperiment requires the colData() to have rownames that match the colnames of the count matrix.\n\ncoldata &lt;- data.frame(\n  SampleName = colnames(counts),\n  Treatment = gl(2, 3, labels = c(\"Control\", \"Treatment\")),\n  Age = sample.int(100, 6),\n  row.names = colnames(counts)\n)\n\n# Take a peek at what this looks like\ncoldata\n\n        SampleName Treatment Age\nSample1    Sample1   Control  38\nSample2    Sample2   Control  71\nSample3    Sample3   Control  30\nSample4    Sample4 Treatment  64\nSample5    Sample5 Treatment   8\nSample6    Sample6 Treatment  47\n\n\nNotice that all of the rownames of the metadata are in the same order as the colnames of the counts matrix. This is necessary.\n\n\nConstruct gene range annotations\nYou will usually have gene annotations or GRanges objects loaded from a GTF file or you may even create GRanges yourself by specifying the chromosome, start, end, and strand, information manually.\n\nrowranges &lt;- GRanges(\n  rep(c(\"chr1\", \"chr2\"), c(50, 150)),\n  IRanges(floor(runif(200, 1e5, 1e6)), width = 100),\n  strand = sample(c(\"+\", \"-\"), 200, TRUE),\n  feature_id = sprintf(\"ID%03d\", 1:200),\n  gene_type = sample(c(\"protein_coding\", \"lncRNA\", \"repeat_element\"), 200, replace = TRUE)\n)\nnames(rowranges) &lt;- rownames(counts)\n\n# Take a peek at what this looks like\nrowranges\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt;\n    Gene1     chr1 954187-954286      - |       ID001 protein_coding\n    Gene2     chr1 552363-552462      + |       ID002 protein_coding\n    Gene3     chr1 303793-303892      - |       ID003         lncRNA\n    Gene4     chr1 875980-876079      + |       ID004 protein_coding\n    Gene5     chr1 427141-427240      - |       ID005 repeat_element\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 268871-268970      + |       ID196 repeat_element\n  Gene197     chr2 599914-600013      + |       ID197 repeat_element\n  Gene198     chr2 477464-477563      + |       ID198 repeat_element\n  Gene199     chr2 461424-461523      - |       ID199 repeat_element\n  Gene200     chr2 619939-620038      + |       ID200 protein_coding\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nConstruct the SummarizedExperiment object\nWith these pieces of information we’re ready to create a SummarizedExperiment object.\n\nse &lt;- SummarizedExperiment(\n  assays = list(counts = counts),\n  colData = coldata,\n  rowRanges = rowranges\n)\n\n# Printing the object gives a summary of what's inside\nse\n\nclass: RangedSummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(2): feature_id gene_type\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(3): SampleName Treatment Age",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#accessing-parts-of-the-summarizedexperiment-object",
    "href": "r-basics.html#accessing-parts-of-the-summarizedexperiment-object",
    "title": "R programming basics",
    "section": "Accessing parts of the SummarizedExperiment object",
    "text": "Accessing parts of the SummarizedExperiment object\nEvery part of the SummarizedExperiment object can be extracted with its accessor function. To extract a particular assay you can use the assay() function. To extract the column metadata you can use the colData() function. To extract the GRanges for the rows of the matrix you can use the rowRanges() function. The rowData() function also allows you to access row-level annotation information from data added to the rowData slot or by the mcols() of the rowRanges. This will be made more clear below.\n\nGetting the count matrix\n\nassay(se, \"counts\") |&gt; head()\n\n      Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene1     100     210     186      14      95      43\nGene2      74      17      62      48      27      71\nGene3     129      72     105     203      73      63\nGene4      73      80      84      81      59     153\nGene5      17     242      32      21      58      40\nGene6      22      91      38      89     164      43\n\n\nTo see what assays are available you can use the assays() function\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\n\n\n\nGetting the column metadata\n\ncolData(se)\n\nDataFrame with 6 rows and 3 columns\n         SampleName Treatment       Age\n        &lt;character&gt;  &lt;factor&gt; &lt;integer&gt;\nSample1     Sample1 Control          38\nSample2     Sample2 Control          71\nSample3     Sample3 Control          30\nSample4     Sample4 Treatment        64\nSample5     Sample5 Treatment         8\nSample6     Sample6 Treatment        47\n\n\n\n\nGetting the rowRanges\n\nrowRanges(se)\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt;\n    Gene1     chr1 954187-954286      - |       ID001 protein_coding\n    Gene2     chr1 552363-552462      + |       ID002 protein_coding\n    Gene3     chr1 303793-303892      - |       ID003         lncRNA\n    Gene4     chr1 875980-876079      + |       ID004 protein_coding\n    Gene5     chr1 427141-427240      - |       ID005 repeat_element\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 268871-268970      + |       ID196 repeat_element\n  Gene197     chr2 599914-600013      + |       ID197 repeat_element\n  Gene198     chr2 477464-477563      + |       ID198 repeat_element\n  Gene199     chr2 461424-461523      - |       ID199 repeat_element\n  Gene200     chr2 619939-620038      + |       ID200 protein_coding\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nGetting the rowData\nNote that rowData in this case is the same as mcols() of the rowRanges\n\nrowData(se)\n\nDataFrame with 200 rows and 2 columns\n         feature_id      gene_type\n        &lt;character&gt;    &lt;character&gt;\nGene1         ID001 protein_coding\nGene2         ID002 protein_coding\nGene3         ID003         lncRNA\nGene4         ID004 protein_coding\nGene5         ID005 repeat_element\n...             ...            ...\nGene196       ID196 repeat_element\nGene197       ID197 repeat_element\nGene198       ID198 repeat_element\nGene199       ID199 repeat_element\nGene200       ID200 protein_coding",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#modifying-a-summarizedexperiment",
    "href": "r-basics.html#modifying-a-summarizedexperiment",
    "title": "R programming basics",
    "section": "Modifying a SummarizedExperiment",
    "text": "Modifying a SummarizedExperiment\nOnce you create a SummarizedExperiment you are not stuck with the information in that object. SummarizedExperiments allow you to add and modify the data within the object.\n\nAdding assays\nFor example, we may wish to calculate counts per million values for our counts matrix and add a new assay back into our SummarizedExperiment object.\n\n# Calculate counts per million\ncounts &lt;- assay(se, \"counts\")\ncpm &lt;- counts / colSums(counts) * 1e6\n\n# Add the CPM data as a new assay to our existing se object\nassay(se, \"cpm\") &lt;- cpm\n\n# And if we wish to log-scale these values\nassay(se, \"logcounts\") &lt;- log2(cpm)\n\n# Now there are three assays available\nassays(se)\n\nList of length 3\nnames(3): counts cpm logcounts\n\n\nNote: Instead of creating intermediate variables we could also directly use the assays like so:\n\nassay(se, \"cpm\") &lt;- assay(se, \"counts\") / colSums(assay(se, \"counts\")) * 1e6\n\n\n\nAdding metadata\nSummarizedExperiment objects use the $ to get and set columns of the metadata contained in the colData slot. For example, to get all of the Ages we can use:\n\nse$Age\n\n[1] 38 71 30 64  8 47\n\n\nIf we want to add a new column we simply create the new column in the same way\n\nse$Batch &lt;- factor(rep(c(\"A\", \"B\", \"C\"), 2))\n\n# Now you can se that a new 'Batch` column has been added to the colData\ncolData(se)\n\nDataFrame with 6 rows and 4 columns\n         SampleName Treatment       Age    Batch\n        &lt;character&gt;  &lt;factor&gt; &lt;integer&gt; &lt;factor&gt;\nSample1     Sample1 Control          38        A\nSample2     Sample2 Control          71        B\nSample3     Sample3 Control          30        C\nSample4     Sample4 Treatment        64        A\nSample5     Sample5 Treatment         8        B\nSample6     Sample6 Treatment        47        C\n\n\n\n\nAdding rowData\nWe can also modify the data which describes each feature in the matrix by adding columns to the rowData. For example, let’s create a new column called Keep if the gene is a protein_coding gene.\n\nrowData(se)$Keep &lt;- rowData(se)$gene_type == \"protein_coding\"\n\nrowData(se)\n\nDataFrame with 200 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene1         ID001 protein_coding      TRUE\nGene2         ID002 protein_coding      TRUE\nGene3         ID003         lncRNA     FALSE\nGene4         ID004 protein_coding      TRUE\nGene5         ID005 repeat_element     FALSE\n...             ...            ...       ...\nGene196       ID196 repeat_element     FALSE\nGene197       ID197 repeat_element     FALSE\nGene198       ID198 repeat_element     FALSE\nGene199       ID199 repeat_element     FALSE\nGene200       ID200 protein_coding      TRUE",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#subsetting-summarizedexperiment-objects",
    "href": "r-basics.html#subsetting-summarizedexperiment-objects",
    "title": "R programming basics",
    "section": "Subsetting SummarizedExperiment objects",
    "text": "Subsetting SummarizedExperiment objects\nSummarizedExperiments follow the basic idea of\nse[the rows you want, the columns you want]\nWith a SummarizedExperiment “the rows you want” corresponds to the features in the rows of the matrix/rowData and “the columns you want” corresponds to the metadata in colData\n\nSubsetting based on sample metadata\nFor example, if we want to select all of the data belonging only to samples in the Treatment group we can use the following:\n\n(trt &lt;- se[, se$Treatment == \"Treatment\"])\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample4 Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim of the object changed from 6 to 3. This is because we have selected only the Samples from the original SummarizedExperiment object from the treatment group. The cool thing about SummarizedExperiments is that all of the assays have also been subsetted to reflect this selection!\nTake a look at the “logcounts” assay. It only contains Samples 4, 5, and 6.\n\nassay(trt, \"logcounts\") |&gt; head()\n\n        Sample4  Sample5  Sample6\nGene1  9.681439 12.23369 11.09368\nGene2 11.306469 10.37294 11.82069\nGene3 13.329167 11.85724 11.85136\nGene4 11.957898 11.55359 12.97889\nGene5 10.059737 11.73207 10.98576\nGene6 12.146676 13.07906 11.04431\n\n\nOf course you can combine multiple conditions as well\n\nse[, se$Batch %in% c(\"B\", \"C\") & se$Age &gt; 10]\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample2 Sample3 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\n\n\nSubsetting based on rows\nWe can also select certain features that we want to keep using row subsetting. For example to select only the first 50 rows:\n\nse[1:50, ]\n\nclass: RangedSummarizedExperiment \ndim: 50 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(50): Gene1 Gene2 ... Gene49 Gene50\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim changed from 200 to 50 reflecting the fact that we have only selected the first 50 rows.\nThis subsetting is very useful when combined with logical operators. Above we created a vector in rowData called “Keep” that contained TRUE if the corresponding row of the se object was a coding gene and FALSE otherwise. Let’s use this vector to subset our se object.\n\n(coding &lt;- se[rowData(se)$Keep, ])\n\nclass: RangedSummarizedExperiment \ndim: 68 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(68): Gene1 Gene2 ... Gene193 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nAnd if we look at the resulting rowData we can see that it only contains the protein_coding features\n\nrowData(coding)\n\nDataFrame with 68 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene1         ID001 protein_coding      TRUE\nGene2         ID002 protein_coding      TRUE\nGene4         ID004 protein_coding      TRUE\nGene8         ID008 protein_coding      TRUE\nGene13        ID013 protein_coding      TRUE\n...             ...            ...       ...\nGene183       ID183 protein_coding      TRUE\nGene185       ID185 protein_coding      TRUE\nGene187       ID187 protein_coding      TRUE\nGene193       ID193 protein_coding      TRUE\nGene200       ID200 protein_coding      TRUE\n\n\nEach assay also reflects this operation\n\nassay(coding, \"cpm\") |&gt; head()\n\n        Sample1    Sample2   Sample3    Sample4  Sample5   Sample6\nGene1  5865.103 10646.3878  9453.141   821.1144 4816.223  2185.404\nGene2  3904.601   834.9295  3158.753  2532.7142 1326.065  3617.281\nGene4  3585.286  4075.8101  4432.250  3978.1936 3005.910  8073.027\nGene8  1582.946 12180.1483  1069.900  3165.8928 2603.016   815.162\nGene13 5571.848  4512.0406  6556.211 12199.4135 9936.629 15704.411\nGene15 5019.011  3405.1637 15366.569  6235.7414 4421.630  3988.270\n\n\n\n\nSubsetting based on rowRanges\nA closely related row-wise subsetting operation can be used if you have a RangedSummarizedExperiment (a SummarizedExperiment with a rowRanges slot) that allows you to perform operations on a SummarizedExperiment object like you would a GRanges object.\nFor example, let’s say we only wanted to extract the features on Chromosome 2 only. Then we can use the GenomicRanges function subsetByOverlaps directly on our SummarizedExperiment object like so:\n\n# Region of interest\nroi &lt;- GRanges(seqnames = \"chr2\", ranges = 1:1e7)\n\n# Subset the SE object for only features on chr2\n(chr2 &lt;- subsetByOverlaps(se, roi))\n\nclass: RangedSummarizedExperiment \ndim: 150 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(150): Gene51 Gene52 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nYou can see again that the dim changed reflecting our selection. Again, all of the associated assays and rowData have also been subsetted reflecting this change as well.\n\nrowData(chr2)\n\nDataFrame with 150 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene51        ID051 protein_coding      TRUE\nGene52        ID052         lncRNA     FALSE\nGene53        ID053 repeat_element     FALSE\nGene54        ID054 repeat_element     FALSE\nGene55        ID055 repeat_element     FALSE\n...             ...            ...       ...\nGene196       ID196 repeat_element     FALSE\nGene197       ID197 repeat_element     FALSE\nGene198       ID198 repeat_element     FALSE\nGene199       ID199 repeat_element     FALSE\nGene200       ID200 protein_coding      TRUE\n\n\n\nassay(chr2, \"counts\") |&gt; head()\n\n       Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene51      32      45     139     261     153      95\nGene52     113      80      80      61      72     131\nGene53     118      24      60     153       5     103\nGene54      20       1      35       4      89      49\nGene55     249      58      43     307      51      33\nGene56     215      96      28     198      50      81\n\n\n\nrowRanges(chr2)\n\nGRanges object with 150 ranges and 3 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type      Keep\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\n   Gene51     chr2 839636-839735      + |       ID051 protein_coding      TRUE\n   Gene52     chr2 388487-388586      + |       ID052         lncRNA     FALSE\n   Gene53     chr2 357210-357309      + |       ID053 repeat_element     FALSE\n   Gene54     chr2 230565-230664      + |       ID054 repeat_element     FALSE\n   Gene55     chr2 491399-491498      + |       ID055 repeat_element     FALSE\n      ...      ...           ...    ... .         ...            ...       ...\n  Gene196     chr2 268871-268970      + |       ID196 repeat_element     FALSE\n  Gene197     chr2 599914-600013      + |       ID197 repeat_element     FALSE\n  Gene198     chr2 477464-477563      + |       ID198 repeat_element     FALSE\n  Gene199     chr2 461424-461523      - |       ID199 repeat_element     FALSE\n  Gene200     chr2 619939-620038      + |       ID200 protein_coding      TRUE\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\nThere’s also a few shortcuts on range operations using GRanges/SummarizedExperiments. See the help pages for %over, %within%, and %outside%. For example:\n\nall.equal(se[se %over% roi, ], subsetByOverlaps(se, roi))\n\n[1] TRUE\n\n\n\n\nCombining subsetting operations\nOf course you don’t have to perform one subsetting operation at a time. Like base R you can combine multiple expressions to subset a SummarizedExperiment object.\nFor example, to select only features labeled as repeat_elements and the Sample from ‘Batch’ A in the ‘Control’ group\n\n(selected &lt;- se[\n  rowData(se)$gene_type == \"repeat_element\",\n  se$Treatment == \"Control\" &\n    se$Batch == \"A\"\n])\n\nclass: RangedSummarizedExperiment \ndim: 72 1 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(72): Gene5 Gene6 ... Gene198 Gene199\nrowData names(3): feature_id gene_type Keep\ncolnames(1): Sample1\ncolData names(4): SampleName Treatment Age Batch",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#saving-a-summarizedexperiment",
    "href": "r-basics.html#saving-a-summarizedexperiment",
    "title": "R programming basics",
    "section": "Saving a SummarizedExperiment",
    "text": "Saving a SummarizedExperiment\nSince SummarizedExperiments keep basically all information about an experiment in one place, it is convenient to save the entire SummarizedExperiment object so that you can pick up an analysis where you left off or even to facilitate better sharing of data between collaborators.\nYou can save the entire SummarizedExperiment object with:\nsaveRDS(se, \"/path/to/se.rds\")\nAnd when you want to read the same object back into R for your next analysis you can do so with:\nse &lt;- readRDS(\"/path/to/se.rds\")",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#summarizedexperiments-in-the-real-world",
    "href": "r-basics.html#summarizedexperiments-in-the-real-world",
    "title": "R programming basics",
    "section": "SummarizedExperiments in the real world",
    "text": "SummarizedExperiments in the real world\nIf you’re working with any Bioconductor packages it’s likely that the object you’re working with either is a SummarizedExperiment or is inherited from one. For example, the DESeqDataSet from the DESeq2 package and BSseq objects from the bsseq package both inherit from a SummarizedExperiment and thus retain all of the same functionality as above. If you go to the SummarizedExperiment landing page and click “See More” under details you can see all of the packages that depend on SummarizedExperiment.\nAlso, many common methods are also implemented for SummarizedExperiment objects. For example, to simplify calculating counts-per-million above I could have simply used the edgeR::cpm() directly on the SummarizedExperiment object. Many functions in bioconductor packages know how to deal directly with SummarizedExperiments so you don’t ever have to take the trouble extracting components and performing tedious calculations yourself.\n\nassay(se, \"cpm\") &lt;- edgeR::cpm(se)\n\nI also left out any discussion of the metadata() slot of the SummarizedExperiment. The metadata slot is simply a list of any R object that contains information about the experiment. The metadata in the metadata slots are not subjected to the same subsetting rules as the other slots. In practice this assay contains additional information about the experiment as a whole. For example, I typically store bootstrap alignments for each sample here.\nTo add something to the SummarizedExperiment metadata slot you can do:\n\nmetadata(se)$additional_info &lt;- \"Experiment performed on 6 samples with three replicates each\"\n\nAnd to retrieve this:\n\nmetadata(se)$additional_info\n\n[1] \"Experiment performed on 6 samples with three replicates each\"",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#closing-thoughts",
    "href": "r-basics.html#closing-thoughts",
    "title": "R programming basics",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nHopefully this was enough information to get you started using SummarizedExperiments. There’s many things I left out such as different backings for storing out of memory data, a tidyverse interface to SummarizedExperiment objects, TreeSummarizedExperiments for microbiome data, MultiAssayExperiments for dealing with experiments containing multiomics data, and much more.\nPlease let me know your thoughts and if anything needs more clarification.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "R programming basics",
    "section": "",
    "text": "Introduction to programming in R\nThere are tons of great resources for learning R. R for Data Science is probably the most popular resource for new useRs to get up to speed with slicing and dicing data in R. The R for Data Science book, however, is taught from the perspective of the Tidyverse. The Tidyverse is an opinionated set of packages and functions that help users perform data manipulations primarily on data.frames. While these packages and functions can be great for experienced users by providing ergonomic and consistent interfaces for data.frame manipulation, it is my personal belief that new users should first learn the base language, especially if their goal is to perform bioinformatics analysis.\nBioinformatics tools rely heavily on subsetting and matrix manipulations. In my experience, users who start learning R using only function from the Tidyverse have a difficult time understanding matrix manipulations and subsetting operations common in bioinformatics workflows. This becomes especially important when using SummarizedExperiments - the backbone of many bioinformatics data structures in R.\nFor this reason, we’re going to focus on learning R from the ground up using functions that exist primarily in the base language. A great resource for learning base R quickly is Norm Matloff’s fasteR which can be found here.",
    "crumbs": [
      "R programming basics"
    ]
  }
]