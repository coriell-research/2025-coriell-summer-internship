[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2025 Coriell Bioinformatics Internship",
    "section": "",
    "text": "Welcome!\nWe’re excited to have you as an intern and we hope that you’ll learn a lot with us this summer. We’ll use this book as a way to teach some fundamental concepts before you start working on an interesting project.\nWe hope to cover:\n\nSome data best practices\nThe basics of programming in R\nThe basics of data visualization\nThe basics of statistical testing with simulations\nHigh-dimensional data analysis\nHow to give great scientific presentations"
  },
  {
    "objectID": "data-best-practices.html",
    "href": "data-best-practices.html",
    "title": "1  Some data best practices",
    "section": "",
    "text": "2 File Management"
  },
  {
    "objectID": "data-best-practices.html#project-organization",
    "href": "data-best-practices.html#project-organization",
    "title": "1  Organizing projects",
    "section": "1.1 Project organization",
    "text": "1.1 Project organization\nScience requires reproducibility. not only to ensure that your results are generalizable but also to make your own life easier. One overlooked aspect of of learning data science is creating consistent and clear organization of your projects."
  },
  {
    "objectID": "data-best-practices.html#why-care-about-data-management",
    "href": "data-best-practices.html#why-care-about-data-management",
    "title": "1  Some data best practices",
    "section": "1.2 Why care about data management?",
    "text": "1.2 Why care about data management?\nComputing is now an essential part of research. This is outlined beautifully in the paper, “All biology is computational biology” by Florian Markowetz. Data is getting bigger and bigger and we need to be equipped with the tools for storing, manipulating, and communicating insights derived from it. However, most researchers are never taught good computational practices. Computational best practices are imperitive. Implementing best (or good enough) practices can improve reproducibility, ensure correctness, and increase efficiency."
  },
  {
    "objectID": "data-best-practices.html#topics",
    "href": "data-best-practices.html#topics",
    "title": "1  Organizing projects",
    "section": "1.2 Topics",
    "text": "1.2 Topics\n\nData management\nProject organization\nTracking changes\nSoftware\nManuscripts"
  },
  {
    "objectID": "data-best-practices.html#save-and-lock-all-raw-data",
    "href": "data-best-practices.html#save-and-lock-all-raw-data",
    "title": "1  Some data best practices",
    "section": "2.6 Save and lock all raw data",
    "text": "2.6 Save and lock all raw data\nKeep raw data in its unedited form. This includes not making changes to filenames. In bioinformatics, it’s common to get data from a sequencing facility with incomprehensible filenames. Don’t fall victim to the temptation of changing these filenames! Instead, it’s much better to keep the filenames exactly how they were sent to you and simply create a spreadsheet that maps the files to their metadata. In the case of a sample mix-up, it’s much easier to make a change to a row in a spreadsheet then to track down all of the filenames that you changed and ensure they’re correctly modified.\nOnce you have your raw data, you don’t want the raw data to change in any way that is not documented by code. To ensure this, you can consider changing file permissions to make the file immutable (unchangable). Using bash, you can change file permissions with:\nchattr +i myfile.txt\nIf you’re using Excel for data analysis, lock the spreadsheet with the raw data and only make references to this sheet when performing calculations."
  },
  {
    "objectID": "data-best-practices.html#backups",
    "href": "data-best-practices.html#backups",
    "title": "1  Some data best practices",
    "section": "2.8 Backups",
    "text": "2.8 Backups\n\nThere are two types of people, those who do backups and those who will do backups.\n\nThe following are NOT backup solutions:\n\nCopies of data on the same disk\nDropbox/Google Drive\nRAID arrays\n\nAll of these solutions mirror the data. Corruption or ransomware will propagate. For example, if you corrupt a file on your local computer and then push that change to DropBox then the file on DropBox is now also corrupted. I’m sure some of these cloud providers have version controlled files but it’s better to just avoid the problem entirely by keeping good backups.\n\n2.8.1 Use the 3-2-1 rule:\n\nKeep 3 copies of any important file: 1 primary and 2 backups.\nKeep the files on 2 different media types to protect against different types of hazards.\nStore 1 copy offsite (e.g., outside your home or business facility).\n\nA backup is only a backup if you can restore the files!"
  },
  {
    "objectID": "data-best-practices.html#files",
    "href": "data-best-practices.html#files",
    "title": "1  Organizing projects",
    "section": "2.4 Files",
    "text": "2.4 Files\n\nStore data in standard, machine readable formats\n\nTXT, CSV, TSV, JSON, YAML, HDF5\nLarge text files should be compressed\n\nxz is better than gzip. We should all use xz\n\n\nEnsure filenames are computer friendly\n\nNo spaces\nUse only letters, numbers, and “-” “_” and “.” as delimiters\n\n\n# Bad\ndata for jozef.txt\n\n# Okay\ndata-for-jozef.txt\n\n# Best\n2023-11-09_repetitive-element-counts.txt\nQuiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone"
  },
  {
    "objectID": "data-best-practices.html#quiz",
    "href": "data-best-practices.html#quiz",
    "title": "1  Organizing projects",
    "section": "2.4 Quiz",
    "text": "2.4 Quiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone"
  },
  {
    "objectID": "data-best-practices.html#more-on-filenames",
    "href": "data-best-practices.html#more-on-filenames",
    "title": "1  Some data best practices",
    "section": "2.3 More on filenames",
    "text": "2.3 More on filenames\nIt cannot be stressed enough how important filenames can be for an analysis. To get the most out of your files and to avoid catastrpohic failures, you should stick to some basic principles for naming files. First, use consistent and unique identifiers across all files that you generate for an experiment. For example, if you’re conducting a study that has both RNA-seq and ATAC-seq data performed on the same subjects, don’t name the files from the RNA-seq experiment subject1.fq.fz and the files from the ATAC-seq experiment control_subject1.fq.gz if they refer to the same sample. For small projects, it’s fairly easy to create consistent and unique IDs for each subject. For large projects unique random IDs can be used.\nFor example, the following filenames would be bad:\nsubject1-control_at_96hr1.txt\ns1_ctl-at-4days_2.txt\ns2TRT4d1.txt\nsbj2_Treatment_4_Days_Replicate_2.txt\nThese are better. Why are they better? They are consistent. The delimiter is consistent between the words (“_“) and each of the words represents something meaningful about the sample. These filenames also do not contain any spaces and can easily be parsed automatically.\nsubject1_control_4days_rep1.txt\nsubject1_control_4days_rep2.txt\nsubject2_treatment_4days_rep1.txt\nsubject2_treatment_4days_rep2.txt\nFile naming best practices also apply to naming executable scripts. The name of the file should describe the function of the script. For example,\n01_align_with_STAR.sh\nis better than simply naming the file\n01_script.sh\nPro-tip\nOne easy way to create unique random IDs for a large project is to concatenate descriptions and take the SHA/MDA5 hashsum.\necho \"subject1_control_4days_rep1\" | sha256\n# 57f458a294542b2ed6ac14ca64d3c8e4599eed7a\n\necho \"subject1_control_4days_rep2\" | shasum\n# b6ea9d729e57cce68b37de390d56c542bc17dea6"
  },
  {
    "objectID": "data-best-practices.html#create-analysis-freindly-data---tidy-data",
    "href": "data-best-practices.html#create-analysis-freindly-data---tidy-data",
    "title": "1  Some data best practices",
    "section": "2.4 Create analysis freindly data - tidy data",
    "text": "2.4 Create analysis freindly data - tidy data\nThe term tidy data was defined by Hadley Wickham to describe data which is amenable to downstream analysis. Most people are familiar with performing a quick and dirty data analysis in a program like Excel. You may have also used some of Excel’s fancy features for coloring cells, adding bold and underlines to text, and formatting cells with other decorations. All of this tends to just be extra fluff. If you format you data properly then it will be much easier to perform downstream analysis on and will not require the use of extra decorations. This is true even in Excel!\nTo conform to the requirements of being tidy, the data should follow some simple principles:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nDon’t cram two variables into one value. e.g. “male_treated”.\nDon’t embed units into data. e.g. “3.4kg”. Instead, put in column name e.g. “weight_kg” = 3.4\n\n\n\nOnce your data is in this format, it can easily be read into downstream programs like R, or parsed with command line text editing programs like sed.\nThe iris dataset in R provides a classic example of this format\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nPro-tip\ndata.frame like objects can be stored and retrieved efficiently using the Apache Arrow format instead of CSV files:\ndf <- arrow::read_parquet(\"path/to/file.parquet\")\narrow::write_parquet(df, \"path/to/different_file.parquet\")\nsee R arrow for more details. There’s also the nanoparquet package which provides a light weight reader/writer for parquet files."
  },
  {
    "objectID": "data-best-practices.html#untidy-data",
    "href": "data-best-practices.html#untidy-data",
    "title": "1  Some data best practices",
    "section": "2.5 Untidy(?) data",
    "text": "2.5 Untidy(?) data\nSome data formats are not amenable to the ‘tidy’ structure, i.e. they’re just not best represented as long tables. For example, large/sparse matrices, geo-spatial data, R objects, etc.the lesson here is to store data in the format that is most appropriate for the data. For example, don’t convert a matrix to a long format and save as a tsv file! Save it as an .rds file instead. Large matrices can also be efficiently stored as HDF5 files. Sparse matrices can be saved and accessed eficiently using the Matrix package in R. And if you are accessing the same data often, consider storing as a SQLite database and accessing with dbplyr or sqlalchemy in Python. The main point is don’t force data into a format that you’re familiar with only because you’re familiar with that format. This will often lead to large file sizes and inefficient performance."
  },
  {
    "objectID": "data-best-practices.html#record-all-steps-used-to-generate-the-data",
    "href": "data-best-practices.html#record-all-steps-used-to-generate-the-data",
    "title": "1  Some data best practices",
    "section": "4.1 Record all steps used to generate the data",
    "text": "4.1 Record all steps used to generate the data\nAlways document all steps you used to generate the data that’s present in your projects. This can be as simple as a README with some comments and a wget command or as complex as a snakemake workflow. The point is, be sure you can track down the exact source of every file that you created or downloaded.\nFor example, a README documenting the creation of the files needed to generate a reference index might look like:\nTranscripts:\nwget http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/gencode.v38.transcripts.fa.gz\n\nPrimary Assembly:\nwget http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/GRCh38.primary_assembly.genome.fa.gz\n\nCreate concatenated transcripts + genome for salmon (i.e. gentrome):\ncat gencode.v38.transcripts.fa.gz GRCh38.primary_assembly.genome.fa.gz > gentrome.fa.gz\n\nCreate decoys file for salmon:\ngrep \">\" <(gunzip -c GRCh38.primary_assembly.genome.fa.gz) | cut -d \" \" -f 1 > decoys.txt\nsed -i.bak -e 's/>//g' decoys.txt\nFor more complicated steps include a script. e.g. creating a new genome index, subsetting BAM files, accessing data from NCBI, etc.\nPro-tip\nA simple way to build a data pipeline that is surprisingly robust is just to create scripts for each step and number them in the order that they should be executed.\n01_download.sh\n02_process.py\n03_makeFigures.R\nYou can also include a runner script that will execute all of the above. Or, for more consistent workflows, use a workflow manager like Nextflow, Snakemake, WDL, or good ole’ GNU Make"
  },
  {
    "objectID": "data-best-practices.html#look-familiar",
    "href": "data-best-practices.html#look-familiar",
    "title": "1  Some data best practices",
    "section": "3.1 Look familiar?",
    "text": "3.1 Look familiar?"
  },
  {
    "objectID": "data-best-practices.html#project-structure",
    "href": "data-best-practices.html#project-structure",
    "title": "1  Some data best practices",
    "section": "3.2 Project structure",
    "text": "3.2 Project structure\nOne of the most useful changes that you can make to your workflow is the create a consistent folder structure for all of your analyses and stick with it. Coming up with a consistent and generalizable structure can be challenging at first but some general guidelines are presented here and here\nFirst of all, when beginning a new project, you should have some way of naming your projects. One good way of naming projects is to each project a descriptive name and append the date the project was started. For example, brca_rnaseq_2023-11-09/ is better than rnaseq_data/. In six months when a collaborator wants their old BRCA data re-analyzed you’ll thank yourself for timestamping the project folder and giving it a descriptive name.\nMy personal structure for every project looks like:\nyyyymmdd_project-name/\n├── data\n├── doc\n├── README\n├── results\n│   ├── data-files\n│   ├── figures\n│   └── rds-files\n└── scripts\n\n\nPrefixing the project directory with the ISO date allows for easy sorting by date\na README text file is present at the top level of the directory with a short description about the project and any notes or updates\ndata/ should contain soft links to any raw data or the results of downloading data from an external source\ndoc/ contains metadata documents about the samples or other metadata information about the experiment\nresults/ contains only data generated within the project. It has sub-directories for figures/, data-files/ and rds-files/. If you have a longer or more complicated analysis then add sub-directories indicating which script generated the results.\nscripts/ contains all analysis scripts numbered in their order of execution. Synchronize the script names with the results they produce.\n\n\n3.2.1 A more complex example\nyyyymmdd_project-name/\n├── data\n│   ├── 00_fastq\n│   ├── 01_fastqc\n│   ├── 02_trimmed\n│   └── 03_quants\n├── delivered\n│   └── 2022-09-11_analysis-results.zip\n├── doc\n│   └── sample-metadata.txt\n├── project.Rproj\n├── README\n├── results\n│   ├── data-files\n│   │   ├── 04\n│   │   │   └── differential-expression.tsv.gz\n│   │   └── 05\n│   │       └── gsea-results.tsv.gz\n│   ├── figures\n│   │   ├── 04\n│   │   │   ├── ma-plots.png\n│   │   │   └── volcano-plots.png\n│   │   └── 05\n│   │       └── enrichment-plots.png\n│   └── rds-files\n│       └── 04\n│           └── SummarizedExperiment.rds\n└── scripts\n    ├── 01_fastqc.sh\n    ├── 02_trim.sh\n    ├── 03_quant.sh\n    ├── 04_differential_expression.Qmd\n    └── 05_gsea.Qmd\nPro-tip\nIf using Rstudio, include an .Rproj file at the top level of your directory. Doing this enables you to use the here package to reference data within your project in a relative fashion. For example, you can more easily save data with:\nplot_volcano(...)\nggsave(here(\"data\", \"results\", \"figures\", \"04\", \"volcano-plots.png\"))"
  },
  {
    "objectID": "data-best-practices.html#project-structure-1",
    "href": "data-best-practices.html#project-structure-1",
    "title": "1  Some data best practices",
    "section": "3.3 Project structure",
    "text": "3.3 Project structure\nA fuller example might look more like:\nyyyymmdd_project-name/\n├── data\n│   ├── 00_fastq\n│   ├── 01_fastqc\n│   ├── 02_trimmed\n│   └── 03_quants\n├── delivered\n│   └── 2022-09-11_analysis-results.zip\n├── doc\n│   └── sample-metadata.txt\n├── project.Rproj\n├── README\n├── results\n│   ├── data-files\n│   │   ├── 04\n│   │   │   └── differential-expression.tsv.gz\n│   │   └── 05\n│   │       └── gsea-results.tsv.gz\n│   ├── figures\n│   │   ├── 04\n│   │   │   ├── ma-plots.png\n│   │   │   └── volcano-plots.png\n│   │   └── 05\n│   │       └── enrichment-plots.png\n│   └── rds-files\n│       └── 04\n│           └── SummarizedExperiment.rds\n└── scripts\n    ├── 01_fastqc.sh\n    ├── 02_trim.sh\n    ├── 03_quant.sh\n    ├── 04_differential_expression.Qmd\n    └── 05_gsea.Qmd\nPro-tip\nIf using Rstudio, include an .Rproj file at the top level of your directory. Doing this enables you to use the here package to reference data within your project in a relative fashion. For example, you can more easily save data with:\nplot_volcano(...)\nggsave(here(\"data\", \"results\", \"figures\", \"04\", \"volcano-plots.png\"))"
  },
  {
    "objectID": "data-best-practices.html#manual-version-control",
    "href": "data-best-practices.html#manual-version-control",
    "title": "1  Some data best practices",
    "section": "4.2 Manual version control",
    "text": "4.2 Manual version control\nVersion control refers to the practice of tracking changes in files and data over their lifetime. You should always track any changes made to your project over the entire life of the project. This can be done either manually or using a dedicated version control system. If doing this manually, add a file called “CHANGELOG.md” in your docs/ directory and add detailed notes in reverse chronological order.\nFor example:\n## 2016-04-08\n\n* Switched to cubic interpolation as default.\n* Moved question about family's TB history to end of questionnaire.\n\n## 2016-04-06\n\n* Added option for cubic interpolation.\n* Removed question about staph exposure (can be inferred from blood test results).\n\nIf you make a significant change to the project, copy the whole directory, date it, and store it such that it will no longer be modified. Copies of these old projects can be compressed and saved with tar + xz compression\ntar -cJvf old.20231109_myproject.tar.xz myproject/`"
  },
  {
    "objectID": "data-best-practices.html#version-control-with-git",
    "href": "data-best-practices.html#version-control-with-git",
    "title": "1  Some data best practices",
    "section": "4.3 Version control with git",
    "text": "4.3 Version control with git\n\n\n\n\n\ngit is probably the de facto version control system in use today for tracking changes across software projects. You should strive to learn and use git to track your projects. Version control systems allow you to track all changes, comment on why changes were made, create parallel branches, and merge existing ones.\ngit is primarily used for source code files. Microsoft Office files and PDFs can be stored with Github but it’s hard to track changes. Rely on Microsoft’s “Track Changes” instead and save frequently.\nIt’s not necessary to version control raw data (back it up!) since it shouldn’t change. Likewise, backup intermediate data and version control the scripts that made it.\nFor a quick primer on Git and GitHub check out the book Happy Git with R or The Official GitHub Training Manual Anyone in the lab can join the coriell-research organization on Github and start tracking their code\nBe careful committing sensitive information to GitHub"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts",
    "title": "1  Some data best practices",
    "section": "5.1 Quick tips to improve your scripts",
    "text": "5.1 Quick tips to improve your scripts\n\n5.1.1 Place a description at the top of every script\nThe description should indicate who the author is. When the code was created. A short description of what the expected inputs and outputs are along with how to use the code. You three months from now will appreciate it when you need to revisit your analysis\nFor example:\n#!/usr/bin/env python3\n# Gennaro Calendo\n# 2023-11-09\n# \n# This scripts performs background correction of all images in the \n#  user supplied directory\n#\n# Usage ./correct-bg.py --input images/ --out_dir out_directory\n#\nfrom image_correction import background_correct\n\nfor img in images:\n  img = background_correct(img)\n  save_image(img, \"out_directory/corrected.png\")\n  \n\n\n5.1.2 Decompose programs into functions\nFunctions make it easier to reason about your code, spot errors, and make changes. This also follows the Don’t Repeat Yourself principle aimed at reducing repetition by replacing it with abstractions that are more stable\nCompare this chunk of code that rescales values using a min-max function (0-1)\ndf <- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b <- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c <- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d <- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\nto this function which does the same thing\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf <- lapply(df, rescale01)\nWhich is easier to read? Which is easier to debug? Which is more efficient?\n\n\n5.1.3 Give functions and variables meaningful names\n\nPrograms are written for people and then computers\nUse variable and function names that are meaningful and correct\nKeep names consistent. Use either snake_case or camelCase but try not to mix both\n\nBad:\nlol <- 1:100\n\nmydata <- data.frame(x = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nf <- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\nBetter:\nages <- 1:100\n\nbioinfo_names <- data.frame(Name = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nplotScatterPlot <- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\n\n\n5.1.4 Do not control program flow with comments\n\nthis is error prone and makes it difficult or impossible to automate\nUse if/else statements instead\n\nBad:\n# Download the file\n#filename <- \"data.tsv\"\n#url <- \"http:://example.com/data.tsv\"\n#download.file(url, filename)\n\n# Read in to a data.frame\ndf <- read.delim(\"data.tsv\", sep=\"\\t\")\nGood:\nfilename <- \"data.tsv\"\nurl <- \"http:://example.com/data.tsv\"\n\nif (!file.exists(filename)) {\n  download.file(url, filename)\n}\ndf <- read.delim(filename)\n\n\n5.1.5 Use a consistent style\nPick a style guide and stick with it. If using R, the styler package can automatically clean up poorly formatted code. If using Python, black is a highly opinionated formatter that is pretty popular. Although, I think ruff is currently all the rage with the Pythonistas these days.\nBad:\nflights|>group_by(dest)|> summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |> \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-> flight_plot\nGood:\nflight_plot <- flights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()\n\n\n5.1.6 Don’t use right hand assignment\nThis is R specific. I’ve seen this pop up with folks who are strong tidyverse adherents. I get it, that’s the direction of the piping operator. However, this right-hand assignment flies in the face of basically every other programming language, and since code is primarily read rather than executed, it’s much harder to scan a codebase and understand the variable assignment when the assignments can be anywhere in the pipe!\nDon’t do this\ndata |> \n  select(...) |> \n  filter(...) |> \n  group_by(...) |> \n  summarize(...) -> by_group\nIt’s much easier to look down a script and see that by_group is created by all of the piped operations when assigned normally.\nby_group <- data |> \n  select(...) |> \n  filter(...) |> \n  group_by(...) |> \n  summarize(...)"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-1",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-1",
    "title": "1  Some data best practices",
    "section": "5.2 Quick tips to improve your scripts",
    "text": "5.2 Quick tips to improve your scripts\n\n5.2.1 Give functions and variables meaningful names\n\nPrograms are written for people and then computers\nUse variable and function names that are meaningful and correct\nKeep names consistent. Use either snake_case or camelCase but try not to mix both\n\nBad:\nlol <- 1:100\n\nmydata <- data.frame(x = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nf <- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\nBetter:\nages <- 1:100\n\nbioinfo_names <- data.frame(Name = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nplotScatterPlot <- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-2",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-2",
    "title": "1  Some data best practices",
    "section": "5.3 Quick tips to improve your scripts",
    "text": "5.3 Quick tips to improve your scripts\n\n5.3.1 Do not control program flow with comments\n\nthis is error prone and makes it difficult or impossible to automate\nUse if/else statements instead\n\nBad:\n# Download the file\n#filename <- \"data.tsv\"\n#url <- \"http:://example.com/data.tsv\"\n#download.file(url, filename)\n\n# Read in to a data.frame\ndf <- read.delim(\"data.tsv\", sep=\"\\t\")\nGood:\nfilename <- \"data.tsv\"\nurl <- \"http:://example.com/data.tsv\"\n\nif (!file.exists(filename)) {\n  download.file(url, filename)\n}\ndf <- read.delim(filename)"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-3",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-3",
    "title": "1  Some data best practices",
    "section": "5.4 Quick tips to improve your scripts",
    "text": "5.4 Quick tips to improve your scripts\n\n5.4.1 Use a consistent style\n\nPick a style guide and stick with it\nIf using R, the styler package can automatically cleanup poorly formatted code\nIf using Python, black is a highly opinionated formatter\nDon’t use right hand assignment\n\nBad:\nflights|>group_by(dest)|> summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |> \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-> flight_plot\nGood:\nflight_plot <- flights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-4",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-4",
    "title": "1  Some data best practices",
    "section": "5.5 Quick tips to improve your scripts",
    "text": "5.5 Quick tips to improve your scripts\n\n5.5.1 Use a consistent style\n\nPick a style guide and stick with it\nIf using R, the styler package can automatically cleanup poorly formatted code\nIf using Python, black is a highly opinionated formatter\nDon’t use right hand assignment\n\nBad:\nflights|>group_by(dest)|> summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |> \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-> flight_plot\nGood:\nflight_plot <- flights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()"
  },
  {
    "objectID": "data-best-practices.html#a-typical-workflow",
    "href": "data-best-practices.html#a-typical-workflow",
    "title": "1  Some data best practices",
    "section": "6.1 A typical workflow",
    "text": "6.1 A typical workflow\n\nA common practice in academic writing is for the lead author to send successive versions of a manuscript to coauthors to collect feedback, which is returned as changes to the document, comments on the document, plain text in email, or a mix of all 3. This allows coauthors to use familiar tools but results in a lot of files to keep track of and a lot of tedious manual labor to merge comments to create the next master version.\n\nInstead of an email based workflow, we should aim to mirror best practices developed for data management and software development\nWe want to:\n\nEnsure text is accessible to yourself and others now and in the future by making a single accessible master document\nMake it easy to track and combine changes from multiple authors\nAvoid duplication and manual entry of references, figures, tables, etc.\nMake it easy to regenerate the final, most updated version and share with collaborators and submit\nMore important than the type of workflow is getting everyone to agree on the workflow chosen and how changes will be tracked"
  },
  {
    "objectID": "data-best-practices.html#workflow-1-single-online-master-document",
    "href": "data-best-practices.html#workflow-1-single-online-master-document",
    "title": "1  Some data best practices",
    "section": "6.2 Workflow 1: single online master document",
    "text": "6.2 Workflow 1: single online master document\n\nKeep a single online master document that all authors can collaborate on\nThe tool should be able to track changes, automatically manage references, track version changes.\nTwo candidates are Google Docs and Microsoft Office online\n\nHere’s a good blog post about using Google Docs for scientific writing\n\nOf course, some collaborators will insist on using familiar desktop based tools which will require saving these files in a versioned doc/ directory and manually merging the changes\n\nPandoc can be useful for these kinds of document merging"
  },
  {
    "objectID": "data-best-practices.html#workflow-2-text-documents-under-version-control",
    "href": "data-best-practices.html#workflow-2-text-documents-under-version-control",
    "title": "1  Some data best practices",
    "section": "6.3 Workflow 2: Text documents under version control",
    "text": "6.3 Workflow 2: Text documents under version control\n\nWrite papers in a text based format like \\(\\LaTeX\\) or markdown managing references in a .bib file and tracking changes via git\nThen convert these plain text documents to a pdf or Word doc for final submission with Pandoc\nMathematics, physics, and astronomy have been doing it this way for decades\nThis of course requires everyone to learn a typsetting language and be able to use version control tools…\n\nWhat is the difference between plain text and word processing?\nText editors?\nWhat does it mean to compile a document?\nBIBTex?\nGit/ Github?\n\n\nthis is all probably a bit too much"
  },
  {
    "objectID": "data-best-practices.html#summary-1",
    "href": "data-best-practices.html#summary-1",
    "title": "1  Some data best practices",
    "section": "7.1 Summary",
    "text": "7.1 Summary\n\nData management\n\nSave all raw data and don’t modify it\nKeep good backups and make sure they work. 3-2-1 rule\nUse consistent, meaningful filenames that make sense to computers and reflect their content or function\nCreate analysis friendly data\nWork with/ save data in the format that it is best suited to\n\nProject Organization\n\nUse a consistent, well-defined project structure across all projects\nGive each project a consistent and meaningful name\nUse the structure of the project to organize where files go\n\nTracking Changes\n\nKeep changes small and save changes frequently\nIf manually tracking changes do so in a logical location in a plain text document\nUse a version control system\n\nSoftware\n\nWrite a short description at the top of every script about what the script does and how to use it\nDecompose programs into functions. Don’t Repeat Yourself\nGive functions and variables meaningful names\nUse statements for control flow instead of comments\nUse a consistent style of coding. Use a code styler\n\nManuscripts\n\nPick a workflow that everyone agrees on and keep a single, active collaborative document using either Microsoft Online or Google Docs"
  },
  {
    "objectID": "data-best-practices.html#resources-1",
    "href": "data-best-practices.html#resources-1",
    "title": "1  Some data best practices",
    "section": "8.1 Resources",
    "text": "8.1 Resources\n\n8.1.1 Best practices\n\nGood Enough Practices in Scientific Computing: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510#sec004\nBackups: https://www.cisa.gov/sites/default/files/publications/data_backup_options.pdf\nFile naming: https://library.si.edu/sites/default/files/filenamingorganizing_20210609.pdf\nTidy data: https://www.jstatsoft.org/article/view/v059i10\nFrank Harrell’s R Workflow for Reproducible Analysis: https://hbiostat.org/rflow/\n\nFH blog is also really good for statistical concepts: https://www.fharrell.com/\n\n\n\n\n8.1.2 Bioinformatics\n\nBioinformatics Data Skills: https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/\nBioStars for bioinfo questions: https://www.biostars.org/\nBioconductor common workflows: https://bioconductor.org/packages/release/BiocViews.html#___Workflow\n\n\n\n8.1.3 Proficiency with computational tools\n\nMIT Missing Semester: https://missing.csail.mit.edu/\n\nReally, check this one out\n\n\n\n\n8.1.4 R\n\nR for Data Science: https://r4ds.hadley.nz/\nAdvanced R: https://adv-r.hadley.nz/\nfasteR (base): https://github.com/matloff/fasteR\nEfficient R Programming: https://bookdown.org/csgillespie/efficientR/\nR performance tips: https://peerj.com/preprints/26605.pdf\nR Inferno: https://www.burns-stat.com/documents/books/the-r-inferno/\nIntroduction to data science: https://rafalab.github.io/dsbook-part-1/\nAdvanced data science: https://rafalab.github.io/dsbook-part-2/"
  },
  {
    "objectID": "r-basics.html#introduction-to-programming-in-r",
    "href": "r-basics.html#introduction-to-programming-in-r",
    "title": "2  R bootcamp",
    "section": "2.1 Introduction to programming in R",
    "text": "2.1 Introduction to programming in R\nThere are tons of great resources for learning R. R for Data Science is probably the most popular resource for new useRs to get up to speed with slicing and dicing data in R. The R for Data Science book, however, is taught from the perspective of the Tidyverse. The Tidyverse is an opinionated set of packages and functions that help users perform data manipulations primarily on data.frames. While these packages and functions can be great for experienced users by providing ergonomic and consistent interfaces for data.frame manipulation, it is my personal belief that new users should first learn the base language, especially if their goal is to perform bioinformatics analysis.\nBioinformatics tools rely heavily on subsetting and matrix manipulations. In my experience, users who start learning R using only function from the Tidyverse have a difficult time understanding matrix manipulations and subsetting operations common in bioinformatics workflows. This becomes especially important when using SummarizedExperiments - the backbone of many bioinformatics data structures in R.\nFor this reason, we’re going to focus on learning R from the ground up using functions that exist primarily in the base language. A great resource for learning base R quickly is Norm Matloff’s fasteR which can be found here."
  },
  {
    "objectID": "data-best-practices.html#large-files",
    "href": "data-best-practices.html#large-files",
    "title": "1  Some data best practices",
    "section": "2.7 Large files",
    "text": "2.7 Large files\nYou’ll probably be dealing with files on the order of 10s of GBs. You do not want to be copying these files from one place to another. This increases confusion and runs the risk of introducing errors. Instead avoid making copies of large local files or persistent databases and simply link to the files.\nYou can use use soft links. A powerful way of finding an linking files can be done with find\n# Link all fastq files to a local directory\nfind /path/to/fq/files -name \"*.fq.gz\") -exec ln -s {} . \\;\nIf using R, you can also sometimes specify a URL in place of a file path for certain functions.\n# Avoid downloading a large GTF file - reads GTF directly into memory\nurl <- \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/gencode.v44.annotation.gtf.gz\"\ngtf <- rtracklayer::import(url)"
  },
  {
    "objectID": "data-best-practices.html#file-types-and-file-names",
    "href": "data-best-practices.html#file-types-and-file-names",
    "title": "1  Some data best practices",
    "section": "2.1 File types and file names",
    "text": "2.1 File types and file names\nAs a data scientist you’ll be dealing with a lot of files but have you ever considered what a file is? Files come in all shapes and formats. Some are very application specific and require specialized programs to open. For example, consider DICOM files that are used to store and manipulate radiology data. Luckily, in bioinformatics we tend to deal mainly with simple, plain text files, most often. Plain text files are typically designed to be both human and machine-readable. If you have the choice of saving any data, you should know that some formats will make your life easier. Certain file formats like TXT, CSV, TSV, JSON, and YAML are standard plain text file formats that are easy to share and easy to open and manipulate. Because of this, you should prefer to store your data in machine-readable formats. Avoid .xlsx files for storing data. Prefer TXT, CSV, TSV, JSON, YAML, and HDF5.\nIf you have very large text files then you can use compression utilities to save space. Most bioinformatics software is designed to work well with gzip compressed data. gzip is a relatively old compression format. You could also consider using xz as a means to compress your data - just know that xz compression is less supported across tools."
  },
  {
    "objectID": "data-best-practices.html#file-naming",
    "href": "data-best-practices.html#file-naming",
    "title": "1  Some data best practices",
    "section": "2.2 File naming",
    "text": "2.2 File naming\nFile naming is important but often overlooked. You want your files to be named logically and communicate their contents. You also want your files to be named in a way that a computer can easily read. For example, spaces in filenames are a royal pain when manipulating files on the command line.\nTo ensure filenames are computer friendly, don’t use spaces in filenames. Use only letters, numbers, and “-” “_” and “.” as delimiters. For example:\n# Bad\ndata for jozef.txt\n\n# Okay\ndata-for-jozef.txt\n\n# Best\n2023-11-09_repetitive-element-counts.txt\nQuiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone"
  },
  {
    "objectID": "data-best-practices.html#best-practices",
    "href": "data-best-practices.html#best-practices",
    "title": "1  Some data best practices",
    "section": "7.1 Best practices",
    "text": "7.1 Best practices\n\nGood Enough Practices in Scientific Computing: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510#sec004\nBackups: https://www.cisa.gov/sites/default/files/publications/data_backup_options.pdf\nFile naming: https://library.si.edu/sites/default/files/filenamingorganizing_20210609.pdf\nTidy data: https://www.jstatsoft.org/article/view/v059i10\nFrank Harrell’s R Workflow for Reproducible Analysis: https://hbiostat.org/rflow/\n\nFH blog is also really good for statistical concepts: https://www.fharrell.com/"
  },
  {
    "objectID": "data-best-practices.html#bioinformatics",
    "href": "data-best-practices.html#bioinformatics",
    "title": "1  Some data best practices",
    "section": "7.2 Bioinformatics",
    "text": "7.2 Bioinformatics\n\nBioinformatics Data Skills: https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/\nBioStars for bioinfo questions: https://www.biostars.org/\nBioconductor common workflows: https://bioconductor.org/packages/release/BiocViews.html#___Workflow"
  },
  {
    "objectID": "data-best-practices.html#proficiency-with-computational-tools",
    "href": "data-best-practices.html#proficiency-with-computational-tools",
    "title": "1  Some data best practices",
    "section": "7.3 Proficiency with computational tools",
    "text": "7.3 Proficiency with computational tools\n\nMIT Missing Semester: https://missing.csail.mit.edu/\n\nReally, check this one out"
  },
  {
    "objectID": "data-best-practices.html#r",
    "href": "data-best-practices.html#r",
    "title": "1  Some data best practices",
    "section": "7.4 R",
    "text": "7.4 R\n\nR for Data Science: https://r4ds.hadley.nz/\nAdvanced R: https://adv-r.hadley.nz/\nfasteR (base): https://github.com/matloff/fasteR\nEfficient R Programming: https://bookdown.org/csgillespie/efficientR/\nR performance tips: https://peerj.com/preprints/26605.pdf\nR Inferno: https://www.burns-stat.com/documents/books/the-r-inferno/\nIntroduction to data science: https://rafalab.github.io/dsbook-part-1/\nAdvanced data science: https://rafalab.github.io/dsbook-part-2/"
  },
  {
    "objectID": "data-best-practices.html#quick-note-todo",
    "href": "data-best-practices.html#quick-note-todo",
    "title": "1  Some data best practices",
    "section": "1.1 Quick note (TODO)",
    "text": "1.1 Quick note (TODO)\nCitations incoming. Please note that much of this document does not contain original material. References are listed at the end of the chapter (I still need to get my .bib files in order to give proper attribution)."
  }
]