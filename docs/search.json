[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2025 Coriell Bioinformatics Internship",
    "section": "",
    "text": "Welcome!\nWe’re excited to have you as an intern and we hope that you’ll learn a lot with us this summer. We’ll use this book as a kind of reference manual for the things we think you should have a little background knowledge about before starting on an interesting project.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "data-best-practices.html",
    "href": "data-best-practices.html",
    "title": "‘Best’ practices for data projects",
    "section": "",
    "text": "Quick note (TODO)\nScience requires reproducibility, not only to ensure that your results are generalizable but also to make your life easier. One overlooked aspect of of learning data science is creating consistent and clear organization of your projects and learning how to keep your data safe and easily searchable.\nCitations incoming. Please note that much of this document does not contain original material. References are listed at the end of the chapter (I still need to get my .bib files in order to give proper attribution).",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#project-organization",
    "href": "data-best-practices.html#project-organization",
    "title": "1  Organizing projects",
    "section": "1.1 Project organization",
    "text": "1.1 Project organization\nScience requires reproducibility. not only to ensure that your results are generalizable but also to make your own life easier. One overlooked aspect of of learning data science is creating consistent and clear organization of your projects."
  },
  {
    "objectID": "data-best-practices.html#why-care-about-data-management",
    "href": "data-best-practices.html#why-care-about-data-management",
    "title": "‘Best’ practices for data projects",
    "section": "Why care about data management?",
    "text": "Why care about data management?\nComputing is now an essential part of research. This is outlined beautifully in the paper, “All biology is computational biology” by Florian Markowetz. Data is getting bigger and bigger and we need to be equipped with the tools for storing, manipulating, and communicating insights derived from it. However, most researchers are never taught good computational practices. Computational best practices are imperitive. Implementing best (or good enough) practices can improve reproducibility, ensure correctness, and increase efficiency.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#topics",
    "href": "data-best-practices.html#topics",
    "title": "1  Organizing projects",
    "section": "1.2 Topics",
    "text": "1.2 Topics\n\nData management\nProject organization\nTracking changes\nSoftware\nManuscripts"
  },
  {
    "objectID": "data-best-practices.html#save-and-lock-all-raw-data",
    "href": "data-best-practices.html#save-and-lock-all-raw-data",
    "title": "‘Best’ practices for data projects",
    "section": "Save and lock all raw data",
    "text": "Save and lock all raw data\nKeep raw data in its unedited form. This includes not making changes to filenames. In bioinformatics, it’s common to get data from a sequencing facility with incomprehensible filenames. Don’t fall victim to the temptation of changing these filenames! Instead, it’s much better to keep the filenames exactly how they were sent to you and simply create a spreadsheet that maps the files to their metadata. In the case of a sample mix-up, it’s much easier to make a change to a row in a spreadsheet then to track down all of the filenames that you changed and ensure they’re correctly modified.\nOnce you have your raw data, you don’t want the raw data to change in any way that is not documented by code. To ensure this, you can consider changing file permissions to make the file immutable (unchangable). Using bash, you can change file permissions with:\nchattr +i myfile.txt\nIf you’re using Excel for data analysis, lock the spreadsheet with the raw data and only make references to this sheet when performing calculations.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#backups",
    "href": "data-best-practices.html#backups",
    "title": "‘Best’ practices for data projects",
    "section": "Backups",
    "text": "Backups\n\nThere are two types of people, those who do backups and those who will do backups.\n\nThe following are NOT backup solutions:\n\nCopies of data on the same disk\nDropbox/Google Drive\nRAID arrays\n\nAll of these solutions mirror the data. Corruption or ransomware will propagate. For example, if you corrupt a file on your local computer and then push that change to DropBox then the file on DropBox is now also corrupted. I’m sure some of these cloud providers have version controlled files but it’s better to just avoid the problem entirely by keeping good backups.\n\nUse the 3-2-1 rule:\n\nKeep 3 copies of any important file: 1 primary and 2 backups.\nKeep the files on 2 different media types to protect against different types of hazards.\nStore 1 copy offsite (e.g., outside your home or business facility).\n\nA backup is only a backup if you can restore the files!",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#files",
    "href": "data-best-practices.html#files",
    "title": "1  Organizing projects",
    "section": "2.4 Files",
    "text": "2.4 Files\n\nStore data in standard, machine readable formats\n\nTXT, CSV, TSV, JSON, YAML, HDF5\nLarge text files should be compressed\n\nxz is better than gzip. We should all use xz\n\n\nEnsure filenames are computer friendly\n\nNo spaces\nUse only letters, numbers, and “-” “_” and “.” as delimiters\n\n\n# Bad\ndata for jozef.txt\n\n# Okay\ndata-for-jozef.txt\n\n# Best\n2023-11-09_repetitive-element-counts.txt\nQuiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone"
  },
  {
    "objectID": "data-best-practices.html#quiz",
    "href": "data-best-practices.html#quiz",
    "title": "1  Organizing projects",
    "section": "2.4 Quiz",
    "text": "2.4 Quiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone"
  },
  {
    "objectID": "data-best-practices.html#more-on-filenames",
    "href": "data-best-practices.html#more-on-filenames",
    "title": "‘Best’ practices for data projects",
    "section": "More on filenames",
    "text": "More on filenames\nIt cannot be stressed enough how important filenames can be for an analysis. To get the most out of your files and to avoid catastrpohic failures, you should stick to some basic principles for naming files. First, use consistent and unique identifiers across all files that you generate for an experiment. For example, if you’re conducting a study that has both RNA-seq and ATAC-seq data performed on the same subjects, don’t name the files from the RNA-seq experiment subject1.fq.fz and the files from the ATAC-seq experiment control_subject1.fq.gz if they refer to the same sample. For small projects, it’s fairly easy to create consistent and unique IDs for each subject. For large projects unique random IDs can be used.\nFor example, the following filenames would be bad:\nsubject1-control_at_96hr1.txt\ns1_ctl-at-4days_2.txt\ns2TRT4d1.txt\nsbj2_Treatment_4_Days_Replicate_2.txt\nInstead, look at these filenames.\nsubject1_control_4days_rep1.txt\nsubject1_control_4days_rep2.txt\nsubject2_treatment_4days_rep1.txt\nsubject2_treatment_4days_rep2.txt\nThese are better. Why are they better? They are consistent. The delimiter is consistent between the words (“_“) and each of the words represents something meaningful about the sample. These filenames also do not contain any spaces and can easily be parsed automatically.\nFile naming best practices also apply to naming executable scripts. The name of the file should describe the function of the script. For example,\n01_align_with_STAR.sh\nis better than simply naming the file\n01_script.sh\nPro-tip\nOne easy way to create unique random IDs for a large project is to concatenate descriptions and take the SHA/MDA5 hashsum.\necho \"subject1_control_4days_rep1\" | sha256\n# 57f458a294542b2ed6ac14ca64d3c8e4599eed7a\n\necho \"subject1_control_4days_rep2\" | shasum\n# b6ea9d729e57cce68b37de390d56c542bc17dea6",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#create-analysis-freindly-data---tidy-data",
    "href": "data-best-practices.html#create-analysis-freindly-data---tidy-data",
    "title": "‘Best’ practices for data projects",
    "section": "Create analysis freindly data - tidy data",
    "text": "Create analysis freindly data - tidy data\nThe term tidy data was defined by Hadley Wickham to describe data which is amenable to downstream analysis. Most people are familiar with performing a quick and dirty data analysis in a program like Excel. You may have also used some of Excel’s fancy features for coloring cells, adding bold and underlines to text, and formatting cells with other decorations. All of this tends to just be extra fluff. If you format you data properly then it will be much easier to perform downstream analysis on and will not require the use of extra decorations. This is true even in Excel!\nTo conform to the requirements of being tidy, the data should follow some simple principles:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nDon’t cram two variables into one value. e.g. “male_treated”.\nDon’t embed units into data. e.g. “3.4kg”. Instead, put in column name e.g. “weight_kg” = 3.4\n\n\n\nOnce your data is in this format, it can easily be read into downstream programs like R, or parsed with command line text editing programs like sed.\nThe iris dataset in R provides a classic example of this format\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nPro-tip\ndata.frame like objects can be stored and retrieved efficiently using the Apache Arrow format instead of CSV files:\ndf &lt;- arrow::read_parquet(\"path/to/file.parquet\")\narrow::write_parquet(df, \"path/to/different_file.parquet\")\nsee R arrow for more details. There’s also the nanoparquet package which provides a light weight reader/writer for parquet files.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#untidy-data",
    "href": "data-best-practices.html#untidy-data",
    "title": "‘Best’ practices for data projects",
    "section": "Untidy(?) data",
    "text": "Untidy(?) data\nSome data formats are not amenable to the ‘tidy’ structure, i.e. they’re just not best represented as long tables. For example, large/sparse matrices, geo-spatial data, R objects, etc.the lesson here is to store data in the format that is most appropriate for the data. For example, don’t convert a matrix to a long format and save as a tsv file! Save it as an .rds file instead. Large matrices can also be efficiently stored as HDF5 files. Sparse matrices can be saved and accessed eficiently using the Matrix package in R. And if you are accessing the same data often, consider storing as a SQLite database and accessing with dbplyr or sqlalchemy in Python. The main point is don’t force data into a format that you’re familiar with only because you’re familiar with that format. This will often lead to large file sizes and inefficient performance.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#record-all-steps-used-to-generate-the-data",
    "href": "data-best-practices.html#record-all-steps-used-to-generate-the-data",
    "title": "‘Best’ practices for data projects",
    "section": "Record all steps used to generate the data",
    "text": "Record all steps used to generate the data\nAlways document all steps you used to generate the data that’s present in your projects. This can be as simple as a README with some comments and a wget command or as complex as a snakemake workflow. The point is, be sure you can track down the exact source of every file that you created or downloaded.\nFor example, a README documenting the creation of the files needed to generate a reference index might look like:\nTranscripts:\nwget http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/gencode.v38.transcripts.fa.gz\n\nPrimary Assembly:\nwget http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/GRCh38.primary_assembly.genome.fa.gz\n\nCreate concatenated transcripts + genome for salmon (i.e. gentrome):\ncat gencode.v38.transcripts.fa.gz GRCh38.primary_assembly.genome.fa.gz &gt; gentrome.fa.gz\n\nCreate decoys file for salmon:\ngrep \"&gt;\" &lt;(gunzip -c GRCh38.primary_assembly.genome.fa.gz) | cut -d \" \" -f 1 &gt; decoys.txt\nsed -i.bak -e 's/&gt;//g' decoys.txt\nFor more complicated steps include a script. e.g. creating a new genome index, subsetting BAM files, accessing data from NCBI, etc.\nPro-tip\nA simple way to build a data pipeline that is surprisingly robust is just to create scripts for each step and number them in the order that they should be executed.\n01_download.sh\n02_process.py\n03_makeFigures.R\nYou can also include a runner script that will execute all of the above. Or, for more consistent workflows, use a workflow manager like Nextflow, Snakemake, WDL, or good ole’ GNU Make",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#look-familiar",
    "href": "data-best-practices.html#look-familiar",
    "title": "‘Best’ practices for data projects",
    "section": "Look familiar?",
    "text": "Look familiar?",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#project-structure",
    "href": "data-best-practices.html#project-structure",
    "title": "‘Best’ practices for data projects",
    "section": "Project structure",
    "text": "Project structure\nOne of the most useful changes that you can make to your workflow is the create a consistent folder structure for all of your analyses and stick with it. Coming up with a consistent and generalizable structure can be challenging at first but some general guidelines are presented here and here\nFirst of all, when beginning a new project, you should have some way of naming your projects. One good way of naming projects is to each project a descriptive name and append the date the project was started. For example, brca_rnaseq_2023-11-09/ is better than rnaseq_data/. In six months when a collaborator wants their old BRCA data re-analyzed you’ll thank yourself for timestamping the project folder and giving it a descriptive name.\nMy personal structure for every project looks like:\nyyyymmdd_project-name/\n├── data\n├── doc\n├── README\n├── results\n│   ├── data-files\n│   ├── figures\n│   └── rds-files\n└── scripts\n\n\nPrefixing the project directory with the ISO date allows for easy sorting by date\na README text file is present at the top level of the directory with a short description about the project and any notes or updates\ndata/ should contain soft links to any raw data or the results of downloading data from an external source\ndoc/ contains metadata documents about the samples or other metadata information about the experiment\nresults/ contains only data generated within the project. It has sub-directories for figures/, data-files/ and rds-files/. If you have a longer or more complicated analysis then add sub-directories indicating which script generated the results.\nscripts/ contains all analysis scripts numbered in their order of execution. Synchronize the script names with the results they produce.\n\n\nA more complex example\nyyyymmdd_project-name/\n├── data\n│   ├── 00_fastq\n│   ├── 01_fastqc\n│   ├── 02_trimmed\n│   └── 03_quants\n├── delivered\n│   └── 2022-09-11_analysis-results.zip\n├── doc\n│   └── sample-metadata.txt\n├── project.Rproj\n├── README\n├── results\n│   ├── data-files\n│   │   ├── 04\n│   │   │   └── differential-expression.tsv.gz\n│   │   └── 05\n│   │       └── gsea-results.tsv.gz\n│   ├── figures\n│   │   ├── 04\n│   │   │   ├── ma-plots.png\n│   │   │   └── volcano-plots.png\n│   │   └── 05\n│   │       └── enrichment-plots.png\n│   └── rds-files\n│       └── 04\n│           └── SummarizedExperiment.rds\n└── scripts\n    ├── 01_fastqc.sh\n    ├── 02_trim.sh\n    ├── 03_quant.sh\n    ├── 04_differential_expression.Qmd\n    └── 05_gsea.Qmd\nPro-tip\nIf using Rstudio, include an .Rproj file at the top level of your directory. Doing this enables you to use the here package to reference data within your project in a relative fashion. For example, you can more easily save data with:\nplot_volcano(...)\nggsave(here(\"data\", \"results\", \"figures\", \"04\", \"volcano-plots.png\"))",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#project-structure-1",
    "href": "data-best-practices.html#project-structure-1",
    "title": "1  Some data best practices",
    "section": "3.3 Project structure",
    "text": "3.3 Project structure\nA fuller example might look more like:\nyyyymmdd_project-name/\n├── data\n│   ├── 00_fastq\n│   ├── 01_fastqc\n│   ├── 02_trimmed\n│   └── 03_quants\n├── delivered\n│   └── 2022-09-11_analysis-results.zip\n├── doc\n│   └── sample-metadata.txt\n├── project.Rproj\n├── README\n├── results\n│   ├── data-files\n│   │   ├── 04\n│   │   │   └── differential-expression.tsv.gz\n│   │   └── 05\n│   │       └── gsea-results.tsv.gz\n│   ├── figures\n│   │   ├── 04\n│   │   │   ├── ma-plots.png\n│   │   │   └── volcano-plots.png\n│   │   └── 05\n│   │       └── enrichment-plots.png\n│   └── rds-files\n│       └── 04\n│           └── SummarizedExperiment.rds\n└── scripts\n    ├── 01_fastqc.sh\n    ├── 02_trim.sh\n    ├── 03_quant.sh\n    ├── 04_differential_expression.Qmd\n    └── 05_gsea.Qmd\nPro-tip\nIf using Rstudio, include an .Rproj file at the top level of your directory. Doing this enables you to use the here package to reference data within your project in a relative fashion. For example, you can more easily save data with:\nplot_volcano(...)\nggsave(here(\"data\", \"results\", \"figures\", \"04\", \"volcano-plots.png\"))"
  },
  {
    "objectID": "data-best-practices.html#manual-version-control",
    "href": "data-best-practices.html#manual-version-control",
    "title": "‘Best’ practices for data projects",
    "section": "Manual version control",
    "text": "Manual version control\nVersion control refers to the practice of tracking changes in files and data over their lifetime. You should always track any changes made to your project over the entire life of the project. This can be done either manually or using a dedicated version control system. If doing this manually, add a file called “CHANGELOG.md” in your docs/ directory and add detailed notes in reverse chronological order.\nFor example:\n## 2016-04-08\n\n* Switched to cubic interpolation as default.\n* Moved question about family's TB history to end of questionnaire.\n\n## 2016-04-06\n\n* Added option for cubic interpolation.\n* Removed question about staph exposure (can be inferred from blood test results).\n\nIf you make a significant change to the project, copy the whole directory, date it, and store it such that it will no longer be modified. Copies of these old projects can be compressed and saved with tar + xz compression\ntar -cJvf old.20231109_myproject.tar.xz myproject/`",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#version-control-with-git",
    "href": "data-best-practices.html#version-control-with-git",
    "title": "‘Best’ practices for data projects",
    "section": "Version control with git",
    "text": "Version control with git\n\ngit is probably the de facto version control system in use today for tracking changes across software projects. You should strive to learn and use git to track your projects. Version control systems allow you to track all changes, comment on why changes were made, create parallel branches, and merge existing ones.\ngit is primarily used for source code files. Microsoft Office files and PDFs can be stored with Github but it’s hard to track changes. Rely on Microsoft’s “Track Changes” instead and save frequently.\nIt’s not necessary to version control raw data (back it up!) since it shouldn’t change. Likewise, backup intermediate data and version control the scripts that made it.\nFor a quick primer on Git and GitHub check out the book Happy Git with R or The Official GitHub Training Manual Anyone in the lab can join the coriell-research organization on Github and start tracking their code\nBe careful committing sensitive information to GitHub",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts",
    "title": "‘Best’ practices for data projects",
    "section": "Quick tips to improve your scripts",
    "text": "Quick tips to improve your scripts\n\nPlace a description at the top of every script\nThe description should indicate who the author is. When the code was created. A short description of what the expected inputs and outputs are along with how to use the code. You three months from now will appreciate it when you need to revisit your analysis\nFor example:\n#!/usr/bin/env python3\n# Gennaro Calendo\n# 2023-11-09\n# \n# This scripts performs background correction of all images in the \n#  user supplied directory\n#\n# Usage ./correct-bg.py --input images/ --out_dir out_directory\n#\nfrom image_correction import background_correct\n\nfor img in images:\n  img = background_correct(img)\n  save_image(img, \"out_directory/corrected.png\")\n  \n\n\nDecompose programs into functions\nFunctions make it easier to reason about your code, spot errors, and make changes. This also follows the Don’t Repeat Yourself principle aimed at reducing repetition by replacing it with abstractions that are more stable\nCompare this chunk of code that rescales values using a min-max function (0-1)\ndf &lt;- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\nto this function which does the same thing\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf &lt;- lapply(df, rescale01)\nWhich is easier to read? Which is easier to debug? Which is more efficient?\n\n\nGive functions and variables meaningful names\n\nPrograms are written for people and then computers\nUse variable and function names that are meaningful and correct\nKeep names consistent. Use either snake_case or camelCase but try not to mix both\n\nBad:\nlol &lt;- 1:100\n\nmydata &lt;- data.frame(x = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nf &lt;- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\nBetter:\nages &lt;- 1:100\n\nbioinfo_names &lt;- data.frame(Name = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nplotScatterPlot &lt;- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\n\n\nDo not control program flow with comments\n\nthis is error prone and makes it difficult or impossible to automate\nUse if/else statements instead\n\nBad:\n# Download the file\n#filename &lt;- \"data.tsv\"\n#url &lt;- \"http:://example.com/data.tsv\"\n#download.file(url, filename)\n\n# Read in to a data.frame\ndf &lt;- read.delim(\"data.tsv\", sep=\"\\t\")\nGood:\nfilename &lt;- \"data.tsv\"\nurl &lt;- \"http:://example.com/data.tsv\"\n\nif (!file.exists(filename)) {\n  download.file(url, filename)\n}\ndf &lt;- read.delim(filename)\n\n\nUse a consistent style\nPick a style guide and stick with it. If using R, the styler package can automatically clean up poorly formatted code. If using Python, black is a highly opinionated formatter that is pretty popular. Although, I think ruff is currently all the rage with the Pythonistas these days.\nBad:\nflights|&gt;group_by(dest)|&gt; summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |&gt; \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-&gt; flight_plot\nGood:\nflight_plot &lt;- flights |&gt; \n  group_by(dest) |&gt; \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |&gt; \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()\n\n\nDon’t use right hand assignment\nThis is R specific. I’ve seen this pop up with folks who are strong tidyverse adherents. I get it, that’s the direction of the piping operator. However, this right-hand assignment flies in the face of basically every other programming language, and since code is primarily read rather than executed, it’s much harder to scan a codebase and understand the variable assignment when the assignments can be anywhere in the pipe!\nDon’t do this\ndata |&gt; \n  select(...) |&gt; \n  filter(...) |&gt; \n  group_by(...) |&gt; \n  summarize(...) -&gt; by_group\nIt’s much easier to look down a script and see that by_group is created by all of the piped operations when assigned normally.\nby_group &lt;- data |&gt; \n  select(...) |&gt; \n  filter(...) |&gt; \n  group_by(...) |&gt; \n  summarize(...)",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-1",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-1",
    "title": "1  Some data best practices",
    "section": "5.2 Quick tips to improve your scripts",
    "text": "5.2 Quick tips to improve your scripts\n\n5.2.1 Give functions and variables meaningful names\n\nPrograms are written for people and then computers\nUse variable and function names that are meaningful and correct\nKeep names consistent. Use either snake_case or camelCase but try not to mix both\n\nBad:\nlol <- 1:100\n\nmydata <- data.frame(x = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nf <- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\nBetter:\nages <- 1:100\n\nbioinfo_names <- data.frame(Name = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nplotScatterPlot <- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-2",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-2",
    "title": "1  Some data best practices",
    "section": "5.3 Quick tips to improve your scripts",
    "text": "5.3 Quick tips to improve your scripts\n\n5.3.1 Do not control program flow with comments\n\nthis is error prone and makes it difficult or impossible to automate\nUse if/else statements instead\n\nBad:\n# Download the file\n#filename <- \"data.tsv\"\n#url <- \"http:://example.com/data.tsv\"\n#download.file(url, filename)\n\n# Read in to a data.frame\ndf <- read.delim(\"data.tsv\", sep=\"\\t\")\nGood:\nfilename <- \"data.tsv\"\nurl <- \"http:://example.com/data.tsv\"\n\nif (!file.exists(filename)) {\n  download.file(url, filename)\n}\ndf <- read.delim(filename)"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-3",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-3",
    "title": "1  Some data best practices",
    "section": "5.4 Quick tips to improve your scripts",
    "text": "5.4 Quick tips to improve your scripts\n\n5.4.1 Use a consistent style\n\nPick a style guide and stick with it\nIf using R, the styler package can automatically cleanup poorly formatted code\nIf using Python, black is a highly opinionated formatter\nDon’t use right hand assignment\n\nBad:\nflights|>group_by(dest)|> summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |> \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-> flight_plot\nGood:\nflight_plot <- flights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-4",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-4",
    "title": "1  Some data best practices",
    "section": "5.5 Quick tips to improve your scripts",
    "text": "5.5 Quick tips to improve your scripts\n\n5.5.1 Use a consistent style\n\nPick a style guide and stick with it\nIf using R, the styler package can automatically cleanup poorly formatted code\nIf using Python, black is a highly opinionated formatter\nDon’t use right hand assignment\n\nBad:\nflights|>group_by(dest)|> summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |> \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-> flight_plot\nGood:\nflight_plot <- flights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()"
  },
  {
    "objectID": "data-best-practices.html#a-typical-workflow",
    "href": "data-best-practices.html#a-typical-workflow",
    "title": "1  Some data best practices",
    "section": "6.1 A typical workflow",
    "text": "6.1 A typical workflow\n\nA common practice in academic writing is for the lead author to send successive versions of a manuscript to coauthors to collect feedback, which is returned as changes to the document, comments on the document, plain text in email, or a mix of all 3. This allows coauthors to use familiar tools but results in a lot of files to keep track of and a lot of tedious manual labor to merge comments to create the next master version.\n\nInstead of an email based workflow, we should aim to mirror best practices developed for data management and software development\nWe want to:\n\nEnsure text is accessible to yourself and others now and in the future by making a single accessible master document\nMake it easy to track and combine changes from multiple authors\nAvoid duplication and manual entry of references, figures, tables, etc.\nMake it easy to regenerate the final, most updated version and share with collaborators and submit\nMore important than the type of workflow is getting everyone to agree on the workflow chosen and how changes will be tracked"
  },
  {
    "objectID": "data-best-practices.html#workflow-1-single-online-master-document",
    "href": "data-best-practices.html#workflow-1-single-online-master-document",
    "title": "1  Some data best practices",
    "section": "6.2 Workflow 1: single online master document",
    "text": "6.2 Workflow 1: single online master document\n\nKeep a single online master document that all authors can collaborate on\nThe tool should be able to track changes, automatically manage references, track version changes.\nTwo candidates are Google Docs and Microsoft Office online\n\nHere’s a good blog post about using Google Docs for scientific writing\n\nOf course, some collaborators will insist on using familiar desktop based tools which will require saving these files in a versioned doc/ directory and manually merging the changes\n\nPandoc can be useful for these kinds of document merging"
  },
  {
    "objectID": "data-best-practices.html#workflow-2-text-documents-under-version-control",
    "href": "data-best-practices.html#workflow-2-text-documents-under-version-control",
    "title": "1  Some data best practices",
    "section": "6.3 Workflow 2: Text documents under version control",
    "text": "6.3 Workflow 2: Text documents under version control\n\nWrite papers in a text based format like \\(\\LaTeX\\) or markdown managing references in a .bib file and tracking changes via git\nThen convert these plain text documents to a pdf or Word doc for final submission with Pandoc\nMathematics, physics, and astronomy have been doing it this way for decades\nThis of course requires everyone to learn a typsetting language and be able to use version control tools…\n\nWhat is the difference between plain text and word processing?\nText editors?\nWhat does it mean to compile a document?\nBIBTex?\nGit/ Github?\n\n\nthis is all probably a bit too much"
  },
  {
    "objectID": "data-best-practices.html#summary-1",
    "href": "data-best-practices.html#summary-1",
    "title": "1  Some data best practices",
    "section": "7.1 Summary",
    "text": "7.1 Summary\n\nData management\n\nSave all raw data and don’t modify it\nKeep good backups and make sure they work. 3-2-1 rule\nUse consistent, meaningful filenames that make sense to computers and reflect their content or function\nCreate analysis friendly data\nWork with/ save data in the format that it is best suited to\n\nProject Organization\n\nUse a consistent, well-defined project structure across all projects\nGive each project a consistent and meaningful name\nUse the structure of the project to organize where files go\n\nTracking Changes\n\nKeep changes small and save changes frequently\nIf manually tracking changes do so in a logical location in a plain text document\nUse a version control system\n\nSoftware\n\nWrite a short description at the top of every script about what the script does and how to use it\nDecompose programs into functions. Don’t Repeat Yourself\nGive functions and variables meaningful names\nUse statements for control flow instead of comments\nUse a consistent style of coding. Use a code styler\n\nManuscripts\n\nPick a workflow that everyone agrees on and keep a single, active collaborative document using either Microsoft Online or Google Docs"
  },
  {
    "objectID": "data-best-practices.html#resources-1",
    "href": "data-best-practices.html#resources-1",
    "title": "1  Some data best practices",
    "section": "8.1 Resources",
    "text": "8.1 Resources\n\n8.1.1 Best practices\n\nGood Enough Practices in Scientific Computing: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510#sec004\nBackups: https://www.cisa.gov/sites/default/files/publications/data_backup_options.pdf\nFile naming: https://library.si.edu/sites/default/files/filenamingorganizing_20210609.pdf\nTidy data: https://www.jstatsoft.org/article/view/v059i10\nFrank Harrell’s R Workflow for Reproducible Analysis: https://hbiostat.org/rflow/\n\nFH blog is also really good for statistical concepts: https://www.fharrell.com/\n\n\n\n\n8.1.2 Bioinformatics\n\nBioinformatics Data Skills: https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/\nBioStars for bioinfo questions: https://www.biostars.org/\nBioconductor common workflows: https://bioconductor.org/packages/release/BiocViews.html#___Workflow\n\n\n\n8.1.3 Proficiency with computational tools\n\nMIT Missing Semester: https://missing.csail.mit.edu/\n\nReally, check this one out\n\n\n\n\n8.1.4 R\n\nR for Data Science: https://r4ds.hadley.nz/\nAdvanced R: https://adv-r.hadley.nz/\nfasteR (base): https://github.com/matloff/fasteR\nEfficient R Programming: https://bookdown.org/csgillespie/efficientR/\nR performance tips: https://peerj.com/preprints/26605.pdf\nR Inferno: https://www.burns-stat.com/documents/books/the-r-inferno/\nIntroduction to data science: https://rafalab.github.io/dsbook-part-1/\nAdvanced data science: https://rafalab.github.io/dsbook-part-2/"
  },
  {
    "objectID": "r-basics.html#introduction-to-programming-in-r",
    "href": "r-basics.html#introduction-to-programming-in-r",
    "title": "R programming basics",
    "section": "Introduction to programming in R",
    "text": "Introduction to programming in R\nThere are tons of great resources for learning R. R for Data Science is probably the most popular resource for new useRs to get up to speed with slicing and dicing data in R. The R for Data Science book, however, is taught from the perspective of the Tidyverse. The Tidyverse is an opinionated set of packages and functions that help users perform data manipulations primarily on data.frames. While these packages and functions can be great for experienced users by providing ergonomic and consistent interfaces for data.frame manipulation, it is my personal belief that new users should first learn the base language, especially if their goal is to perform bioinformatics analysis.\nBioinformatics tools rely heavily on subsetting and matrix manipulations. In my experience, users who start learning R using only function from the Tidyverse have a difficult time understanding matrix manipulations and subsetting operations common in bioinformatics workflows. This becomes especially important when using SummarizedExperiments - the backbone of many bioinformatics data structures in R.\nFor this reason, we’re going to focus on learning R from the ground up using functions that exist primarily in the base language. A great resource for learning base R quickly is Norm Matloff’s fasteR which can be found here."
  },
  {
    "objectID": "data-best-practices.html#large-files",
    "href": "data-best-practices.html#large-files",
    "title": "‘Best’ practices for data projects",
    "section": "Large files",
    "text": "Large files\nYou’ll probably be dealing with files on the order of 10s of GBs. You do not want to be copying these files from one place to another. This increases confusion and runs the risk of introducing errors. Instead avoid making copies of large local files or persistent databases and simply link to the files.\nYou can use use soft links. A powerful way of finding an linking files can be done with find\n# Link all fastq files to a local directory\nfind /path/to/fq/files -name \"*.fq.gz\") -exec ln -s {} . \\;\nIf using R, you can also sometimes specify a URL in place of a file path for certain functions.\n# Avoid downloading a large GTF file - reads GTF directly into memory\nurl &lt;- \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/gencode.v44.annotation.gtf.gz\"\ngtf &lt;- rtracklayer::import(url)",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#file-types-and-file-names",
    "href": "data-best-practices.html#file-types-and-file-names",
    "title": "‘Best’ practices for data projects",
    "section": "File types and file names",
    "text": "File types and file names\nAs a data scientist you’ll be dealing with a lot of files but have you ever considered what a file is? Files come in all shapes and formats. Some are very application specific and require specialized programs to open. For example, consider DICOM files that are used to store and manipulate radiology data. Luckily, in bioinformatics we tend to deal mainly with simple, plain text files, most often. Plain text files are typically designed to be both human and machine-readable. If you have the choice of saving any data, you should know that some formats will make your life easier. Certain file formats like TXT, CSV, TSV, JSON, and YAML are standard plain text file formats that are easy to share and easy to open and manipulate. Because of this, you should prefer to store your data in machine-readable formats. Avoid .xlsx files for storing data. Prefer TXT, CSV, TSV, JSON, YAML, and HDF5.\nIf you have very large text files then you can use compression utilities to save space. Most bioinformatics software is designed to work well with gzip compressed data. gzip is a relatively old compression format. You could also consider using xz as a means to compress your data - just know that xz compression is less supported across tools.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#file-naming",
    "href": "data-best-practices.html#file-naming",
    "title": "‘Best’ practices for data projects",
    "section": "File naming",
    "text": "File naming\nFile naming is important but often overlooked. You want your files to be named logically and communicate their contents. You also want your files to be named in a way that a computer can easily read. For example, spaces in filenames are a royal pain when manipulating files on the command line.\nTo ensure filenames are computer friendly, don’t use spaces in filenames. Use only letters, numbers, and “-” “_” and “.” as delimiters. For example:\n# Bad\ndata for jozef.txt\n\n# Okay\ndata-for-jozef.txt\n\n# Best\n2023-11-09_repetitive-element-counts.txt\nQuiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#best-practices",
    "href": "data-best-practices.html#best-practices",
    "title": "‘Best’ practices for data projects",
    "section": "Best practices",
    "text": "Best practices\n\nGood Enough Practices in Scientific Computing: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510#sec004\nBackups: https://www.cisa.gov/sites/default/files/publications/data_backup_options.pdf\nFile naming: https://library.si.edu/sites/default/files/filenamingorganizing_20210609.pdf\nTidy data: https://www.jstatsoft.org/article/view/v059i10\nFrank Harrell’s R Workflow for Reproducible Analysis: https://hbiostat.org/rflow/\n\nFH blog is also really good for statistical concepts: https://www.fharrell.com/",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#bioinformatics",
    "href": "data-best-practices.html#bioinformatics",
    "title": "‘Best’ practices for data projects",
    "section": "Bioinformatics",
    "text": "Bioinformatics\n\nBioinformatics Data Skills: https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/\nBioStars for bioinfo questions: https://www.biostars.org/\nBioconductor common workflows: https://bioconductor.org/packages/release/BiocViews.html#___Workflow",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#proficiency-with-computational-tools",
    "href": "data-best-practices.html#proficiency-with-computational-tools",
    "title": "‘Best’ practices for data projects",
    "section": "Proficiency with computational tools",
    "text": "Proficiency with computational tools\n\nMIT Missing Semester: https://missing.csail.mit.edu/\n\nReally, check this one out",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#r",
    "href": "data-best-practices.html#r",
    "title": "‘Best’ practices for data projects",
    "section": "R",
    "text": "R\n\nR for Data Science: https://r4ds.hadley.nz/\nAdvanced R: https://adv-r.hadley.nz/\nfasteR (base): https://github.com/matloff/fasteR\nEfficient R Programming: https://bookdown.org/csgillespie/efficientR/\nR performance tips: https://peerj.com/preprints/26605.pdf\nR Inferno: https://www.burns-stat.com/documents/books/the-r-inferno/\nIntroduction to data science: https://rafalab.github.io/dsbook-part-1/\nAdvanced data science: https://rafalab.github.io/dsbook-part-2/",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#quick-note-todo",
    "href": "data-best-practices.html#quick-note-todo",
    "title": "1  Some data best practices",
    "section": "1.1 Quick note (TODO)",
    "text": "1.1 Quick note (TODO)\nCitations incoming. Please note that much of this document does not contain original material. References are listed at the end of the chapter (I still need to get my .bib files in order to give proper attribution)."
  },
  {
    "objectID": "r-basics.html#subsetting-in-r",
    "href": "r-basics.html#subsetting-in-r",
    "title": "R programming basics",
    "section": "Subsetting in R",
    "text": "Subsetting in R\nYou may have only ever encountered R from the perspective of the tidyverse. tidyverse functions provide useful abstractions for munging tidy data however, most genomics data is often best represented and operated on as matrices. Keeping your data in matrix format can provide many benefits as far as speed and code clarity, which in turn helps to ensure correctness. You can think of matrices as just fancy 2D versions of vectors. So what are vectors?\nVectors are the main building blocks of most R analyses. Whenever you use the c() function (‘concatenate’), like: x &lt;- c('a', 'b', 'c') you’re creating a vector. Vectors hold R objects and are the building block of more complex structures in R.\nNOTE: the following is heavily inspired by Norm Matloff’s excellent fasteR tutorial. Take a look there to get a brief and concise overview base R. You should also check out the first few chapters of Hadley Wickham’s amazing book Advanced R. The first edition contains some more information on base R.\n\nSubsetting vectors\nBelow, we’ll use the built-in R constant called LETTERS. The LETTERS vector is simply a ‘list’ of all uppercase letters in the Roman alphabet.\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\nWe can subset the vector by position. For example, to get the 3rd letter we use the [ operator and the position we want to extract.\n\nLETTERS[3]\n\n[1] \"C\"\n\n\nWe can also use a range of positions. The notation 3:7 is a shortcut that generates the numbers, 3, 4, 5, 6, 7.\n\nLETTERS[3:7]\n\n[1] \"C\" \"D\" \"E\" \"F\" \"G\"\n\n\nWe don’t have to select sequential elements either. We can extract elements by using another vector of positions.\n\nLETTERS[c(7, 5, 14, 14, 1, 18, 15)]\n\n[1] \"G\" \"E\" \"N\" \"N\" \"A\" \"R\" \"O\"\n\n\nVectors become really powerful when we start combining them with logical operations. R supports all of the usual logical and comparison operators you can expect from a programming language, &lt;, &gt;, ==, !=, &lt;=, &gt;=, %in%, & and |.\n\nmy_favorite_letters &lt;- c(\"A\", \"B\", \"C\")\n\n# See that this produces a logical vector of (TRUE/FALSE) values\n# TRUE when LETTERS is one of my_favorite_letters and FALSE otherwise\nLETTERS %in% my_favorite_letters\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE\n\n# We can use that same expression to filter the vector\nLETTERS[LETTERS %in% my_favorite_letters]\n\n[1] \"A\" \"B\" \"C\"\n\n\nThis same kind of subsetting works on vectors that contain numeric data as well. For example, we can filter the measurements of annual flow of water through the Nile river like so:\nNile is another built-in dataset\n\n# Any values strictly greater than 1200\nNile[Nile &gt; 1200]\n\n[1] 1210 1230 1370 1210 1250 1260 1220\n\n# Any even number - `%%` is the modulus operator\nNile[Nile %% 2 == 0]\n\n [1] 1120 1160 1210 1160 1160 1230 1370 1140 1110  994 1020  960 1180  958 1140\n[16] 1100 1210 1150 1250 1260 1220 1030 1100  774  840  874  694  940  916  692\n[31] 1020 1050  726  456  824  702 1120 1100  832  764  768  864  862  698  744\n[46]  796 1040  944  984  822 1010  676  846  812  742 1040  860  874  848  890\n[61]  744  838 1050  918  986 1020  906 1170  912  746  718  714  740\n\n\nAt this point it’s important to take a step back and appreciate what R is doing. Each of the comparison operators that we used above is vectorized. This means that the comparison is applied to all elements of the vector at one time. If you’re used to a programming language like Python this might seem foreign at first. In Python, you would have to write a list comprehension to filter observations from a list that meet a certain condition. For example, [x for x in Nile if x &gt; 1200]. However in R, most functions and operators are vectorized allowing us to do things like Nile &gt; 1200 and have the comparison applied to all of the elements of the vector automatically.\n\n\nSubsetting data.frames\nBut these are just one dimensional vectors. In R, we usually deal with data.frames (tibbles for you tidyverse folks) and matrices. Lucky for us, the subsetting operations we learned for vectors work the same way for data.frames and matrices.\nLet’s take a look at the built-in ToothGrowth dataset. The data consists of the length of odontoblasts in 60 guinea pigs receiving one of three levels of vitamin C by one of two delivery methods.\n\nhead(ToothGrowth)\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5\n\n\nThe dollar sign $ is used to extract an individual column from the data.frame, which is just a vector.\n\nhead(ToothGrowth$len)\n\n[1]  4.2 11.5  7.3  5.8  6.4 10.0\n\n\nWe can also use the [[ to get the same thing. Double-brackets come in handy when your columns are not valid R names since $ only works when columns are valid names.\n\nhead(ToothGrowth[[\"len\"]])\n\n[1]  4.2 11.5  7.3  5.8  6.4 10.0\n\n\nWhen subsetting a data.frame in base R, the general scheme is:\ndf[the rows you want, the columns you want]\nSo in order to get the 5th row of the first column we could do:\n\nToothGrowth[5, 1]\n\n[1] 6.4\n\n\nAgain, we can combine this kind of thinking to extract rows and columns matching logical conditions. For example, if we want to get all of the animals administered orange juice (‘OJ’)\n\nToothGrowth[ToothGrowth$supp == \"OJ\", ]\n\n    len supp dose\n31 15.2   OJ  0.5\n32 21.5   OJ  0.5\n33 17.6   OJ  0.5\n34  9.7   OJ  0.5\n35 14.5   OJ  0.5\n36 10.0   OJ  0.5\n37  8.2   OJ  0.5\n38  9.4   OJ  0.5\n39 16.5   OJ  0.5\n40  9.7   OJ  0.5\n41 19.7   OJ  1.0\n42 23.3   OJ  1.0\n43 23.6   OJ  1.0\n44 26.4   OJ  1.0\n45 20.0   OJ  1.0\n46 25.2   OJ  1.0\n47 25.8   OJ  1.0\n48 21.2   OJ  1.0\n49 14.5   OJ  1.0\n50 27.3   OJ  1.0\n51 25.5   OJ  2.0\n52 26.4   OJ  2.0\n53 22.4   OJ  2.0\n54 24.5   OJ  2.0\n55 24.8   OJ  2.0\n56 30.9   OJ  2.0\n57 26.4   OJ  2.0\n58 27.3   OJ  2.0\n59 29.4   OJ  2.0\n60 23.0   OJ  2.0\n\n\nWe can also combine logical statements. For example, to get all of the rows for animals administered orange juice and with odontoblast length (‘len’) less than 10.\n\nToothGrowth[ToothGrowth$supp == \"OJ\" & ToothGrowth$len &lt; 10, ]\n\n   len supp dose\n34 9.7   OJ  0.5\n37 8.2   OJ  0.5\n38 9.4   OJ  0.5\n40 9.7   OJ  0.5\n\n# We can also use the bracket notation to select rows and columns at the same time\n# Although this gets a little difficult to read\nToothGrowth[ToothGrowth$supp == \"OJ\" & ToothGrowth$len &lt; 10, c(\"len\", \"supp\")]\n\n   len supp\n34 9.7   OJ\n37 8.2   OJ\n38 9.4   OJ\n40 9.7   OJ\n\n\nIt gets annoying typing ToothGrowth every time we want to subset the data.frame. Base R has a very useful function called subset() that can help us type less. subset() essentially ‘looks inside’ the data.frame for the given columns and evaluates the expression without having to explicitly tell R where to find the columns. Think of it like dplyr::filter(), if you are familiar with that function.\n\nsubset(ToothGrowth, supp == \"OJ\" & len &lt; 10)\n\n   len supp dose\n34 9.7   OJ  0.5\n37 8.2   OJ  0.5\n38 9.4   OJ  0.5\n40 9.7   OJ  0.5\n\n\n\n\nSubsetting Lists\nAnother data structure to be aware of, which is used frequently, is the List. We’ve actually already encountered Lists above. data.frames are really just Lists where each vector contains the same data type and all List elements are the same length.\nWe can create a List in R using the list() function. Notice how each list element has a name and can contain a different type of data and number of data elements\n\nl &lt;- list(\n  element1 = c(1, 10, 12, 3, 6, 12, 13, 2, 5, 6, 3, 7),\n  element2 = c(\"a\", \"b\", \"c\"),\n  element3 = c(TRUE, TRUE, FALSE, FALSE, FALSE),\n  element4 = c(0.001, 0.05, 0.86, 1.098, 345.0)\n)\n\nLists can be tricky at first. To extract the data from a particular list element you can use the [[ or the $ (as in the case of data.frames above). Like vectors, you can use either the index or the name of the element you wish to extract.\n\nl[[1]]\n\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\nl[[\"element1\"]]\n\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\nl$element1\n\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\n\nWhat is returned if you only use the single bracket [?\n\nl[1]\n\n$element1\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\n\nYou get another List, but now with a single element. This behavior might seem unintuitive at first, but it can be very useful for creating new lists.\n\nnumeric_l &lt;- l[c(1, 4)]\n\n\n\nSubsetting matrices\nMatrices behave much like data.frames but unlike data.frames matrices can only contain one type of data. This might sound like a limitation at first but you’ll soon come to realize that matrices are very powerful (and fast) to work with in R.\n\nset.seed(123)\n\n# Create some random data that looks like methylation values\n(m &lt;- matrix(\n  data = runif(6 * 10),\n  ncol = 6,\n  dimnames = list(\n    paste0(\"CpG.\", 1:10),\n    paste0(\"Sample\", 1:6)\n  )\n))\n\n         Sample1    Sample2   Sample3    Sample4   Sample5    Sample6\nCpG.1  0.2875775 0.95683335 0.8895393 0.96302423 0.1428000 0.04583117\nCpG.2  0.7883051 0.45333416 0.6928034 0.90229905 0.4145463 0.44220007\nCpG.3  0.4089769 0.67757064 0.6405068 0.69070528 0.4137243 0.79892485\nCpG.4  0.8830174 0.57263340 0.9942698 0.79546742 0.3688455 0.12189926\nCpG.5  0.9404673 0.10292468 0.6557058 0.02461368 0.1524447 0.56094798\nCpG.6  0.0455565 0.89982497 0.7085305 0.47779597 0.1388061 0.20653139\nCpG.7  0.5281055 0.24608773 0.5440660 0.75845954 0.2330341 0.12753165\nCpG.8  0.8924190 0.04205953 0.5941420 0.21640794 0.4659625 0.75330786\nCpG.9  0.5514350 0.32792072 0.2891597 0.31818101 0.2659726 0.89504536\nCpG.10 0.4566147 0.95450365 0.1471136 0.23162579 0.8578277 0.37446278\n\n\nIf we want to extract the value for CpG.3 for Sample3\n\nm[3, 3]\n\n[1] 0.6405068\n\n\nOr all values of CpG.3 for every sample\n\nm[3, ]\n\n  Sample1   Sample2   Sample3   Sample4   Sample5   Sample6 \n0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248 \n\n# Or refer to the row by it's name\nm[\"CpG.3\", ]\n\n  Sample1   Sample2   Sample3   Sample4   Sample5   Sample6 \n0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248 \n\n\nOr all CpGs for Sample3\n\nm[, 3]\n\n    CpG.1     CpG.2     CpG.3     CpG.4     CpG.5     CpG.6     CpG.7     CpG.8 \n0.8895393 0.6928034 0.6405068 0.9942698 0.6557058 0.7085305 0.5440660 0.5941420 \n    CpG.9    CpG.10 \n0.2891597 0.1471136 \n\n# Or refer to the column by it's name\nm[, \"Sample3\"]\n\n    CpG.1     CpG.2     CpG.3     CpG.4     CpG.5     CpG.6     CpG.7     CpG.8 \n0.8895393 0.6928034 0.6405068 0.9942698 0.6557058 0.7085305 0.5440660 0.5941420 \n    CpG.9    CpG.10 \n0.2891597 0.1471136 \n\n\nWe can also apply a mask to the entire matrix at once. For example, the following will mark any value that is greater than 0.5 with TRUE\n\nm &gt; 0.5\n\n       Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nCpG.1    FALSE    TRUE    TRUE    TRUE   FALSE   FALSE\nCpG.2     TRUE   FALSE    TRUE    TRUE   FALSE   FALSE\nCpG.3    FALSE    TRUE    TRUE    TRUE   FALSE    TRUE\nCpG.4     TRUE    TRUE    TRUE    TRUE   FALSE   FALSE\nCpG.5     TRUE   FALSE    TRUE   FALSE   FALSE    TRUE\nCpG.6    FALSE    TRUE    TRUE   FALSE   FALSE   FALSE\nCpG.7     TRUE   FALSE    TRUE    TRUE   FALSE   FALSE\nCpG.8     TRUE   FALSE    TRUE   FALSE   FALSE    TRUE\nCpG.9     TRUE   FALSE   FALSE   FALSE   FALSE    TRUE\nCpG.10   FALSE    TRUE   FALSE   FALSE    TRUE   FALSE\n\n\nWe can use this kind of masking to filter rows of the matrix using some very helpful base R functions that operate on matrices. For example, to get only those CpGs where 3 or more samples have a value &gt; 0.5 we can use the rowSums() like so:\n\nm[rowSums(m &gt; 0.5) &gt; 3, ]\n\n        Sample1   Sample2   Sample3   Sample4   Sample5   Sample6\nCpG.3 0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248\nCpG.4 0.8830174 0.5726334 0.9942698 0.7954674 0.3688455 0.1218993\n\n\nThis pattern is very common when dealing with sequencing data. Base R functions like rowSums() and colMeans() are specialized to operate over matrices and are the most efficient way to summarize matrix data. The R package matrixStats also contains highly optimized functions for operating on matrices.\nCompare the above to the tidy solution given the same matrix.\n\ntidyr::as_tibble(m, rownames = \"CpG\") |&gt;\n  tidyr::pivot_longer(!CpG, names_to = \"SampleName\", values_to = \"beta\") |&gt;\n  dplyr::group_by(CpG) |&gt;\n  dplyr::mutate(n = sum(beta &gt; 0.5)) |&gt;\n  dplyr::filter(n &gt; 3) |&gt;\n  tidyr::pivot_wider(id_cols = CpG, names_from = \"SampleName\", values_from = \"beta\") |&gt;\n  tibble::column_to_rownames(var = \"CpG\") |&gt;\n  data.matrix()\n\n        Sample1   Sample2   Sample3   Sample4   Sample5   Sample6\nCpG.3 0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248\nCpG.4 0.8830174 0.5726334 0.9942698 0.7954674 0.3688455 0.1218993\n\n\nThere’s probably some kind of tidy solution using across() that I’m missing but this is how most of the tidy code in the wild that I have seen looks",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#what-are-summarizedexperiments",
    "href": "r-basics.html#what-are-summarizedexperiments",
    "title": "R programming basics",
    "section": "What are SummarizedExperiments",
    "text": "What are SummarizedExperiments\nSummarizedExperiments are R objects meant for organizing and manipulating rectangular matrices that are typically produced by arrays or high-throughput sequencing. If you are doing any kind of analysis that requires associating feature-level data (RNA-seq gene counts, methylation array loci, ATAC-seq regions, etc.) with the genomic coordinates of those features and the sample-level metadata with which those features were measured, then you should be using a SummarizedExperiment to organize, manipulate, and store your results.\nPlease take a moment to read through the first 2 sections (at least) of the SummarizedExperiment vignette in order to familiarize yourself with what SummarizedExperiments are and their structure. I will demonstrate how you can use SummarizedExperiments below.\nFrom the SummarizedExperiment vignette:\n\nThe SummarizedExperiment object coordinates four main parts:\n\nassay(), assays(): A matrix-like or list of matrix-like objects of identical dimension\n\nmatrix-like: implements dim(), dimnames(), and 2-dimensional [, [<- methods.\nrows: genes, genomic coordinates, etc.\ncolumns: samples, cells, etc.\n\ncolData(): Annotations on each column, as a DataFrame.\n\nE.g., description of each sample\n\nrowData() and / or rowRanges(): Annotations on each row.\n\nrowRanges(): coordinates of gene / exons in transcripts / etc.\nrowData(): P-values and log-fold change of each gene after differential expression analysis.\n\nmetadata(): List of unstructured metadata describing the overall content of the object.\n\nIn order to better understand how they work, let’s construct a SummarizedExperiment from scratch.\n\nConstructing a SummarizedExperiment\nHopefully you’ll already be working with data that is in a SummarizedExperiment or some other class that derives from one. But just in case you don’t have data structured as a SummarizedExperiment it’s useful and instructive to understand how to create one from scratch.\nTo be most useful, a SummarizedExperiment should have at least:\n\nA matrix of data with features in rows and samples in columns\nA metadata data.frame with samples as rownames and columns describing their properties\n\nAnother really useful object to add to SummarizedExperiments is a GRanges object describing the genomic locations of each feature in the matrix. Adding this to the SummarizedExperiment creates what is called a RangedSummarizedExperiment that acts just like a regular SummarizedExperiment with some extra features.\nTo construct our basic SummarizedExperiment:\n\nWe’ll create a ‘counts’ matrix with gene IDs as rows and Samples in columns\nWe’ll add some metadata describing the Samples\nWe’ll add on GRanges() describing the genomic location of the genes\n\n\nConstruct the counts matrix\n\nsuppressPackageStartupMessages(library(SummarizedExperiment))\n\n\ncounts <- matrix(\n  data = rnbinom(n = 200 * 6, mu = 100, size = 1 / 0.5),\n  nrow = 200,\n  dimnames = list(paste0(\"Gene\", 1:200), paste0(\"Sample\", 1:6))\n)\n\n# Take a peek at what this looks like\ncounts[1:5, 1:5]\n\n      Sample1 Sample2 Sample3 Sample4 Sample5\nGene1     100     210     186      14      95\nGene2      74      17      62      48      27\nGene3     129      72     105     203      73\nGene4      73      80      84      81      59\nGene5      17     242      32      21      58\n\n\n\n\nConstruct the sample metadata\nIt is important that the sample metadata be either a data.frame or a DataFrame object because SummarizedExperiment requires the colData() to have rownames that match the colnames of the count matrix.\n\ncoldata <- data.frame(\n  SampleName = colnames(counts),\n  Treatment = gl(2, 3, labels = c(\"Control\", \"Treatment\")),\n  Age = sample.int(100, 6),\n  row.names = colnames(counts)\n)\n\n# Take a peek at what this looks like\ncoldata\n\n        SampleName Treatment Age\nSample1    Sample1   Control  38\nSample2    Sample2   Control  71\nSample3    Sample3   Control  30\nSample4    Sample4 Treatment  64\nSample5    Sample5 Treatment   8\nSample6    Sample6 Treatment  47\n\n\nNotice that all of the rownames of the metadata are in the same order as the colnames of the counts matrix. This is necessary.\n\n\nConstruct gene range annotations\nYou will usually have gene annotations or GRanges objects loaded from a GTF file or you may even create GRanges yourself by specifying the chromosome, start, end, and strand, information manually.\n\nrowranges <- GRanges(\n  rep(c(\"chr1\", \"chr2\"), c(50, 150)),\n  IRanges(floor(runif(200, 1e5, 1e6)), width = 100),\n  strand = sample(c(\"+\", \"-\"), 200, TRUE),\n  feature_id = sprintf(\"ID%03d\", 1:200),\n  gene_type = sample(c(\"protein_coding\", \"lncRNA\", \"repeat_element\"), 200, replace = TRUE)\n)\nnames(rowranges) <- rownames(counts)\n\n# Take a peek at what this looks like\nrowranges\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             <Rle>     <IRanges>  <Rle> | <character>    <character>\n    Gene1     chr1 954187-954286      - |       ID001 protein_coding\n    Gene2     chr1 552363-552462      + |       ID002 protein_coding\n    Gene3     chr1 303793-303892      - |       ID003         lncRNA\n    Gene4     chr1 875980-876079      + |       ID004 protein_coding\n    Gene5     chr1 427141-427240      - |       ID005 repeat_element\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 268871-268970      + |       ID196 repeat_element\n  Gene197     chr2 599914-600013      + |       ID197 repeat_element\n  Gene198     chr2 477464-477563      + |       ID198 repeat_element\n  Gene199     chr2 461424-461523      - |       ID199 repeat_element\n  Gene200     chr2 619939-620038      + |       ID200 protein_coding\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nConstruct the SummarizedExperiment object\nWith these pieces of information we’re ready to create a SummarizedExperiment object.\n\nse <- SummarizedExperiment(\n  assays = list(counts = counts),\n  colData = coldata,\n  rowRanges = rowranges\n)\n\n# Printing the object gives a summary of what's inside\nse\n\nclass: RangedSummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(2): feature_id gene_type\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(3): SampleName Treatment Age"
  },
  {
    "objectID": "r-basics.html#accessing-parts-of-the-summarizedexperiment-object",
    "href": "r-basics.html#accessing-parts-of-the-summarizedexperiment-object",
    "title": "R programming basics",
    "section": "Accessing parts of the SummarizedExperiment object",
    "text": "Accessing parts of the SummarizedExperiment object\nEvery part of the SummarizedExperiment object can be extracted with its accessor function. To extract a particular assay you can use the assay() function. To extract the column metadata you can use the colData() function. To extract the GRanges for the rows of the matrix you can use the rowRanges() function. The rowData() function also allows you to access row-level annotation information from data added to the rowData slot or by the mcols() of the rowRanges. This will be made more clear below.\n\nGetting the count matrix\n\nassay(se, \"counts\") |> head()\n\n      Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene1     100     210     186      14      95      43\nGene2      74      17      62      48      27      71\nGene3     129      72     105     203      73      63\nGene4      73      80      84      81      59     153\nGene5      17     242      32      21      58      40\nGene6      22      91      38      89     164      43\n\n\nTo see what assays are available you can use the assays() function\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\n\n\n\nGetting the column metadata\n\ncolData(se)\n\nDataFrame with 6 rows and 3 columns\n         SampleName Treatment       Age\n        <character>  <factor> <integer>\nSample1     Sample1 Control          38\nSample2     Sample2 Control          71\nSample3     Sample3 Control          30\nSample4     Sample4 Treatment        64\nSample5     Sample5 Treatment         8\nSample6     Sample6 Treatment        47\n\n\n\n\nGetting the rowRanges\n\nrowRanges(se)\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             <Rle>     <IRanges>  <Rle> | <character>    <character>\n    Gene1     chr1 954187-954286      - |       ID001 protein_coding\n    Gene2     chr1 552363-552462      + |       ID002 protein_coding\n    Gene3     chr1 303793-303892      - |       ID003         lncRNA\n    Gene4     chr1 875980-876079      + |       ID004 protein_coding\n    Gene5     chr1 427141-427240      - |       ID005 repeat_element\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 268871-268970      + |       ID196 repeat_element\n  Gene197     chr2 599914-600013      + |       ID197 repeat_element\n  Gene198     chr2 477464-477563      + |       ID198 repeat_element\n  Gene199     chr2 461424-461523      - |       ID199 repeat_element\n  Gene200     chr2 619939-620038      + |       ID200 protein_coding\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nGetting the rowData\nNote that rowData in this case is the same as mcols() of the rowRanges\n\nrowData(se)\n\nDataFrame with 200 rows and 2 columns\n         feature_id      gene_type\n        <character>    <character>\nGene1         ID001 protein_coding\nGene2         ID002 protein_coding\nGene3         ID003         lncRNA\nGene4         ID004 protein_coding\nGene5         ID005 repeat_element\n...             ...            ...\nGene196       ID196 repeat_element\nGene197       ID197 repeat_element\nGene198       ID198 repeat_element\nGene199       ID199 repeat_element\nGene200       ID200 protein_coding"
  },
  {
    "objectID": "r-basics.html#modifying-a-summarizedexperiment",
    "href": "r-basics.html#modifying-a-summarizedexperiment",
    "title": "R programming basics",
    "section": "Modifying a SummarizedExperiment",
    "text": "Modifying a SummarizedExperiment\nOnce you create a SummarizedExperiment you are not stuck with the information in that object. SummarizedExperiments allow you to add and modify the data within the object.\n\nAdding assays\nFor example, we may wish to calculate counts per million values for our counts matrix and add a new assay back into our SummarizedExperiment object.\n\n# Calculate counts per million\ncounts <- assay(se, \"counts\")\ncpm <- counts / colSums(counts) * 1e6\n\n# Add the CPM data as a new assay to our existing se object\nassay(se, \"cpm\") <- cpm\n\n# And if we wish to log-scale these values\nassay(se, \"logcounts\") <- log2(cpm)\n\n# Now there are three assays available\nassays(se)\n\nList of length 3\nnames(3): counts cpm logcounts\n\n\nNote: Instead of creating intermediate variables we could also directly use the assays like so:\n\nassay(se, \"cpm\") <- assay(se, \"counts\") / colSums(assay(se, \"counts\")) * 1e6\n\n\n\nAdding metadata\nSummarizedExperiment objects use the $ to get and set columns of the metadata contained in the colData slot. For example, to get all of the Ages we can use:\n\nse$Age\n\n[1] 38 71 30 64  8 47\n\n\nIf we want to add a new column we simply create the new column in the same way\n\nse$Batch <- factor(rep(c(\"A\", \"B\", \"C\"), 2))\n\n# Now you can se that a new 'Batch` column has been added to the colData\ncolData(se)\n\nDataFrame with 6 rows and 4 columns\n         SampleName Treatment       Age    Batch\n        <character>  <factor> <integer> <factor>\nSample1     Sample1 Control          38        A\nSample2     Sample2 Control          71        B\nSample3     Sample3 Control          30        C\nSample4     Sample4 Treatment        64        A\nSample5     Sample5 Treatment         8        B\nSample6     Sample6 Treatment        47        C\n\n\n\n\nAdding rowData\nWe can also modify the data which describes each feature in the matrix by adding columns to the rowData. For example, let’s create a new column called Keep if the gene is a protein_coding gene.\n\nrowData(se)$Keep <- rowData(se)$gene_type == \"protein_coding\"\n\nrowData(se)\n\nDataFrame with 200 rows and 3 columns\n         feature_id      gene_type      Keep\n        <character>    <character> <logical>\nGene1         ID001 protein_coding      TRUE\nGene2         ID002 protein_coding      TRUE\nGene3         ID003         lncRNA     FALSE\nGene4         ID004 protein_coding      TRUE\nGene5         ID005 repeat_element     FALSE\n...             ...            ...       ...\nGene196       ID196 repeat_element     FALSE\nGene197       ID197 repeat_element     FALSE\nGene198       ID198 repeat_element     FALSE\nGene199       ID199 repeat_element     FALSE\nGene200       ID200 protein_coding      TRUE"
  },
  {
    "objectID": "r-basics.html#subsetting-summarizedexperiment-objects",
    "href": "r-basics.html#subsetting-summarizedexperiment-objects",
    "title": "R programming basics",
    "section": "Subsetting SummarizedExperiment objects",
    "text": "Subsetting SummarizedExperiment objects\nSummarizedExperiments follow the basic idea of\nse[the rows you want, the columns you want]\nWith a SummarizedExperiment “the rows you want” corresponds to the features in the rows of the matrix/rowData and “the columns you want” corresponds to the metadata in colData\n\nSubsetting based on sample metadata\nFor example, if we want to select all of the data belonging only to samples in the Treatment group we can use the following:\n\n(trt <- se[, se$Treatment == \"Treatment\"])\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample4 Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim of the object changed from 6 to 3. This is because we have selected only the Samples from the original SummarizedExperiment object from the treatment group. The cool thing about SummarizedExperiments is that all of the assays have also been subsetted to reflect this selection!\nTake a look at the “logcounts” assay. It only contains Samples 4, 5, and 6.\n\nassay(trt, \"logcounts\") |> head()\n\n        Sample4  Sample5  Sample6\nGene1  9.681439 12.23369 11.09368\nGene2 11.306469 10.37294 11.82069\nGene3 13.329167 11.85724 11.85136\nGene4 11.957898 11.55359 12.97889\nGene5 10.059737 11.73207 10.98576\nGene6 12.146676 13.07906 11.04431\n\n\nOf course you can combine multiple conditions as well\n\nse[, se$Batch %in% c(\"B\", \"C\") & se$Age > 10]\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample2 Sample3 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\n\n\nSubsetting based on rows\nWe can also select certain features that we want to keep using row subsetting. For example to select only the first 50 rows:\n\nse[1:50, ]\n\nclass: RangedSummarizedExperiment \ndim: 50 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(50): Gene1 Gene2 ... Gene49 Gene50\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim changed from 200 to 50 reflecting the fact that we have only selected the first 50 rows.\nThis subsetting is very useful when combined with logical operators. Above we created a vector in rowData called “Keep” that contained TRUE if the corresponding row of the se object was a coding gene and FALSE otherwise. Let’s use this vector to subset our se object.\n\n(coding <- se[rowData(se)$Keep, ])\n\nclass: RangedSummarizedExperiment \ndim: 68 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(68): Gene1 Gene2 ... Gene193 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nAnd if we look at the resulting rowData we can see that it only contains the protein_coding features\n\nrowData(coding)\n\nDataFrame with 68 rows and 3 columns\n         feature_id      gene_type      Keep\n        <character>    <character> <logical>\nGene1         ID001 protein_coding      TRUE\nGene2         ID002 protein_coding      TRUE\nGene4         ID004 protein_coding      TRUE\nGene8         ID008 protein_coding      TRUE\nGene13        ID013 protein_coding      TRUE\n...             ...            ...       ...\nGene183       ID183 protein_coding      TRUE\nGene185       ID185 protein_coding      TRUE\nGene187       ID187 protein_coding      TRUE\nGene193       ID193 protein_coding      TRUE\nGene200       ID200 protein_coding      TRUE\n\n\nEach assay also reflects this operation\n\nassay(coding, \"cpm\") |> head()\n\n        Sample1    Sample2   Sample3    Sample4  Sample5   Sample6\nGene1  5865.103 10646.3878  9453.141   821.1144 4816.223  2185.404\nGene2  3904.601   834.9295  3158.753  2532.7142 1326.065  3617.281\nGene4  3585.286  4075.8101  4432.250  3978.1936 3005.910  8073.027\nGene8  1582.946 12180.1483  1069.900  3165.8928 2603.016   815.162\nGene13 5571.848  4512.0406  6556.211 12199.4135 9936.629 15704.411\nGene15 5019.011  3405.1637 15366.569  6235.7414 4421.630  3988.270\n\n\n\n\nSubsetting based on rowRanges\nA closely related row-wise subsetting operation can be used if you have a RangedSummarizedExperiment (a SummarizedExperiment with a rowRanges slot) that allows you to perform operations on a SummarizedExperiment object like you would a GRanges object.\nFor example, let’s say we only wanted to extract the features on Chromosome 2 only. Then we can use the GenomicRanges function subsetByOverlaps directly on our SummarizedExperiment object like so:\n\n# Region of interest\nroi <- GRanges(seqnames = \"chr2\", ranges = 1:1e7)\n\n# Subset the SE object for only features on chr2\n(chr2 <- subsetByOverlaps(se, roi))\n\nclass: RangedSummarizedExperiment \ndim: 150 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(150): Gene51 Gene52 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nYou can see again that the dim changed reflecting our selection. Again, all of the associated assays and rowData have also been subsetted reflecting this change as well.\n\nrowData(chr2)\n\nDataFrame with 150 rows and 3 columns\n         feature_id      gene_type      Keep\n        <character>    <character> <logical>\nGene51        ID051 protein_coding      TRUE\nGene52        ID052         lncRNA     FALSE\nGene53        ID053 repeat_element     FALSE\nGene54        ID054 repeat_element     FALSE\nGene55        ID055 repeat_element     FALSE\n...             ...            ...       ...\nGene196       ID196 repeat_element     FALSE\nGene197       ID197 repeat_element     FALSE\nGene198       ID198 repeat_element     FALSE\nGene199       ID199 repeat_element     FALSE\nGene200       ID200 protein_coding      TRUE\n\n\n\nassay(chr2, \"counts\") |> head()\n\n       Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene51      32      45     139     261     153      95\nGene52     113      80      80      61      72     131\nGene53     118      24      60     153       5     103\nGene54      20       1      35       4      89      49\nGene55     249      58      43     307      51      33\nGene56     215      96      28     198      50      81\n\n\n\nrowRanges(chr2)\n\nGRanges object with 150 ranges and 3 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type      Keep\n             <Rle>     <IRanges>  <Rle> | <character>    <character> <logical>\n   Gene51     chr2 839636-839735      + |       ID051 protein_coding      TRUE\n   Gene52     chr2 388487-388586      + |       ID052         lncRNA     FALSE\n   Gene53     chr2 357210-357309      + |       ID053 repeat_element     FALSE\n   Gene54     chr2 230565-230664      + |       ID054 repeat_element     FALSE\n   Gene55     chr2 491399-491498      + |       ID055 repeat_element     FALSE\n      ...      ...           ...    ... .         ...            ...       ...\n  Gene196     chr2 268871-268970      + |       ID196 repeat_element     FALSE\n  Gene197     chr2 599914-600013      + |       ID197 repeat_element     FALSE\n  Gene198     chr2 477464-477563      + |       ID198 repeat_element     FALSE\n  Gene199     chr2 461424-461523      - |       ID199 repeat_element     FALSE\n  Gene200     chr2 619939-620038      + |       ID200 protein_coding      TRUE\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\nThere’s also a few shortcuts on range operations using GRanges/SummarizedExperiments. See the help pages for %over, %within%, and %outside%. For example:\n\nall.equal(se[se %over% roi, ], subsetByOverlaps(se, roi))\n\n[1] TRUE\n\n\n\n\nCombining subsetting operations\nOf course you don’t have to perform one subsetting operation at a time. Like base R you can combine multiple expressions to subset a SummarizedExperiment object.\nFor example, to select only features labeled as repeat_elements and the Sample from ‘Batch’ A in the ‘Control’ group\n\n(selected <- se[\n  rowData(se)$gene_type == \"repeat_element\",\n  se$Treatment == \"Control\" &\n    se$Batch == \"A\"\n])\n\nclass: RangedSummarizedExperiment \ndim: 72 1 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(72): Gene5 Gene6 ... Gene198 Gene199\nrowData names(3): feature_id gene_type Keep\ncolnames(1): Sample1\ncolData names(4): SampleName Treatment Age Batch"
  },
  {
    "objectID": "r-basics.html#saving-a-summarizedexperiment",
    "href": "r-basics.html#saving-a-summarizedexperiment",
    "title": "R programming basics",
    "section": "Saving a SummarizedExperiment",
    "text": "Saving a SummarizedExperiment\nSince SummarizedExperiments keep basically all information about an experiment in one place, it is convenient to save the entire SummarizedExperiment object so that you can pick up an analysis where you left off or even to facilitate better sharing of data between collaborators.\nYou can save the entire SummarizedExperiment object with:\nsaveRDS(se, \"/path/to/se.rds\")\nAnd when you want to read the same object back into R for your next analysis you can do so with:\nse <- readRDS(\"/path/to/se.rds\")"
  },
  {
    "objectID": "r-basics.html#summarizedexperiments-in-the-real-world",
    "href": "r-basics.html#summarizedexperiments-in-the-real-world",
    "title": "R programming basics",
    "section": "SummarizedExperiments in the real world",
    "text": "SummarizedExperiments in the real world\nIf you’re working with any Bioconductor packages it’s likely that the object you’re working with either is a SummarizedExperiment or is inherited from one. For example, the DESeqDataSet from the DESeq2 package and BSseq objects from the bsseq package both inherit from a SummarizedExperiment and thus retain all of the same functionality as above. If you go to the SummarizedExperiment landing page and click “See More” under details you can see all of the packages that depend on SummarizedExperiment.\nAlso, many common methods are also implemented for SummarizedExperiment objects. For example, to simplify calculating counts-per-million above I could have simply used the edgeR::cpm() directly on the SummarizedExperiment object. Many functions in bioconductor packages know how to deal directly with SummarizedExperiments so you don’t ever have to take the trouble extracting components and performing tedious calculations yourself.\n\nassay(se, \"cpm\") <- edgeR::cpm(se)\n\nI also left out any discussion of the metadata() slot of the SummarizedExperiment. The metadata slot is simply a list of any R object that contains information about the experiment. The metadata in the metadata slots are not subjected to the same subsetting rules as the other slots. In practice this assay contains additional information about the experiment as a whole. For example, I typically store bootstrap alignments for each sample here.\nTo add something to the SummarizedExperiment metadata slot you can do:\n\nmetadata(se)$additional_info <- \"Experiment performed on 6 samples with three replicates each\"\n\nAnd to retrieve this:\n\nmetadata(se)$additional_info\n\n[1] \"Experiment performed on 6 samples with three replicates each\""
  },
  {
    "objectID": "r-basics.html#closing-thoughts",
    "href": "r-basics.html#closing-thoughts",
    "title": "R programming basics",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nHopefully this was enough information to get you started using SummarizedExperiments. There’s many things I left out such as different backings for storing out of memory data, a tidyverse interface to SummarizedExperiment objects, TreeSummarizedExperiments for microbiome data, MultiAssayExperiments for dealing with experiments containing multiomics data, and much more.\nPlease let me know your thoughts and if anything needs more clarification."
  },
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "R programming basics",
    "section": "",
    "text": "Introduction to programming in R\nThere are tons of great resources for learning R. R for Data Science is probably the most popular resource for new useRs to get up to speed with slicing and dicing data in R. The R for Data Science book, however, is taught from the perspective of the Tidyverse. The Tidyverse is an opinionated set of packages and functions that help users perform data manipulations primarily on data.frames. While these packages and functions can be great for experienced users by providing ergonomic and consistent interfaces for data.frame manipulation, it is my personal belief that new users should first learn the base language, especially if their goal is to perform bioinformatics analysis.\nBioinformatics tools rely heavily on subsetting and matrix manipulations. In my experience, users who start learning R using only function from the Tidyverse have a difficult time understanding matrix manipulations and subsetting operations common in bioinformatics workflows. This becomes especially important when using SummarizedExperiments - the backbone of many bioinformatics data structures in R.\nFor this reason, we’re going to focus on learning R from the ground up using functions that exist primarily in the base language. A great resource for learning base R quickly is Norm Matloff’s fasteR which can be found here.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "data-best-practices.html#create-analysis-friendly-data---tidy-data",
    "href": "data-best-practices.html#create-analysis-friendly-data---tidy-data",
    "title": "‘Best’ practices for data projects",
    "section": "Create analysis friendly data - tidy data",
    "text": "Create analysis friendly data - tidy data\nThe term tidy data was defined by Hadley Wickham to describe data which is amenable to downstream analysis. Most people are familiar with performing a quick and dirty data analysis in a program like Excel. You may have also used some of Excel’s fancy features for coloring cells, adding bold and underlines to text, and formatting cells with other decorations. All of this tends to just be extra fluff. If you format you data properly then it will be much easier to perform downstream analysis on and will not require the use of extra decorations. This is true even in Excel!\nTo conform to the requirements of being tidy, the data should follow some simple principles:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nDon’t cram two variables into one value. e.g. “male_treated”.\nDon’t embed units into data. e.g. “3.4kg”. Instead, put in column name e.g. “weight_kg” = 3.4\n\n\n\nOnce your data is in this format, it can easily be read into downstream programs like R, or parsed with command line text editing programs like sed.\nThe iris dataset in R provides a classic example of this format\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nPro-tip\ndata.frame like objects can be stored and retrieved efficiently using the Apache Arrow format instead of CSV files:\ndf &lt;- arrow::read_parquet(\"path/to/file.parquet\")\narrow::write_parquet(df, \"path/to/different_file.parquet\")\nsee R arrow for more details. There’s also the nanoparquet package which provides a light weight reader/writer for parquet files.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "linux.html",
    "href": "linux.html",
    "title": "Command line basics",
    "section": "",
    "text": "Accessing the terminal\nBasic proficiency with the Unix shell is essential for anyone who wants to start doing computational work outside of their laptop and Excel. Unix shells provide an interface for interacting with Unix-like (Mac OS, Linux, etc.) operating systems and a scripting language for controlling the system. The Unix philosopy is a set of software engineering norms and concepts that guide how the tools of the Unix shell interact with one another. Learning a few of these command line tools, and how they can be strung together into what are called “pipes”, is a powerful skill for developing quick and composable bioinformatics programs. Here, we’ll describe some essential commands to get you started using the command line.\nFirst, you’ll have to open the Terminal application. If you’re on Mac OS, the quickest way to access your terminal is: “command + space”, typing “terminal” and pressing Enter. On Windows, you’ll have to install Windows Subsystem for Linux which will allow you to interact with a (default) Ubuntu OS.\nOnce you’ve opened the terminal app, you’re ready to start typing commands at the command line.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#where-am-i",
    "href": "linux.html#where-am-i",
    "title": "Command line basics",
    "section": "Where am I?",
    "text": "Where am I?\nThe first command you should know is pwd. pwd will print your current working directory. This command is used to display where you are currently in the file system. For example, if I open a terminal window in my “Downloads” directory and type and hit Enter:\npwd \nIt will return\n/home/gennaro/Downloads\nindicating that I am in my “Downloads” directory.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#listing-files",
    "href": "linux.html#listing-files",
    "title": "Command line basics",
    "section": "Listing files",
    "text": "Listing files\nNow that I’m in my “Downloads” directory I want to see what files I’ve downloaded. To do this, I can use the ls command to list files in the directory.\nls\nwhich returns:\nBDNF-data.tsv  CORI_Candidate_SNP_draft_250528_clean.docx  differential-expression2.tsv\nYour “Downloads” directory will of course have different files. If I need to display more information about these files, such as the time that they were created or how large they are, I can supply the ls command with arguments.\nFor example\nls -lah\nReturns\ntotal 15M\ndrwxr-xr-x  2 gennaro gennaro 4.0K Jun  1 14:52 .\ndrwxr-x--- 51 gennaro gennaro 4.0K Jun  1 09:34 ..\n-rw-rw-r--  1 gennaro gennaro 6.8K May 30 18:00 BDNF-data.tsv\n-rw-rw-r--  1 gennaro gennaro 973K May 30 17:34 CORI_Candidate_SNP_draft_250528_clean.docx\n-rw-rw-r--  1 gennaro gennaro  14M May 30 12:54 differential-expression2.tsv\nWhich provides information about the file permissions, the file sizes, and when the files were created.\n\nLearning more about a command\nTo learn more about what arguments are available to any of the command line programs you run, you can use the man, or manual, command. This command will open the user manual for the given command.\nTry typing\nman ls\nto view all of the options available when listing files with ls.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#moving-around",
    "href": "linux.html#moving-around",
    "title": "Command line basics",
    "section": "Moving around",
    "text": "Moving around\nLet’s say I want to move from my “Downloads” directory to my “Documents” directory. The command I have to use is cd, short for “change directory”. We can use the cd command with the argument for the target directory we want to go to. For example, to move to my “Documents” directory\ncd /home/gennaro/Documents\n\nDirectory shortcuts\nThe shell has a few shortcuts that make moving around a little easier. Running cd without any arguments will bring you back into your home directory.\ncd\nIn Bash, there is an additional shortcut to specify the “/home” as well. You can use ~ in place of “/home”. For example, to move into my “Documents” folder I can use\ncd ~/Documents\ninstead of typing the full path. To go up one level in the directory you can use ... So to go from my Documents directory ‘up’ into my “/home” directory I can use\ncd ..\nFinally, to go back to the same directory that you were just in you can use\ncd -",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#creating-files",
    "href": "linux.html#creating-files",
    "title": "Command line basics",
    "section": "Creating files",
    "text": "Creating files\nYou can create files with the touch command. For example, to create an empty file in my “Downloads” directory called “A.txt” I can run\ntouch ~/Downloads/A.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#redirection",
    "href": "linux.html#redirection",
    "title": "Command line basics",
    "section": "Redirection",
    "text": "Redirection\nI’ll add some content to this file using the echo command. echo simply prints it’s arguments back out to the terminal. I’ll also use what is called redirection to append the results of the echo command into the text file.\nRedirection is a core concept in Unix pipes. It allows you to take the output from one program and use it as input to another program. In this example, I’ll take the output from echo and redirect it to the file “A.txt” that we just created.\necho \"This is a new line in the file\" &gt;&gt; ~/Downloads/A.txt\necho \"Here is another new line in the file\" &gt;&gt; ~/Downloads/A.txt\nThe &gt;&gt; took the output of the echo command and inserted it as a new line in “A.txt”. Importantly, &gt;&gt; appended these lines into “A.txt”. If I were instead to use &gt; like\necho \"This will replace the current contents of A.txt\" &gt; ~/Downloads/A.txt\n“A.txt” will be overwritten with the new contents. The final essential redirection operator is the pipe |. The pipe lets you take the output from one program and use it as input to another. I’ll show an example of this later.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#displaying-the-content-of-files",
    "href": "linux.html#displaying-the-content-of-files",
    "title": "Command line basics",
    "section": "Displaying the content of files",
    "text": "Displaying the content of files\nThe simplest way to display the contents of a file on the command line is by using the cat command. The cat command is actually designed to concatenate file together, but running it on a single file will print the entire contents of the file to the command line. For example, to print the contents of “A.txt”\ncat ~/Downloads/A.txt\nWill print\nThis will replace the current contents of A.txt\nto the console. If you have a lot of text that you would like to display cat can result in too much information being displayed on the screen. Instead, you can use the less command. less will print the contents of the file as pages on the screen. You can use the d key to scroll down a page, or the u key to scroll up a page.\nAnother way to display only some of the contents of a file is to use the head or tail commands. head -n10 will print the first 10 lines of a file, whereas tail -n10 can be used to print the last 10 lines of a file.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#copying-files",
    "href": "linux.html#copying-files",
    "title": "Command line basics",
    "section": "Copying files",
    "text": "Copying files\nYou can copy a file using the cp command. For example, to copy the “A.txt” file into a new file “B.txt” I can use\ncp ~/Downloads/A.txt ~/Downloads/B.txt\nTo copy an entire directory you need to supply the -r, or recursive, argument to the cp command. For example, to create a copy of my Downloads directory inside of my Documents directory\ncp -r ~/Downloads ~/Documents/Downloads-copy",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#moving-and-renaming-files",
    "href": "linux.html#moving-and-renaming-files",
    "title": "Command line basics",
    "section": "Moving and renaming files",
    "text": "Moving and renaming files\nThe mv command can be used to move files and rename them. For example, to move the “A.txt” file into my Documents directory I can use\nmv A.txt ~/Documents\nIf I now want to change the name of that file I can also use the mv command. Now you need to specify the new file name instead of the location to move the file to\nmv ~/Documents/A.txt ~/Documents/C.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#making-new-directories",
    "href": "linux.html#making-new-directories",
    "title": "Command line basics",
    "section": "Making new directories",
    "text": "Making new directories\nTo make a new directory you can use the mkdir command. To make a new directory inside of my Downloads directory I can use\nmkdir ~/Downloads/textfiles\nBy default, the mkdir command doesn’t allow you to create nested directories. To enable this, set the mkdir -p flag. For example I can create a parent folder and subfolders using\nmkdir -p ~/Downloads/imagefiles/jpegs\n\nShell expansion\nAnother useful trick is to learn shell expansion. Shell expansion ‘expands’ the arguments. Shell expansion can be a shortcut when creating new project directories. For example\nmkdir -p data doc scripts results/{figures,data-files,rds-files}\nThe results/{figures,data-files,rds-files} expands this command into\nmkdir -p data doc scripts results/figures results/data-files results/rds-files\nWhich saves some typing. Shell expansion can also be used in other contexts. For example, I can create 260 empty text files using the following command\ntouch ~/Downloads/textfiles/{A..Z}{1..10}.txt\nAnother useful shell expansion is *. For example, if I needed to display the contents of each of the file we just created I could run\ncat ~/Downloads/textfiles/*.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#removing-files",
    "href": "linux.html#removing-files",
    "title": "Command line basics",
    "section": "Removing files",
    "text": "Removing files\nRemoving files on the command line can be done with the rm command. Unlike when using a GUI, when you remove files on the command line you cannot get them back so use rm wisely. To remove one of the empty files I just created I can use\nrm ~/Downloads/textfiles/A1.txt\nIf I want to remove the entire “textfiles” directory I can use the -r, or recursive flag with rm.\nrm -r ~/Downloads/textfiles\nBe careful when using rm. A simple space can mean removing entire file systems by mistake!",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#finding-files",
    "href": "linux.html#finding-files",
    "title": "Command line basics",
    "section": "Finding files",
    "text": "Finding files\nOne incredibly useful but often overlooked command line tools is find. find does exactly what you expect it to do, it finds files and folders. find has many arguments but the simplest usage is for finding files using a specific pattern. For example, to find all text (.txt) files in a particular directory and all of its subdirectories you can use:\nfind . -name \"*.txt\" -type f\nThis command says, “find any file (-type f) that has a name like ‘.txt’”. find is especially powerful when combined with the -exec argument. For example, to remove all .txt file in a directory you can use:\nfind . -name \"*.txt\" -type f -exec rm {} \\;",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#searching-the-contents-of-files",
    "href": "linux.html#searching-the-contents-of-files",
    "title": "Command line basics",
    "section": "Searching the contents of files",
    "text": "Searching the contents of files\ngrep is a tool that’s use to search the contents of files for specific text patterns. For example, if you wanted to find every line in a text file that contains the word “the” you could use:\ngrep \"the\" pg100.txt\ngrep also has many useful arguments. One of the most useful is that grep can return the count of the number of lines that are returned. For example, to count the number of lines in a text file that contain the word “the”:\ngrep -c \"the\" pg100.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#piping-commands",
    "href": "linux.html#piping-commands",
    "title": "Command line basics",
    "section": "Piping commands",
    "text": "Piping commands\nThe Unix pipe is what makes the command line so powerful. You can string together small programs to build up solutions to complex problems. The pipe allows you to take the output from one program and use it as input to another program directly.\nFor example, suppose we wanted count the top 10 most frequently used words across all of the works of Shakespeare\ncurl https://www.gutenberg.org/cache/epub/100/pg100.txt | \\\nsed 's/[^a-zA-Z ]/ /g' | \\\ntr 'A-Z ' 'a-z\\n' | \\\ngrep '[a-z]' | \\\nsort | \\\nuniq -c | \\\nsort -nr -k1 | \\\nhead -n10\n\ncurl downloads the text file from Project Gutenberg and streams it to stdout\nsed replaces all characters that are not spaces or letters, with spaces.\ntr changes all of the uppercase letters into lowercase and converts the spaces in the lines of text to newlines (each ‘word’ is now on a separate line)\ngrep includes only lines that contain at least one lowercase alphabetical character (removing any blank lines)\nsort sorts the list of ‘words’ into alphabetical order\nuniq counts the occurrences of each word\nsort sorts the occurrences numerically in descending order\nhead shows the top 10 lines",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#for-loops",
    "href": "linux.html#for-loops",
    "title": "Command line basics",
    "section": "For-loops",
    "text": "For-loops\nThe command line is also a scripting language and like any scripting language, it provides some basic control flow utilities. One of the more useful of these is the basic for loop. In Bash, the for-loop takes the form of a for each loop. the looping variable can be referred to in the loop by using the $ syntax. For example, to loop through\nfor F in *.txt; do sort $F | uniq -c | sort -nr -k1 | head -n1 &gt;&gt; top-words.txt; done",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#gnu-parallel",
    "href": "linux.html#gnu-parallel",
    "title": "Command line basics",
    "section": "GNU parallel",
    "text": "GNU parallel\nGNU parallel is a command line tool that takes away the need to use for-loops entirely. parallel is extremely powerful and feature filled. Importantly, it lets you run commands across multiple jobs. For example, instead of writing a for loop we can process the text files above using 8 jobs at once with parallel\nparallel --jobs 8 \"sort {} | uniq -c | sort -nr -k1 | head -n1 &gt;&gt; top-words.txt\" ::: *.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#downloading-files",
    "href": "linux.html#downloading-files",
    "title": "Command line basics",
    "section": "Downloading files",
    "text": "Downloading files\ncurl and wget are both command line utilities for downloading files from remote resources. curl will download and stream the results to your terminal by default. wget will save the result to a file by default.\ncurl https://www.gutenberg.org/cache/epub/100/pg100.txt\nwget https://www.gutenberg.org/cache/epub/100/pg100.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#compressing-and-uncompressing",
    "href": "linux.html#compressing-and-uncompressing",
    "title": "Command line basics",
    "section": "Compressing and uncompressing",
    "text": "Compressing and uncompressing\nBioinformatics and command line tools can generally work with compressed data. Data compression saves space which can be really beneficial when transferring files over the internet. gzip is an old but commonly used compression utility that is compatible with many command line utilities. To compress a file for example.\ngzip pg100.txt\nWill produce a compressed version of the “pg100.txt” file called “pg100.txt.gz”. Many Unix tools can work directly with gzipped files. For example,\nzcat pg100.txt.gz\nWill unzip and print the contents of the file to the terminal and zgrep can be used to directly search the contents of a gzipped file without the need to decompress the entire file\nzgrep -c \"the\" pg100.txt.gz\nTo decompress a file you use the ‘un’ version of the compression command, gunzip.\ngunzip somefile.txt.gz",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#replacing-file-contents",
    "href": "linux.html#replacing-file-contents",
    "title": "Command line basics",
    "section": "Replacing file contents",
    "text": "Replacing file contents\nsed 's/find/replace/' myfile.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#editors",
    "href": "linux.html#editors",
    "title": "Command line basics",
    "section": "Editors",
    "text": "Editors\nYou’ll eventually need to edit some code or files from the terminal. Two options for code editing from the command line are vim and nano. vim can be more difficult to use for a beginner but is very powerful.\nvim\nOnce you’re in vim you can use the i key to enter “input” mode. “input” mode let’s you type in new characters. Once you’ve typed away, save your work and exit with esc + :wq. vim can be difficult to get used to which lead to the most famous of StackOverflow questions: nano provides a more user friendly interface.\nnano",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#accessing-the-terminal",
    "href": "linux.html#accessing-the-terminal",
    "title": "Command line basics",
    "section": "Accessing the terminal",
    "text": "Accessing the terminal\nFirst, you’ll have to open the Terminal application. If you’re on Mac OS, the quickest way to access your terminal is: “command + space”, typing “terminal” and pressing Enter. On Windows, you’ll have to install Windows Subsystem for Linux which will allow you to interact with a (default) Ubuntu OS.\nOnce you’ve opened the terminal app, you’re ready to start typing commands at the command line."
  },
  {
    "objectID": "linux.html#resources",
    "href": "linux.html#resources",
    "title": "Command line basics",
    "section": "Resources",
    "text": "Resources\n\nTerminus is a fun game designed to get you comfortable navigating the command line\nOverTheWire is another game designed to tech you command line tools through the lense of a ‘hacker’\nvimtutor can be used to learn vim",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "summarized-experiments.html",
    "href": "summarized-experiments.html",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "",
    "text": "What are SummarizedExperiments\nSummarizedExperiments are R objects meant for organizing and manipulating rectangular matrices that are typically produced by arrays or high-throughput sequencing. If you are doing any kind of analysis that requires associating feature-level data (RNA-seq gene counts, methylation array loci, ATAC-seq regions, etc.) with the genomic coordinates of those features and the sample-level metadata with which those features were measured, then you should be using a SummarizedExperiment to organize, manipulate, and store your results.\nPlease take a moment to read through the first 2 sections (at least) of the SummarizedExperiment vignette in order to familiarize yourself with what SummarizedExperiments are and their structure. I will demonstrate how you can use SummarizedExperiments below.\nFrom the SummarizedExperiment vignette:\nThe SummarizedExperiment object coordinates four main parts:\nIn order to better understand how they work, let’s construct a SummarizedExperiment from scratch.",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#what-are-summarizedexperiments",
    "href": "summarized-experiments.html#what-are-summarizedexperiments",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "",
    "text": "assay(), assays(): A matrix-like or list of matrix-like objects of identical dimension\n\nmatrix-like: implements dim(), dimnames(), and 2-dimensional [, [&lt;- methods.\nrows: genes, genomic coordinates, etc.\ncolumns: samples, cells, etc.\n\ncolData(): Annotations on each column, as a DataFrame.\n\nE.g., description of each sample\n\nrowData() and / or rowRanges(): Annotations on each row.\n\nrowRanges(): coordinates of gene / exons in transcripts / etc.\nrowData(): P-values and log-fold change of each gene after differential expression analysis.\n\nmetadata(): List of unstructured metadata describing the overall content of the object.\n\n\n\nConstructing a SummarizedExperiment\nHopefully you’ll already be working with data that is in a SummarizedExperiment or some other class that derives from one. But just in case you don’t have data structured as a SummarizedExperiment it’s useful and instructive to understand how to create one from scratch.\nTo be most useful, a SummarizedExperiment should have at least:\n\nA matrix of data with features in rows and samples in columns\nA metadata data.frame with samples as rownames and columns describing their properties\n\nAnother really useful object to add to SummarizedExperiments is a GRanges object describing the genomic locations of each feature in the matrix. Adding this to the SummarizedExperiment creates what is called a RangedSummarizedExperiment that acts just like a regular SummarizedExperiment with some extra features.\nTo construct our basic SummarizedExperiment:\n\nWe’ll create a ‘counts’ matrix with gene IDs as rows and Samples in columns\nWe’ll add some metadata describing the Samples\nWe’ll add on GRanges() describing the genomic location of the genes\n\n\nConstruct the counts matrix\n\nsuppressPackageStartupMessages(library(SummarizedExperiment))\n\n\ncounts &lt;- matrix(\n  data = rnbinom(n = 200 * 6, mu = 100, size = 1 / 0.5),\n  nrow = 200,\n  dimnames = list(paste0(\"Gene\", 1:200), paste0(\"Sample\", 1:6))\n)\n\n# Take a peek at what this looks like\ncounts[1:5, 1:5]\n\n      Sample1 Sample2 Sample3 Sample4 Sample5\nGene1     142      87     169      30      36\nGene2      88      54      47      69     150\nGene3      21     113     129      84      33\nGene4     163      10      24      44     301\nGene5      83     106     247     162      40\n\n\n\n\nConstruct the sample metadata\nIt is important that the sample metadata be either a data.frame or a DataFrame object because SummarizedExperiment requires the colData() to have rownames that match the colnames of the count matrix.\n\ncoldata &lt;- data.frame(\n  SampleName = colnames(counts),\n  Treatment = gl(2, 3, labels = c(\"Control\", \"Treatment\")),\n  Age = sample.int(100, 6),\n  row.names = colnames(counts)\n)\n\n# Take a peek at what this looks like\ncoldata\n\n        SampleName Treatment Age\nSample1    Sample1   Control   2\nSample2    Sample2   Control  48\nSample3    Sample3   Control   3\nSample4    Sample4 Treatment  42\nSample5    Sample5 Treatment  81\nSample6    Sample6 Treatment   4\n\n\nNotice that all of the rownames of the metadata are in the same order as the colnames of the counts matrix. This is necessary.\n\n\nConstruct gene range annotations\nYou will usually have gene annotations or GRanges objects loaded from a GTF file or you may even create GRanges yourself by specifying the chromosome, start, end, and strand, information manually.\n\nrowranges &lt;- GRanges(\n  rep(c(\"chr1\", \"chr2\"), c(50, 150)),\n  IRanges(floor(runif(200, 1e5, 1e6)), width = 100),\n  strand = sample(c(\"+\", \"-\"), 200, TRUE),\n  feature_id = sprintf(\"ID%03d\", 1:200),\n  gene_type = sample(c(\"protein_coding\", \"lncRNA\", \"repeat_element\"), 200, replace = TRUE)\n)\nnames(rowranges) &lt;- rownames(counts)\n\n# Take a peek at what this looks like\nrowranges\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt;\n    Gene1     chr1 229730-229829      + |       ID001 repeat_element\n    Gene2     chr1 210967-211066      + |       ID002 repeat_element\n    Gene3     chr1 445612-445711      + |       ID003 protein_coding\n    Gene4     chr1 402262-402361      - |       ID004 repeat_element\n    Gene5     chr1 214542-214641      - |       ID005 protein_coding\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 929350-929449      + |       ID196         lncRNA\n  Gene197     chr2 545444-545543      + |       ID197 protein_coding\n  Gene198     chr2 932266-932365      + |       ID198 protein_coding\n  Gene199     chr2 463898-463997      + |       ID199 protein_coding\n  Gene200     chr2 904679-904778      - |       ID200 repeat_element\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nConstruct the SummarizedExperiment object\nWith these pieces of information we’re ready to create a SummarizedExperiment object.\n\nse &lt;- SummarizedExperiment(\n  assays = list(counts = counts),\n  colData = coldata,\n  rowRanges = rowranges\n)\n\n# Printing the object gives a summary of what's inside\nse\n\nclass: RangedSummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(2): feature_id gene_type\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(3): SampleName Treatment Age",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#accessing-parts-of-the-summarizedexperiment-object",
    "href": "summarized-experiments.html#accessing-parts-of-the-summarizedexperiment-object",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Accessing parts of the SummarizedExperiment object",
    "text": "Accessing parts of the SummarizedExperiment object\nEvery part of the SummarizedExperiment object can be extracted with its accessor function. To extract a particular assay you can use the assay() function. To extract the column metadata you can use the colData() function. To extract the GRanges for the rows of the matrix you can use the rowRanges() function. The rowData() function also allows you to access row-level annotation information from data added to the rowData slot or by the mcols() of the rowRanges. This will be made more clear below.\n\nGetting the count matrix\n\nassay(se, \"counts\") |&gt; head()\n\n      Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene1     142      87     169      30      36       6\nGene2      88      54      47      69     150      61\nGene3      21     113     129      84      33      63\nGene4     163      10      24      44     301     158\nGene5      83     106     247     162      40     110\nGene6      72     116      28      38      34      69\n\n\nTo see what assays are available you can use the assays() function\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\n\n\n\nGetting the column metadata\n\ncolData(se)\n\nDataFrame with 6 rows and 3 columns\n         SampleName Treatment       Age\n        &lt;character&gt;  &lt;factor&gt; &lt;integer&gt;\nSample1     Sample1 Control           2\nSample2     Sample2 Control          48\nSample3     Sample3 Control           3\nSample4     Sample4 Treatment        42\nSample5     Sample5 Treatment        81\nSample6     Sample6 Treatment         4\n\n\n\n\nGetting the rowRanges\n\nrowRanges(se)\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt;\n    Gene1     chr1 229730-229829      + |       ID001 repeat_element\n    Gene2     chr1 210967-211066      + |       ID002 repeat_element\n    Gene3     chr1 445612-445711      + |       ID003 protein_coding\n    Gene4     chr1 402262-402361      - |       ID004 repeat_element\n    Gene5     chr1 214542-214641      - |       ID005 protein_coding\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 929350-929449      + |       ID196         lncRNA\n  Gene197     chr2 545444-545543      + |       ID197 protein_coding\n  Gene198     chr2 932266-932365      + |       ID198 protein_coding\n  Gene199     chr2 463898-463997      + |       ID199 protein_coding\n  Gene200     chr2 904679-904778      - |       ID200 repeat_element\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nGetting the rowData\nNote that rowData in this case is the same as mcols() of the rowRanges\n\nrowData(se)\n\nDataFrame with 200 rows and 2 columns\n         feature_id      gene_type\n        &lt;character&gt;    &lt;character&gt;\nGene1         ID001 repeat_element\nGene2         ID002 repeat_element\nGene3         ID003 protein_coding\nGene4         ID004 repeat_element\nGene5         ID005 protein_coding\n...             ...            ...\nGene196       ID196         lncRNA\nGene197       ID197 protein_coding\nGene198       ID198 protein_coding\nGene199       ID199 protein_coding\nGene200       ID200 repeat_element",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#modifying-a-summarizedexperiment",
    "href": "summarized-experiments.html#modifying-a-summarizedexperiment",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Modifying a SummarizedExperiment",
    "text": "Modifying a SummarizedExperiment\nOnce you create a SummarizedExperiment you are not stuck with the information in that object. SummarizedExperiments allow you to add and modify the data within the object.\n\nAdding assays\nFor example, we may wish to calculate counts per million values for our counts matrix and add a new assay back into our SummarizedExperiment object.\n\n# Calculate counts per million\ncounts &lt;- assay(se, \"counts\")\ncpm &lt;- counts / colSums(counts) * 1e6\n\n# Add the CPM data as a new assay to our existing se object\nassay(se, \"cpm\") &lt;- cpm\n\n# And if we wish to log-scale these values\nassay(se, \"logcounts\") &lt;- log2(cpm)\n\n# Now there are three assays available\nassays(se)\n\nList of length 3\nnames(3): counts cpm logcounts\n\n\nNote: Instead of creating intermediate variables we could also directly use the assays like so:\n\nassay(se, \"cpm\") &lt;- assay(se, \"counts\") / colSums(assay(se, \"counts\")) * 1e6\n\n\n\nAdding metadata\nSummarizedExperiment objects use the $ to get and set columns of the metadata contained in the colData slot. For example, to get all of the Ages we can use:\n\nse$Age\n\n[1]  2 48  3 42 81  4\n\n\nIf we want to add a new column we simply create the new column in the same way\n\nse$Batch &lt;- factor(rep(c(\"A\", \"B\", \"C\"), 2))\n\n# Now you can se that a new 'Batch` column has been added to the colData\ncolData(se)\n\nDataFrame with 6 rows and 4 columns\n         SampleName Treatment       Age    Batch\n        &lt;character&gt;  &lt;factor&gt; &lt;integer&gt; &lt;factor&gt;\nSample1     Sample1 Control           2        A\nSample2     Sample2 Control          48        B\nSample3     Sample3 Control           3        C\nSample4     Sample4 Treatment        42        A\nSample5     Sample5 Treatment        81        B\nSample6     Sample6 Treatment         4        C\n\n\n\n\nAdding rowData\nWe can also modify the data which describes each feature in the matrix by adding columns to the rowData. For example, let’s create a new column called Keep if the gene is a protein_coding gene.\n\nrowData(se)$Keep &lt;- rowData(se)$gene_type == \"protein_coding\"\n\nrowData(se)\n\nDataFrame with 200 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene1         ID001 repeat_element     FALSE\nGene2         ID002 repeat_element     FALSE\nGene3         ID003 protein_coding      TRUE\nGene4         ID004 repeat_element     FALSE\nGene5         ID005 protein_coding      TRUE\n...             ...            ...       ...\nGene196       ID196         lncRNA     FALSE\nGene197       ID197 protein_coding      TRUE\nGene198       ID198 protein_coding      TRUE\nGene199       ID199 protein_coding      TRUE\nGene200       ID200 repeat_element     FALSE",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#subsetting-summarizedexperiment-objects",
    "href": "summarized-experiments.html#subsetting-summarizedexperiment-objects",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Subsetting SummarizedExperiment objects",
    "text": "Subsetting SummarizedExperiment objects\nSummarizedExperiments follow the basic idea of\nse[the rows you want, the columns you want]\nWith a SummarizedExperiment “the rows you want” corresponds to the features in the rows of the matrix/rowData and “the columns you want” corresponds to the metadata in colData\n\nSubsetting based on sample metadata\nFor example, if we want to select all of the data belonging only to samples in the Treatment group we can use the following:\n\n(trt &lt;- se[, se$Treatment == \"Treatment\"])\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample4 Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim of the object changed from 6 to 3. This is because we have selected only the Samples from the original SummarizedExperiment object from the treatment group. The cool thing about SummarizedExperiments is that all of the assays have also been subsetted to reflect this selection!\nTake a look at the “logcounts” assay. It only contains Samples 4, 5, and 6.\n\nassay(trt, \"logcounts\") |&gt; head()\n\n       Sample4  Sample5   Sample6\nGene1 10.53368 10.86301  8.189821\nGene2 11.69081 12.79256 11.532159\nGene3 12.08541 10.64925 11.604069\nGene4 11.02318 13.83504 12.886068\nGene5 12.94471 10.94872 12.474449\nGene6 10.84935 10.66975 11.672269\n\n\nOf course you can combine multiple conditions as well\n\nse[, se$Batch %in% c(\"B\", \"C\") & se$Age &gt; 10]\n\nclass: RangedSummarizedExperiment \ndim: 200 2 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(2): Sample2 Sample5\ncolData names(4): SampleName Treatment Age Batch\n\n\n\n\nSubsetting based on rows\nWe can also select certain features that we want to keep using row subsetting. For example to select only the first 50 rows:\n\nse[1:50, ]\n\nclass: RangedSummarizedExperiment \ndim: 50 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(50): Gene1 Gene2 ... Gene49 Gene50\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim changed from 200 to 50 reflecting the fact that we have only selected the first 50 rows.\nThis subsetting is very useful when combined with logical operators. Above we created a vector in rowData called “Keep” that contained TRUE if the corresponding row of the se object was a coding gene and FALSE otherwise. Let’s use this vector to subset our se object.\n\n(coding &lt;- se[rowData(se)$Keep, ])\n\nclass: RangedSummarizedExperiment \ndim: 68 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(68): Gene3 Gene5 ... Gene198 Gene199\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nAnd if we look at the resulting rowData we can see that it only contains the protein_coding features\n\nrowData(coding)\n\nDataFrame with 68 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene3         ID003 protein_coding      TRUE\nGene5         ID005 protein_coding      TRUE\nGene9         ID009 protein_coding      TRUE\nGene14        ID014 protein_coding      TRUE\nGene16        ID016 protein_coding      TRUE\n...             ...            ...       ...\nGene193       ID193 protein_coding      TRUE\nGene194       ID194 protein_coding      TRUE\nGene197       ID197 protein_coding      TRUE\nGene198       ID198 protein_coding      TRUE\nGene199       ID199 protein_coding      TRUE\n\n\nEach assay also reflects this operation\n\nassay(coding, \"cpm\") |&gt; head()\n\n        Sample1  Sample2   Sample3  Sample4   Sample5  Sample6\nGene3  1086.450 5499.319  6374.148 4345.802  1605.996 3112.956\nGene5  4039.323 5237.672 12778.726 7883.979  1976.480 5690.931\nGene9  7346.474 4039.323  2124.716 5897.874 12653.300 3063.544\nGene14 7042.928 3783.937  1747.827 4503.641 13527.575 8205.078\nGene16 6811.087 2184.784  3832.886 4966.418  8739.137 7905.328\nGene19 3705.900 2483.315  3211.991 6225.912  8381.189 1362.663\n\n\n\n\nSubsetting based on rowRanges\nA closely related row-wise subsetting operation can be used if you have a RangedSummarizedExperiment (a SummarizedExperiment with a rowRanges slot) that allows you to perform operations on a SummarizedExperiment object like you would a GRanges object.\nFor example, let’s say we only wanted to extract the features on Chromosome 2 only. Then we can use the GenomicRanges function subsetByOverlaps directly on our SummarizedExperiment object like so:\n\n# Region of interest\nroi &lt;- GRanges(seqnames = \"chr2\", ranges = 1:1e7)\n\n# Subset the SE object for only features on chr2\n(chr2 &lt;- subsetByOverlaps(se, roi))\n\nclass: RangedSummarizedExperiment \ndim: 150 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(150): Gene51 Gene52 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nYou can see again that the dim changed reflecting our selection. Again, all of the associated assays and rowData have also been subsetted reflecting this change as well.\n\nrowData(chr2)\n\nDataFrame with 150 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene51        ID051         lncRNA     FALSE\nGene52        ID052 protein_coding      TRUE\nGene53        ID053         lncRNA     FALSE\nGene54        ID054 protein_coding      TRUE\nGene55        ID055 repeat_element     FALSE\n...             ...            ...       ...\nGene196       ID196         lncRNA     FALSE\nGene197       ID197 protein_coding      TRUE\nGene198       ID198 protein_coding      TRUE\nGene199       ID199 protein_coding      TRUE\nGene200       ID200 repeat_element     FALSE\n\n\n\nassay(chr2, \"counts\") |&gt; head()\n\n       Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene51      86      84      48     150      62      87\nGene52     122      66     375     212      16     104\nGene53     176     268     141      45     219      87\nGene54      37      84      69      60     128     118\nGene55      77     226     135      67     103     164\nGene56      46      16      49      16      21      44\n\n\n\nrowRanges(chr2)\n\nGRanges object with 150 ranges and 3 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type      Keep\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\n   Gene51     chr2 778627-778726      - |       ID051         lncRNA     FALSE\n   Gene52     chr2 115047-115146      - |       ID052 protein_coding      TRUE\n   Gene53     chr2 694825-694924      + |       ID053         lncRNA     FALSE\n   Gene54     chr2 782291-782390      - |       ID054 protein_coding      TRUE\n   Gene55     chr2 335491-335590      - |       ID055 repeat_element     FALSE\n      ...      ...           ...    ... .         ...            ...       ...\n  Gene196     chr2 929350-929449      + |       ID196         lncRNA     FALSE\n  Gene197     chr2 545444-545543      + |       ID197 protein_coding      TRUE\n  Gene198     chr2 932266-932365      + |       ID198 protein_coding      TRUE\n  Gene199     chr2 463898-463997      + |       ID199 protein_coding      TRUE\n  Gene200     chr2 904679-904778      - |       ID200 repeat_element     FALSE\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\nThere’s also a few shortcuts on range operations using GRanges/SummarizedExperiments. See the help pages for %over, %within%, and %outside%. For example:\n\nall.equal(se[se %over% roi, ], subsetByOverlaps(se, roi))\n\n[1] TRUE\n\n\n\n\nCombining subsetting operations\nOf course you don’t have to perform one subsetting operation at a time. Like base R you can combine multiple expressions to subset a SummarizedExperiment object.\nFor example, to select only features labeled as repeat_elements and the Sample from ‘Batch’ A in the ‘Control’ group\n\n(selected &lt;- se[\n  rowData(se)$gene_type == \"repeat_element\",\n  se$Treatment == \"Control\" &\n    se$Batch == \"A\"\n])\n\nclass: RangedSummarizedExperiment \ndim: 65 1 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(65): Gene1 Gene2 ... Gene188 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(1): Sample1\ncolData names(4): SampleName Treatment Age Batch",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#saving-a-summarizedexperiment",
    "href": "summarized-experiments.html#saving-a-summarizedexperiment",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Saving a SummarizedExperiment",
    "text": "Saving a SummarizedExperiment\nSince SummarizedExperiments keep basically all information about an experiment in one place, it is convenient to save the entire SummarizedExperiment object so that you can pick up an analysis where you left off or even to facilitate better sharing of data between collaborators.\nYou can save the entire SummarizedExperiment object with:\nsaveRDS(se, \"/path/to/se.rds\")\nAnd when you want to read the same object back into R for your next analysis you can do so with:\nse &lt;- readRDS(\"/path/to/se.rds\")",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#summarizedexperiments-in-the-real-world",
    "href": "summarized-experiments.html#summarizedexperiments-in-the-real-world",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "SummarizedExperiments in the real world",
    "text": "SummarizedExperiments in the real world\nIf you’re working with any Bioconductor packages it’s likely that the object you’re working with either is a SummarizedExperiment or is inherited from one. For example, the DESeqDataSet from the DESeq2 package and BSseq objects from the bsseq package both inherit from a SummarizedExperiment and thus retain all of the same functionality as above. If you go to the SummarizedExperiment landing page and click “See More” under details you can see all of the packages that depend on SummarizedExperiment.\nAlso, many common methods are also implemented for SummarizedExperiment objects. For example, to simplify calculating counts-per-million above I could have simply used the edgeR::cpm() directly on the SummarizedExperiment object. Many functions in bioconductor packages know how to deal directly with SummarizedExperiments so you don’t ever have to take the trouble extracting components and performing tedious calculations yourself.\n\nassay(se, \"cpm\") &lt;- edgeR::cpm(se)\n\nI also left out any discussion of the metadata() slot of the SummarizedExperiment. The metadata slot is simply a list of any R object that contains information about the experiment. The metadata in the metadata slots are not subjected to the same subsetting rules as the other slots. In practice this assay contains additional information about the experiment as a whole. For example, I typically store bootstrap alignments for each sample here.\nTo add something to the SummarizedExperiment metadata slot you can do:\n\nmetadata(se)$additional_info &lt;- \"Experiment performed on 6 samples with three replicates each\"\n\nAnd to retrieve this:\n\nmetadata(se)$additional_info\n\n[1] \"Experiment performed on 6 samples with three replicates each\"",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#closing-thoughts",
    "href": "summarized-experiments.html#closing-thoughts",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nHopefully this was enough information to get you started using SummarizedExperiments. There’s many things I left out such as different backings for storing out of memory data, a tidyverse interface to SummarizedExperiment objects, TreeSummarizedExperiments for microbiome data, MultiAssayExperiments for dealing with experiments containing multiomics data, and much more.\nPlease let me know your thoughts and if anything needs more clarification.",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "r-basics.html#data-transformation",
    "href": "r-basics.html#data-transformation",
    "title": "R programming basics",
    "section": "Data transformation",
    "text": "Data transformation\nNow that we’ve got a handle on some different R data types, and how to slice and dice them, we can start learning basic data cleaning. We’ll focus on using data.frames since they’re the primary workhorse of data analysis in R. But remember, data.frames are just lists of vectors with some special rules, so many concepts you learn which apply to data.frames, also apply to vectors and lists.\n\nData import\nWe’ll use the built in penguins_raw dataset to learn some basic data cleaning. This dataset is built into R version 4.5 so you can just load it by running penguins_raw. However, to illustrate data import, we’ll read in the data from an external source.\nThe read.csv() function can read in comma-separated value files that are located either on your local machine or from remote sources if provided a URL.\n\nurl &lt;- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins_raw.csv\"\npenguins &lt;- read.csv(url)\n\n\n\nTaking a look at the data\nThe lines above read the data into data.frame that we called penguins. We can take a look at the first few rows of the penguins data.frame using the head() function.\n\nhead(penguins)\n\n  studyName Sample.Number                             Species Region    Island\n1   PAL0708             1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n2   PAL0708             2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n3   PAL0708             3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n4   PAL0708             4 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n5   PAL0708             5 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n6   PAL0708             6 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n               Stage Individual.ID Clutch.Completion   Date.Egg\n1 Adult, 1 Egg Stage          N1A1               Yes 2007-11-11\n2 Adult, 1 Egg Stage          N1A2               Yes 2007-11-11\n3 Adult, 1 Egg Stage          N2A1               Yes 2007-11-16\n4 Adult, 1 Egg Stage          N2A2               Yes 2007-11-16\n5 Adult, 1 Egg Stage          N3A1               Yes 2007-11-16\n6 Adult, 1 Egg Stage          N3A2               Yes 2007-11-16\n  Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.    Sex\n1               39.1              18.7                 181          3750   MALE\n2               39.5              17.4                 186          3800 FEMALE\n3               40.3              18.0                 195          3250 FEMALE\n4                 NA                NA                  NA            NA   &lt;NA&gt;\n5               36.7              19.3                 193          3450 FEMALE\n6               39.3              20.6                 190          3650   MALE\n  Delta.15.N..o.oo. Delta.13.C..o.oo.                       Comments\n1                NA                NA Not enough blood for isotopes.\n2           8.94956         -24.69454                           &lt;NA&gt;\n3           8.36821         -25.33302                           &lt;NA&gt;\n4                NA                NA             Adult not sampled.\n5           8.76651         -25.32426                           &lt;NA&gt;\n6           8.66496         -25.29805                           &lt;NA&gt;\n\n\nIf we want to get a general overview of the data, we can use the str() function.\n\nstr(penguins)\n\n'data.frame':   344 obs. of  17 variables:\n $ studyName          : chr  \"PAL0708\" \"PAL0708\" \"PAL0708\" \"PAL0708\" ...\n $ Sample.Number      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Species            : chr  \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" ...\n $ Region             : chr  \"Anvers\" \"Anvers\" \"Anvers\" \"Anvers\" ...\n $ Island             : chr  \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ Stage              : chr  \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" ...\n $ Individual.ID      : chr  \"N1A1\" \"N1A2\" \"N2A1\" \"N2A2\" ...\n $ Clutch.Completion  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Date.Egg           : chr  \"2007-11-11\" \"2007-11-11\" \"2007-11-16\" \"2007-11-16\" ...\n $ Culmen.Length..mm. : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ Culmen.Depth..mm.  : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ Flipper.Length..mm.: int  181 186 195 NA 193 190 181 195 193 190 ...\n $ Body.Mass..g.      : int  3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ Sex                : chr  \"MALE\" \"FEMALE\" \"FEMALE\" NA ...\n $ Delta.15.N..o.oo.  : num  NA 8.95 8.37 NA 8.77 ...\n $ Delta.13.C..o.oo.  : num  NA -24.7 -25.3 NA -25.3 ...\n $ Comments           : chr  \"Not enough blood for isotopes.\" NA NA \"Adult not sampled.\" ...\n\n\nThere are a few external packages that are also very useful for getting summaries of data.frames. Hmsic::describe() and skimr::skim() are two standouts.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#exploratory-data-analysis",
    "href": "r-basics.html#exploratory-data-analysis",
    "title": "R programming basics",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nNow that we’ve got a handle on some different R data types, and how to slice and dice them, we can start learning basic data cleaning and exploratory data analysis. We’ll focus on using data.frames since they’re the primary workhorse of data analysis in R. But remember, data.frames are just lists of vectors with some special rules, so many concepts you learn will apply to data.frames but also apply to vectors and lists.\n\nData import\nWe’ll use the built in penguins_raw dataset to learn some basic data cleaning. This dataset is built into R version 4.5 so you can just load it by running penguins_raw if you have that R version installed. However, to illustrate data import, we’ll read in the data from an external source.\nThe read.csv() function can read in comma-separated value files that are located either on your local machine or from remote sources if provided a URL.\n\nurl &lt;- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins_raw.csv\"\npenguins &lt;- read.csv(url)\n\nThe read.csv() function has many options for reading in data. If you want to learn about all of the options any particular R function has, you can prefix the function name with a ? like, ?read.csv() to bring up the help documentation.\n\n\nViewing data\nThe code above read the data into data.frame that we called penguins. We can take a look at the first few rows of the penguins data.frame using the head() function.\n\nhead(penguins)\n\n  studyName Sample.Number                             Species Region    Island\n1   PAL0708             1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n2   PAL0708             2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n3   PAL0708             3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n4   PAL0708             4 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n5   PAL0708             5 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n6   PAL0708             6 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n               Stage Individual.ID Clutch.Completion   Date.Egg\n1 Adult, 1 Egg Stage          N1A1               Yes 2007-11-11\n2 Adult, 1 Egg Stage          N1A2               Yes 2007-11-11\n3 Adult, 1 Egg Stage          N2A1               Yes 2007-11-16\n4 Adult, 1 Egg Stage          N2A2               Yes 2007-11-16\n5 Adult, 1 Egg Stage          N3A1               Yes 2007-11-16\n6 Adult, 1 Egg Stage          N3A2               Yes 2007-11-16\n  Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.    Sex\n1               39.1              18.7                 181          3750   MALE\n2               39.5              17.4                 186          3800 FEMALE\n3               40.3              18.0                 195          3250 FEMALE\n4                 NA                NA                  NA            NA   &lt;NA&gt;\n5               36.7              19.3                 193          3450 FEMALE\n6               39.3              20.6                 190          3650   MALE\n  Delta.15.N..o.oo. Delta.13.C..o.oo.                       Comments\n1                NA                NA Not enough blood for isotopes.\n2           8.94956         -24.69454                           &lt;NA&gt;\n3           8.36821         -25.33302                           &lt;NA&gt;\n4                NA                NA             Adult not sampled.\n5           8.76651         -25.32426                           &lt;NA&gt;\n6           8.66496         -25.29805                           &lt;NA&gt;\n\n\nIf we want to get a general overview of the data, we can use the str() function.\n\nstr(penguins)\n\n'data.frame':   344 obs. of  17 variables:\n $ studyName          : chr  \"PAL0708\" \"PAL0708\" \"PAL0708\" \"PAL0708\" ...\n $ Sample.Number      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Species            : chr  \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" ...\n $ Region             : chr  \"Anvers\" \"Anvers\" \"Anvers\" \"Anvers\" ...\n $ Island             : chr  \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ Stage              : chr  \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" ...\n $ Individual.ID      : chr  \"N1A1\" \"N1A2\" \"N2A1\" \"N2A2\" ...\n $ Clutch.Completion  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Date.Egg           : chr  \"2007-11-11\" \"2007-11-11\" \"2007-11-16\" \"2007-11-16\" ...\n $ Culmen.Length..mm. : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ Culmen.Depth..mm.  : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ Flipper.Length..mm.: int  181 186 195 NA 193 190 181 195 193 190 ...\n $ Body.Mass..g.      : int  3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ Sex                : chr  \"MALE\" \"FEMALE\" \"FEMALE\" NA ...\n $ Delta.15.N..o.oo.  : num  NA 8.95 8.37 NA 8.77 ...\n $ Delta.13.C..o.oo.  : num  NA -24.7 -25.3 NA -25.3 ...\n $ Comments           : chr  \"Not enough blood for isotopes.\" NA NA \"Adult not sampled.\" ...\n\n\nThere are a few external packages that are also very useful for getting summaries of data.frames. Hmsic::describe() and skimr::skim() are two standouts.\nOne of the most basic ways to get an idea of the data is to summarize each variable. There are a few functions we can use to get summaries of the data. The table() function will count the number of occurrences of each type in a vector.\nFor example, how many observations of each species of penguin are in the dataset?\n\ntable(penguins$Species)\n\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                      152 \nChinstrap penguin (Pygoscelis antarctica) \n                                       68 \n        Gentoo penguin (Pygoscelis papua) \n                                      124 \n\n\nR also provides the typical summary functions that you would expect from a statistical programming language such as mean(), median(), min(), and max(), and length().\nFor example, what is the mean flipper length?\n\nmean(penguins$Flipper.Length..mm.)\n\n[1] NA\n\n\nOn no! This returned NA but there is clearly data in this column. What happened? Missing data is commonly observed across all data domains. NAs simply represent unknown values in this context and it’s impossible to know how to take the mean of a value that’s known with a value that’s unknown. It is for this reason that many R functions have an argument called na.rm=. Setting na.rm=TRUE in these functions tells R to ignore the NA values.\n\nmean(penguins$Flipper.Length..mm., na.rm = TRUE)\n\n[1] 200.9152\n\n\nNow we can see that the mean flipper length is ~200 mm across all observations in the dataset. Another useful function is summary(). Running summary() on a numeric vector returns a lot of useful information.\n\nsummary(penguins$Flipper.Length..mm., na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nFinally, if you are using and IDE like Rstudio or Positron you can run the View() function on your data.frame. This will bring up an interactive data viewer.\n\n\nSplitting data\nYou may have noticed above that we computed the mean flipper length across all species of penguins. But do all species have the same mean? A common pattern in R is called “split-apply-combine”. This pattern means, split the data into groups you’re interested in, apply a function to each of those groups, and combine the results. One such function that performs this operation is called tapply(). tapply() will split the data by a given variable into groups and apply a function to each group.\nFor example, to find the mean flipper length for each species we could use\n\ntapply(penguins$Flipper.Length..mm., penguins$Species, mean, na.rm = TRUE)\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                 189.9536 \nChinstrap penguin (Pygoscelis antarctica) \n                                 195.8235 \n        Gentoo penguin (Pygoscelis papua) \n                                 217.1870 \n\n\nThe basic format of the tapply() function is\n\ntapply(\"data to split\", \"what to split by\", \"what to compute\")\n\nSplitting can also by applied directly to data.frames using the split() function. Let’s say we wanted to split the penguins data.frame into one data.frame for each species. We can use the split() function for this purpose.\n\nby_species &lt;- split(penguins, f = penguins$Species)\n\nThis function returns a list of data.frames, one for each species in the original data.frame. Use names(by_species) to see what each list element is named. To extract the first data.frame from this list, which contains Adelie penguin data only, we can subset the list of data.frames.\n\nadelie &lt;- by_species[[1]]\n\nPerforming operations on lists is such a common task in R that a function exists specifically to apply functions to list elements. This function is called lapply(). lapply() takes a list and a function and applies that function to each list element. The lapply() function returns the results as a list.\nFor example, to see how many rows are in each of the data.frames in the “by_species” list:\n\nlapply(by_species, nrow)\n\n$`Adelie Penguin (Pygoscelis adeliae)`\n[1] 152\n\n$`Chinstrap penguin (Pygoscelis antarctica)`\n[1] 68\n\n$`Gentoo penguin (Pygoscelis papua)`\n[1] 124\n\n\n\n\nPlotting data\nData cleaning and data visualization go hand-in-hand. To effectively clean data, you should be examining the changes you’re making in real time. Base R actually has very powerful graphics capabilities for quickly visualizing data. Packages like ggplot2 and lattice provide powerful alternatives to base R plots. We’ll cover ggplot2 later. For now, base R plotting can provide all we need for exploratory analyses.\nOne of the most useful plots for numeric data is a histogram. Histograms bin the data and plot how many occurrences of a particular bin are present. This plot allows you to get an idea of the numeric summary of a variable. To plot a histogram of a numeric variable we can use the hist() function.\n\nhist(penguins$Flipper.Length..mm.)\n\n\n\n\n\n\n\n\nThe histogram has a few arguments we can use to adjust the plot. Use ?hist() to see the full list. Below, we can adjust the axes to be more informative and modify the number of bins we’re computing.\n\nhist(penguins$Flipper.Length..mm., \n     breaks = 30,\n     main = \"Flipper Length of All Palmer Penguins\", \n     xlab = \"Flipper Length (mm)\",\n     ylab = \"Count\"\n     )\n\n\n\n\n\n\n\n\nThe distribution appears to be bimodal. Is this because there are different species present in this plot? We can check by applying the hist() function to each of the groups using the list of data.frames from above.\n\n# Adelie penguins \nhist(by_species[[1]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Adelie\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n# Chinstrap penguins\nhist(by_species[[2]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Chinstrap\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n# Gentoo penguins\nhist(by_species[[3]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Gentoo\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n\nHistograms are useful for plotting the distribution of a single numeric variable. Often, we wish to see how two variables are related. The plot() function provides a way to create a scatter plot of two variables against each other on the same plot. For example, to plot the culmen length vs the culmen depth:\n\nplot(x = penguins$Culmen.Length..mm., \n     y = penguins$Culmen.Depth..mm.,\n     xlab = \"Culmen Length (mm)\",\n     ylab = \"Culmen Depth (mm)\",\n     main = \"Relationship between culmen length and depth\"\n     )\n\n\n\n\n\n\n\n\nAgain, these data points seem to be split by the different species of penguins present in the dataset. We can color these points using an additional variable in the call to the plot() function.\n\nplot(x = penguins$Culmen.Length..mm., \n     y = penguins$Culmen.Depth..mm.,\n     col = factor(penguins$Species),\n     xlab = \"Culmen Length (mm)\",\n     ylab = \"Culmen Depth (mm)\",\n     main = \"Relationship between culmen length and depth\"\n     )\n\n\n\n\n\n\n\n\nYou may have noticed that we did something special to that vector of species. We wrapped it in the factor() function. In R, factors are used when we have categorical values. R uses factors to represent the different levels of each category. It may not seem important now, but factors are very useful for statistical analysis. We’ll build on this topic shortly.\nThe plot() function is very versatile. You can use it to create line plots as well, to show trends of variables over time. Here is a contrived example of using plot() to create a simple line chart.\n\nplot(\n  x = adelie$Sample.Number, \n  y = adelie$Body.Mass..g., \n  type = \"l\", \n  main = \"Body mass vs sample number in Adelie penguins\",\n  xlab = \"Sample Number\",\n  ylab = \"Body Mass (g)\"\n  )\n\n\n\n\n\n\n\n\nScatter plots are useful for displaying two numeric values against each other. However you’ll often want to compare numeric values across categorical values. One such plot for doing this is called a boxplot. Boxplots show the median and interquartile (IQR) range for a set of data. The ‘whiskers’ of the boxplot display the 1.5 x IQR of the data. We can plot a boxplot of the flipper length for each species using the boxplot() function.\n\nboxplot(\n  Flipper.Length..mm. ~ Species, \n  data = penguins, \n  main = \"Flipper length by species\"\n  )\n\n\n\n\n\n\n\n\nThe boxplot() function is a little funny because it uses formula notation. Many functions in R accept formula notation as input. In this case, the formula notation means “plot the flipper length as a function of the species type”. You may have also noticed that we gave this function the penguins data.frame as the value for the data= argument. This allowed the boxplot function to use the column names without us having to explicitly say that they came from the penguins data.frame.\nIf there’s not too much data, boxplots can actually hide a lot of information. In this case, another option is to use a stripchart. A stripchart shows every data point on the plot instead of a depiction of the distribution as in the boxplot.\n\nstripchart(\n  Flipper.Length..mm. ~ Species, \n  data = penguins, \n  method = \"jitter\",\n  pch = 1,\n  main = \"Flipper length by species\",\n  vertical = TRUE\n  )\n\n\n\n\n\n\n\n\nBar charts are another common way that people show summaries of data. To display a barplot of data, we can use the barplot() function. To create a barplot of the mean flipper length for each species we first need to summarize the flipper length for each group and then supply these summaries to the barplot() function.\n\nspecies_means &lt;- tapply(penguins$Flipper.Length..mm., penguins$Species, mean, na.rm = TRUE)\nbarplot(species_means, main = \"Mean flipper length by species\")\n\n\n\n\n\n\n\n\nThere are many ways to modify these default plot types and make the charts look great. Take the time to look into some of base R graphing capabilities. A really good tutorial for learning about base R plotting can be found here.\n\n\nFiltering data\nWe’ve already learned how to subset a data.frame but let’s just take a step back and quickly revisit subsetting data.frames. To subset a data.frame means to select rows of a data.frame based on some condition that you’re interested in. This subsetting can be accomplished using the [ operator and setting conditions by specifying df[the rows we want, the cols we want] syntax.\nFor example, in the above plots it looked like the Gentoo penguins typically had the largest flippers. It looked like a value &gt; 205 roughly separated the Gentoo penguins from the others. How could we select the rows where the flipper length is greater than or equal to 205 and count the species types?\n\nbig_flippers &lt;- penguins[penguins$Flipper.Length..mm. &gt;= 205, ]\ntable(big_flippers$Species)\n\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                        3 \nChinstrap penguin (Pygoscelis antarctica) \n                                        8 \n        Gentoo penguin (Pygoscelis papua) \n                                      122 \n\n\nWe can combine conditions as well to select on multiple conditions. For example, let’s select the female penguins with flippers &gt;= 205 mm.\n\nhead(penguins[penguins$Sex == \"FEMALE\" & penguins$Flipper.Length..mm. &gt;= 205, ])\n\n    studyName Sample.Number                           Species Region Island\nNA       &lt;NA&gt;            NA                              &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n153   PAL0708             1 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n155   PAL0708             3 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n158   PAL0708             6 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n159   PAL0708             7 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n161   PAL0708             9 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n                 Stage Individual.ID Clutch.Completion   Date.Egg\nNA                &lt;NA&gt;          &lt;NA&gt;              &lt;NA&gt;       &lt;NA&gt;\n153 Adult, 1 Egg Stage         N31A1               Yes 2007-11-27\n155 Adult, 1 Egg Stage         N32A1               Yes 2007-11-27\n158 Adult, 1 Egg Stage         N33A2               Yes 2007-11-18\n159 Adult, 1 Egg Stage         N34A1               Yes 2007-11-27\n161 Adult, 1 Egg Stage         N35A1               Yes 2007-11-27\n    Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.\nNA                  NA                NA                  NA            NA\n153               46.1              13.2                 211          4500\n155               48.7              14.1                 210          4450\n158               46.5              13.5                 210          4550\n159               45.4              14.6                 211          4800\n161               43.3              13.4                 209          4400\n       Sex Delta.15.N..o.oo. Delta.13.C..o.oo. Comments\nNA    &lt;NA&gt;                NA                NA     &lt;NA&gt;\n153 FEMALE           7.99300         -25.51390     &lt;NA&gt;\n155 FEMALE           8.14705         -25.46172     &lt;NA&gt;\n158 FEMALE           7.99530         -25.32829     &lt;NA&gt;\n159 FEMALE           8.24515         -25.46782     &lt;NA&gt;\n161 FEMALE           8.13643         -25.32176     &lt;NA&gt;",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#data-wrangling",
    "href": "r-basics.html#data-wrangling",
    "title": "R programming basics",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nNow that we’ve got a handle on some different R data types, and how to slice and dice them, we can start learning basic data cleaning and exploratory data analysis. We’ll focus on using data.frames since they’re the primary workhorse of data analysis in R. But remember, data.frames are just lists of vectors with some special rules, so many concepts you learn will apply to data.frames but also apply to vectors and lists.\n\nData import\nWe’ll use the built in penguins_raw dataset to learn some basic data cleaning. This dataset is built into R version 4.5 so you can just load it by running penguins_raw if you have that R version installed. However, to illustrate data import, we’ll read in the data from an external source.\nThe read.csv() function can read in comma-separated value files that are located either on your local machine or from remote sources if provided a URL.\n\nurl &lt;- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins_raw.csv\"\npenguins &lt;- read.csv(url)\n\nThe read.csv() function has many options for reading in data. If you want to learn about all of the options any particular R function has, you can prefix the function name with a ? like, ?read.csv() to bring up the help documentation.\n\n\nViewing data\nThe code above read the data into data.frame that we called penguins. We can take a look at the first few rows of the penguins data.frame using the head() function.\n\nhead(penguins)\n\n  studyName Sample.Number                             Species Region    Island\n1   PAL0708             1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n2   PAL0708             2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n3   PAL0708             3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n4   PAL0708             4 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n5   PAL0708             5 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n6   PAL0708             6 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n               Stage Individual.ID Clutch.Completion   Date.Egg\n1 Adult, 1 Egg Stage          N1A1               Yes 2007-11-11\n2 Adult, 1 Egg Stage          N1A2               Yes 2007-11-11\n3 Adult, 1 Egg Stage          N2A1               Yes 2007-11-16\n4 Adult, 1 Egg Stage          N2A2               Yes 2007-11-16\n5 Adult, 1 Egg Stage          N3A1               Yes 2007-11-16\n6 Adult, 1 Egg Stage          N3A2               Yes 2007-11-16\n  Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.    Sex\n1               39.1              18.7                 181          3750   MALE\n2               39.5              17.4                 186          3800 FEMALE\n3               40.3              18.0                 195          3250 FEMALE\n4                 NA                NA                  NA            NA   &lt;NA&gt;\n5               36.7              19.3                 193          3450 FEMALE\n6               39.3              20.6                 190          3650   MALE\n  Delta.15.N..o.oo. Delta.13.C..o.oo.                       Comments\n1                NA                NA Not enough blood for isotopes.\n2           8.94956         -24.69454                           &lt;NA&gt;\n3           8.36821         -25.33302                           &lt;NA&gt;\n4                NA                NA             Adult not sampled.\n5           8.76651         -25.32426                           &lt;NA&gt;\n6           8.66496         -25.29805                           &lt;NA&gt;\n\n\nIf we want to get a general overview of the data, we can use the str() function.\n\nstr(penguins)\n\n'data.frame':   344 obs. of  17 variables:\n $ studyName          : chr  \"PAL0708\" \"PAL0708\" \"PAL0708\" \"PAL0708\" ...\n $ Sample.Number      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Species            : chr  \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" ...\n $ Region             : chr  \"Anvers\" \"Anvers\" \"Anvers\" \"Anvers\" ...\n $ Island             : chr  \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ Stage              : chr  \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" ...\n $ Individual.ID      : chr  \"N1A1\" \"N1A2\" \"N2A1\" \"N2A2\" ...\n $ Clutch.Completion  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Date.Egg           : chr  \"2007-11-11\" \"2007-11-11\" \"2007-11-16\" \"2007-11-16\" ...\n $ Culmen.Length..mm. : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ Culmen.Depth..mm.  : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ Flipper.Length..mm.: int  181 186 195 NA 193 190 181 195 193 190 ...\n $ Body.Mass..g.      : int  3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ Sex                : chr  \"MALE\" \"FEMALE\" \"FEMALE\" NA ...\n $ Delta.15.N..o.oo.  : num  NA 8.95 8.37 NA 8.77 ...\n $ Delta.13.C..o.oo.  : num  NA -24.7 -25.3 NA -25.3 ...\n $ Comments           : chr  \"Not enough blood for isotopes.\" NA NA \"Adult not sampled.\" ...\n\n\nThere are a few external packages that are also very useful for getting summaries of data.frames. Hmsic::describe() and skimr::skim() are two standouts.\nOne of the most basic ways to get an idea of the data is to summarize each variable. There are a few functions we can use to get summaries of the data. The table() function will count the number of occurrences of each type in a vector.\nFor example, how many observations of each species of penguin are in the dataset?\n\ntable(penguins$Species)\n\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                      152 \nChinstrap penguin (Pygoscelis antarctica) \n                                       68 \n        Gentoo penguin (Pygoscelis papua) \n                                      124 \n\n\nR also provides the typical summary functions that you would expect from a statistical programming language such as mean(), median(), min(), and max(), and length().\nFor example, what is the mean flipper length?\n\nmean(penguins$Flipper.Length..mm.)\n\n[1] NA\n\n\nOn no! This returned NA but there is clearly data in this column. What happened? Missing data is commonly observed across all data domains. NAs simply represent unknown values in this context and it’s impossible to know how to take the mean of a value that’s known with a value that’s unknown. It is for this reason that many R functions have an argument called na.rm=. Setting na.rm=TRUE in these functions tells R to ignore the NA values.\n\nmean(penguins$Flipper.Length..mm., na.rm = TRUE)\n\n[1] 200.9152\n\n\nNow we can see that the mean flipper length is ~200 mm across all observations in the dataset. Another useful function is summary(). Running summary() on a numeric vector returns a lot of useful information.\n\nsummary(penguins$Flipper.Length..mm., na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nFinally, if you are using and IDE like Rstudio or Positron you can run the View() function on your data.frame. This will bring up an interactive data viewer.\n\n\nSplitting data\nYou may have noticed above that we computed the mean flipper length across all species of penguins. But do all species have the same mean? A common pattern in R is called “split-apply-combine”. This pattern means, split the data into groups you’re interested in, apply a function to each of those groups, and combine the results. One such function that performs this operation is called tapply(). tapply() will split the data by a given variable into groups and apply a function to each group.\nFor example, to find the mean flipper length for each species we could use\n\ntapply(penguins$Flipper.Length..mm., penguins$Species, mean, na.rm = TRUE)\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                 189.9536 \nChinstrap penguin (Pygoscelis antarctica) \n                                 195.8235 \n        Gentoo penguin (Pygoscelis papua) \n                                 217.1870 \n\n\nThe basic format of the tapply() function is\n\ntapply(\"data to split\", \"what to split by\", \"what to compute\")\n\nSplitting can also by applied directly to data.frames using the split() function. Let’s say we wanted to split the penguins data.frame into one data.frame for each species. We can use the split() function for this purpose.\n\nby_species &lt;- split(penguins, f = penguins$Species)\n\nThis function returns a list of data.frames, one for each species in the original data.frame. Use names(by_species) to see what each list element is named. To extract the first data.frame from this list, which contains Adelie penguin data only, we can subset the list of data.frames.\n\nadelie &lt;- by_species[[1]]\n\nPerforming operations on lists is such a common task in R that a function exists specifically to apply functions to list elements. This function is called lapply(). lapply() takes a list and a function and applies that function to each list element. The lapply() function returns the results as a list.\nFor example, to see how many rows are in each of the data.frames in the “by_species” list:\n\nlapply(by_species, nrow)\n\n$`Adelie Penguin (Pygoscelis adeliae)`\n[1] 152\n\n$`Chinstrap penguin (Pygoscelis antarctica)`\n[1] 68\n\n$`Gentoo penguin (Pygoscelis papua)`\n[1] 124\n\n\n\n\nPlotting data\nData cleaning and data visualization go hand-in-hand. To effectively clean data, you should be examining the changes you’re making in real time. Base R actually has very powerful graphics capabilities for quickly visualizing data. Packages like ggplot2 and lattice provide powerful alternatives to base R plots. We’ll cover ggplot2 later. For now, base R plotting can provide all we need for exploratory analyses.\nOne of the most useful plots for numeric data is a histogram. Histograms bin the data and plot how many occurrences of a particular bin are present. This plot allows you to get an idea of the numeric summary of a variable. To plot a histogram of a numeric variable we can use the hist() function.\n\nhist(penguins$Flipper.Length..mm.)\n\n\n\n\n\n\n\n\nThe histogram has a few arguments we can use to adjust the plot. Use ?hist() to see the full list. Below, we can adjust the axes to be more informative and modify the number of bins we’re computing.\n\nhist(penguins$Flipper.Length..mm., \n     breaks = 30,\n     main = \"Flipper Length of All Palmer Penguins\", \n     xlab = \"Flipper Length (mm)\",\n     ylab = \"Count\"\n     )\n\n\n\n\n\n\n\n\nThe distribution appears to be bimodal. Is this because there are different species present in this plot? We can check by applying the hist() function to each of the groups using the list of data.frames from above.\n\n# Adelie penguins \nhist(by_species[[1]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Adelie\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n# Chinstrap penguins\nhist(by_species[[2]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Chinstrap\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n# Gentoo penguins\nhist(by_species[[3]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Gentoo\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n\nHistograms are useful for plotting the distribution of a single numeric variable. Often, we wish to see how two variables are related. The plot() function provides a way to create a scatter plot of two variables against each other on the same plot. For example, to plot the culmen length vs the culmen depth:\n\nplot(x = penguins$Culmen.Length..mm., \n     y = penguins$Culmen.Depth..mm.,\n     xlab = \"Culmen Length (mm)\",\n     ylab = \"Culmen Depth (mm)\",\n     main = \"Relationship between culmen length and depth\"\n     )\n\n\n\n\n\n\n\n\nAgain, these data points seem to be split by the different species of penguins present in the dataset. We can color these points using an additional variable in the call to the plot() function.\n\nplot(x = penguins$Culmen.Length..mm., \n     y = penguins$Culmen.Depth..mm.,\n     col = factor(penguins$Species),\n     xlab = \"Culmen Length (mm)\",\n     ylab = \"Culmen Depth (mm)\",\n     main = \"Relationship between culmen length and depth\"\n     )\n\n\n\n\n\n\n\n\nYou may have noticed that we did something special to that vector of species. We wrapped it in the factor() function. In R, factors are used when we have categorical values. R uses factors to represent the different levels of each category. It may not seem important now, but factors are very useful for statistical analysis. We’ll build on this topic shortly.\nThe plot() function is very versatile. You can use it to create line plots as well, to show trends of variables over time. Here is a contrived example of using plot() to create a simple line chart.\n\nplot(\n  x = adelie$Sample.Number, \n  y = adelie$Body.Mass..g., \n  type = \"l\", \n  main = \"Body mass vs sample number in Adelie penguins\",\n  xlab = \"Sample Number\",\n  ylab = \"Body Mass (g)\"\n  )\n\n\n\n\n\n\n\n\nScatter plots are useful for displaying two numeric values against each other. However you’ll often want to compare numeric values across categorical values. One such plot for doing this is called a boxplot. Boxplots show the median and interquartile (IQR) range for a set of data. The ‘whiskers’ of the boxplot display the 1.5 x IQR of the data. We can plot a boxplot of the flipper length for each species using the boxplot() function.\n\nboxplot(\n  Flipper.Length..mm. ~ Species, \n  data = penguins, \n  main = \"Flipper length by species\"\n  )\n\n\n\n\n\n\n\n\nThe boxplot() function is a little funny because it uses formula notation. Many functions in R accept formula notation as input. In this case, the formula notation means “plot the flipper length as a function of the species type”. You may have also noticed that we gave this function the penguins data.frame as the value for the data= argument. This allowed the boxplot function to use the column names without us having to explicitly say that they came from the penguins data.frame.\nIf there’s not too much data, boxplots can actually hide a lot of information. In this case, another option is to use a stripchart. A stripchart shows every data point on the plot instead of a depiction of the distribution as in the boxplot.\n\nstripchart(\n  Flipper.Length..mm. ~ Species, \n  data = penguins, \n  method = \"jitter\",\n  pch = 1,\n  main = \"Flipper length by species\",\n  vertical = TRUE\n  )\n\n\n\n\n\n\n\n\nBar charts are another common way that people show summaries of data. To display a barplot of data, we can use the barplot() function. To create a barplot of the mean flipper length for each species we first need to summarize the flipper length for each group and then supply these summaries to the barplot() function.\n\nspecies_means &lt;- tapply(penguins$Flipper.Length..mm., penguins$Species, mean, na.rm = TRUE)\nbarplot(species_means, main = \"Mean flipper length by species\")\n\n\n\n\n\n\n\n\nThere are many ways to modify these default plot types and make the charts look great. Take the time to look into some of base R graphing capabilities. A really good tutorial for learning about base R plotting can be found here.\n\n\nFiltering data\nWe’ve already learned how to subset a data.frame but let’s just take a step back and quickly revisit subsetting data.frames. To subset a data.frame means to select rows of a data.frame based on some condition that you’re interested in. This subsetting can be accomplished using the [ operator and setting conditions by specifying df[the rows we want, the cols we want] syntax.\nFor example, in the above plots it looked like the Gentoo penguins typically had the largest flippers. It looked like a value &gt; 205 roughly separated the Gentoo penguins from the others. How could we select the rows where the flipper length is greater than or equal to 205 and count the species types?\n\nbig_flippers &lt;- penguins[penguins$Flipper.Length..mm. &gt;= 205, ]\ntable(big_flippers$Species)\n\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                        3 \nChinstrap penguin (Pygoscelis antarctica) \n                                        8 \n        Gentoo penguin (Pygoscelis papua) \n                                      122 \n\n\nWe can combine conditions as well to select on multiple conditions. For example, let’s select the female penguins with flippers &gt;= 205 mm.\n\nhead(penguins[penguins$Sex == \"FEMALE\" & penguins$Flipper.Length..mm. &gt;= 205, ])\n\n    studyName Sample.Number                           Species Region Island\nNA       &lt;NA&gt;            NA                              &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n153   PAL0708             1 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n155   PAL0708             3 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n158   PAL0708             6 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n159   PAL0708             7 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n161   PAL0708             9 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n                 Stage Individual.ID Clutch.Completion   Date.Egg\nNA                &lt;NA&gt;          &lt;NA&gt;              &lt;NA&gt;       &lt;NA&gt;\n153 Adult, 1 Egg Stage         N31A1               Yes 2007-11-27\n155 Adult, 1 Egg Stage         N32A1               Yes 2007-11-27\n158 Adult, 1 Egg Stage         N33A2               Yes 2007-11-18\n159 Adult, 1 Egg Stage         N34A1               Yes 2007-11-27\n161 Adult, 1 Egg Stage         N35A1               Yes 2007-11-27\n    Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.\nNA                  NA                NA                  NA            NA\n153               46.1              13.2                 211          4500\n155               48.7              14.1                 210          4450\n158               46.5              13.5                 210          4550\n159               45.4              14.6                 211          4800\n161               43.3              13.4                 209          4400\n       Sex Delta.15.N..o.oo. Delta.13.C..o.oo. Comments\nNA    &lt;NA&gt;                NA                NA     &lt;NA&gt;\n153 FEMALE           7.99300         -25.51390     &lt;NA&gt;\n155 FEMALE           8.14705         -25.46172     &lt;NA&gt;\n158 FEMALE           7.99530         -25.32829     &lt;NA&gt;\n159 FEMALE           8.24515         -25.46782     &lt;NA&gt;\n161 FEMALE           8.13643         -25.32176     &lt;NA&gt;\n\n\n\n\nReordering data\nWe can also change the order of the rows in a data.frame. This is similar to subsetting but instead rearranges the data based on some condition. To reorder the data.frame we can use the order() function.\nFor example, we can order the data by decreasing flipper length\n\nhead(penguins[order(penguins$Flipper.Length..mm., decreasing = TRUE), ])\n\n    studyName Sample.Number                           Species Region Island\n216   PAL0809            64 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n154   PAL0708             2 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n186   PAL0708            34 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n218   PAL0809            66 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n228   PAL0809            76 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n242   PAL0910            90 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n                 Stage Individual.ID Clutch.Completion   Date.Egg\n216 Adult, 1 Egg Stage         N19A2               Yes 2008-11-13\n154 Adult, 1 Egg Stage         N31A2               Yes 2007-11-27\n186 Adult, 1 Egg Stage         N56A2               Yes 2007-12-03\n218 Adult, 1 Egg Stage         N20A2               Yes 2008-11-04\n228 Adult, 1 Egg Stage         N56A2               Yes 2008-11-06\n242 Adult, 1 Egg Stage         N14A2               Yes 2009-11-25\n    Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.  Sex\n216               54.3              15.7                 231          5650 MALE\n154               50.0              16.3                 230          5700 MALE\n186               59.6              17.0                 230          6050 MALE\n218               49.8              16.8                 230          5700 MALE\n228               48.6              16.0                 230          5800 MALE\n242               52.1              17.0                 230          5550 MALE\n    Delta.15.N..o.oo. Delta.13.C..o.oo. Comments\n216           8.49662         -26.84166     &lt;NA&gt;\n154           8.14756         -25.39369     &lt;NA&gt;\n186           7.76843         -25.68210     &lt;NA&gt;\n218           8.47067         -26.69166     &lt;NA&gt;\n228           8.59640         -26.71199     &lt;NA&gt;\n242           8.27595         -26.11657     &lt;NA&gt;\n\n\nR also provides the sort() function. See if you can understand the difference between order() and sort().\n\n\nSelecting columns of data\n\n\nModifying columns of data",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#string-operations",
    "href": "r-basics.html#string-operations",
    "title": "R programming basics",
    "section": "String operations",
    "text": "String operations",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#useful-functions",
    "href": "r-basics.html#useful-functions",
    "title": "R programming basics",
    "section": "Useful functions",
    "text": "Useful functions",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#tips-and-tricks",
    "href": "r-basics.html#tips-and-tricks",
    "title": "R programming basics",
    "section": "Tips and tricks",
    "text": "Tips and tricks",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#functions",
    "href": "r-basics.html#functions",
    "title": "R programming basics",
    "section": "Functions",
    "text": "Functions",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#functional-programming",
    "href": "r-basics.html#functional-programming",
    "title": "R programming basics",
    "section": "Functional programming",
    "text": "Functional programming",
    "crumbs": [
      "R programming basics"
    ]
  }
]