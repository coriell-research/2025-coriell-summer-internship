[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2025 Coriell Bioinformatics Internship",
    "section": "",
    "text": "Welcome!\nWe’re excited to have you as an intern and we hope that you’ll learn a lot with us this summer. We’ll use this book as a kind of reference manual for the things we think you should have a little background knowledge about before starting on an interesting project.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "data-best-practices.html",
    "href": "data-best-practices.html",
    "title": "‘Best’ practices for data projects",
    "section": "",
    "text": "Quick note (TODO)\nScience requires reproducibility, not only to ensure that your results are generalizable but also to make your life easier. One overlooked aspect of of learning data science is creating consistent and clear organization of your projects and learning how to keep your data safe and easily searchable.\nCitations incoming. Please note that much of this document does not contain original material. References are listed at the end of the chapter (I still need to get my .bib files in order to give proper attribution).",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#project-organization",
    "href": "data-best-practices.html#project-organization",
    "title": "1  Organizing projects",
    "section": "1.1 Project organization",
    "text": "1.1 Project organization\nScience requires reproducibility. not only to ensure that your results are generalizable but also to make your own life easier. One overlooked aspect of of learning data science is creating consistent and clear organization of your projects."
  },
  {
    "objectID": "data-best-practices.html#why-care-about-data-management",
    "href": "data-best-practices.html#why-care-about-data-management",
    "title": "‘Best’ practices for data projects",
    "section": "Why care about data management?",
    "text": "Why care about data management?\nComputing is now an essential part of research. This is outlined beautifully in the paper, “All biology is computational biology” by Florian Markowetz. Data is getting bigger and bigger and we need to be equipped with the tools for storing, manipulating, and communicating insights derived from it. However, most researchers are never taught good computational practices. Computational best practices are imperitive. Implementing best (or good enough) practices can improve reproducibility, ensure correctness, and increase efficiency.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#topics",
    "href": "data-best-practices.html#topics",
    "title": "1  Organizing projects",
    "section": "1.2 Topics",
    "text": "1.2 Topics\n\nData management\nProject organization\nTracking changes\nSoftware\nManuscripts"
  },
  {
    "objectID": "data-best-practices.html#save-and-lock-all-raw-data",
    "href": "data-best-practices.html#save-and-lock-all-raw-data",
    "title": "‘Best’ practices for data projects",
    "section": "Save and lock all raw data",
    "text": "Save and lock all raw data\nKeep raw data in its unedited form. This includes not making changes to filenames. In bioinformatics, it’s common to get data from a sequencing facility with incomprehensible filenames. Don’t fall victim to the temptation of changing these filenames! Instead, it’s much better to keep the filenames exactly how they were sent to you and simply create a spreadsheet that maps the files to their metadata. In the case of a sample mix-up, it’s much easier to make a change to a row in a spreadsheet then to track down all of the filenames that you changed and ensure they’re correctly modified.\nOnce you have your raw data, you don’t want the raw data to change in any way that is not documented by code. To ensure this, you can consider changing file permissions to make the file immutable (unchangable). Using bash, you can change file permissions with:\nchattr +i myfile.txt\nIf you’re using Excel for data analysis, lock the spreadsheet with the raw data and only make references to this sheet when performing calculations.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#backups",
    "href": "data-best-practices.html#backups",
    "title": "‘Best’ practices for data projects",
    "section": "Backups",
    "text": "Backups\n\nThere are two types of people, those who do backups and those who will do backups.\n\nThe following are NOT backup solutions:\n\nCopies of data on the same disk\nDropbox/Google Drive\nRAID arrays\n\nAll of these solutions mirror the data. Corruption or ransomware will propagate. For example, if you corrupt a file on your local computer and then push that change to DropBox then the file on DropBox is now also corrupted. I’m sure some of these cloud providers have version controlled files but it’s better to just avoid the problem entirely by keeping good backups.\n\nUse the 3-2-1 rule:\n\nKeep 3 copies of any important file: 1 primary and 2 backups.\nKeep the files on 2 different media types to protect against different types of hazards.\nStore 1 copy offsite (e.g., outside your home or business facility).\n\nA backup is only a backup if you can restore the files!",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#files",
    "href": "data-best-practices.html#files",
    "title": "1  Organizing projects",
    "section": "2.4 Files",
    "text": "2.4 Files\n\nStore data in standard, machine readable formats\n\nTXT, CSV, TSV, JSON, YAML, HDF5\nLarge text files should be compressed\n\nxz is better than gzip. We should all use xz\n\n\nEnsure filenames are computer friendly\n\nNo spaces\nUse only letters, numbers, and “-” “_” and “.” as delimiters\n\n\n# Bad\ndata for jozef.txt\n\n# Okay\ndata-for-jozef.txt\n\n# Best\n2023-11-09_repetitive-element-counts.txt\nQuiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone"
  },
  {
    "objectID": "data-best-practices.html#quiz",
    "href": "data-best-practices.html#quiz",
    "title": "1  Organizing projects",
    "section": "2.4 Quiz",
    "text": "2.4 Quiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone"
  },
  {
    "objectID": "data-best-practices.html#more-on-filenames",
    "href": "data-best-practices.html#more-on-filenames",
    "title": "‘Best’ practices for data projects",
    "section": "More on filenames",
    "text": "More on filenames\nIt cannot be stressed enough how important filenames can be for an analysis. To get the most out of your files and to avoid catastrpohic failures, you should stick to some basic principles for naming files. First, use consistent and unique identifiers across all files that you generate for an experiment. For example, if you’re conducting a study that has both RNA-seq and ATAC-seq data performed on the same subjects, don’t name the files from the RNA-seq experiment subject1.fq.fz and the files from the ATAC-seq experiment control_subject1.fq.gz if they refer to the same sample. For small projects, it’s fairly easy to create consistent and unique IDs for each subject. For large projects unique random IDs can be used.\nFor example, the following filenames would be bad:\nsubject1-control_at_96hr1.txt\ns1_ctl-at-4days_2.txt\ns2TRT4d1.txt\nsbj2_Treatment_4_Days_Replicate_2.txt\nInstead, look at these filenames.\nsubject1_control_4days_rep1.txt\nsubject1_control_4days_rep2.txt\nsubject2_treatment_4days_rep1.txt\nsubject2_treatment_4days_rep2.txt\nThese are better. Why are they better? They are consistent. The delimiter is consistent between the words (“_“) and each of the words represents something meaningful about the sample. These filenames also do not contain any spaces and can easily be parsed automatically.\nFile naming best practices also apply to naming executable scripts. The name of the file should describe the function of the script. For example,\n01_align_with_STAR.sh\nis better than simply naming the file\n01_script.sh\nPro-tip\nOne easy way to create unique random IDs for a large project is to concatenate descriptions and take the SHA/MDA5 hashsum.\necho \"subject1_control_4days_rep1\" | sha256\n# 57f458a294542b2ed6ac14ca64d3c8e4599eed7a\n\necho \"subject1_control_4days_rep2\" | shasum\n# b6ea9d729e57cce68b37de390d56c542bc17dea6",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#create-analysis-freindly-data---tidy-data",
    "href": "data-best-practices.html#create-analysis-freindly-data---tidy-data",
    "title": "‘Best’ practices for data projects",
    "section": "Create analysis freindly data - tidy data",
    "text": "Create analysis freindly data - tidy data\nThe term tidy data was defined by Hadley Wickham to describe data which is amenable to downstream analysis. Most people are familiar with performing a quick and dirty data analysis in a program like Excel. You may have also used some of Excel’s fancy features for coloring cells, adding bold and underlines to text, and formatting cells with other decorations. All of this tends to just be extra fluff. If you format you data properly then it will be much easier to perform downstream analysis on and will not require the use of extra decorations. This is true even in Excel!\nTo conform to the requirements of being tidy, the data should follow some simple principles:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nDon’t cram two variables into one value. e.g. “male_treated”.\nDon’t embed units into data. e.g. “3.4kg”. Instead, put in column name e.g. “weight_kg” = 3.4\n\n\n\nOnce your data is in this format, it can easily be read into downstream programs like R, or parsed with command line text editing programs like sed.\nThe iris dataset in R provides a classic example of this format\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nPro-tip\ndata.frame like objects can be stored and retrieved efficiently using the Apache Arrow format instead of CSV files:\ndf &lt;- arrow::read_parquet(\"path/to/file.parquet\")\narrow::write_parquet(df, \"path/to/different_file.parquet\")\nsee R arrow for more details. There’s also the nanoparquet package which provides a light weight reader/writer for parquet files.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#untidy-data",
    "href": "data-best-practices.html#untidy-data",
    "title": "‘Best’ practices for data projects",
    "section": "Untidy(?) data",
    "text": "Untidy(?) data\nSome data formats are not amenable to the ‘tidy’ structure, i.e. they’re just not best represented as long tables. For example, large/sparse matrices, geo-spatial data, R objects, etc.the lesson here is to store data in the format that is most appropriate for the data. For example, don’t convert a matrix to a long format and save as a tsv file! Save it as an .rds file instead. Large matrices can also be efficiently stored as HDF5 files. Sparse matrices can be saved and accessed eficiently using the Matrix package in R. And if you are accessing the same data often, consider storing as a SQLite database and accessing with dbplyr or sqlalchemy in Python. The main point is don’t force data into a format that you’re familiar with only because you’re familiar with that format. This will often lead to large file sizes and inefficient performance.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#record-all-steps-used-to-generate-the-data",
    "href": "data-best-practices.html#record-all-steps-used-to-generate-the-data",
    "title": "‘Best’ practices for data projects",
    "section": "Record all steps used to generate the data",
    "text": "Record all steps used to generate the data\nAlways document all steps you used to generate the data that’s present in your projects. This can be as simple as a README with some comments and a wget command or as complex as a snakemake workflow. The point is, be sure you can track down the exact source of every file that you created or downloaded.\nFor example, a README documenting the creation of the files needed to generate a reference index might look like:\nTranscripts:\nwget http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/gencode.v38.transcripts.fa.gz\n\nPrimary Assembly:\nwget http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/GRCh38.primary_assembly.genome.fa.gz\n\nCreate concatenated transcripts + genome for salmon (i.e. gentrome):\ncat gencode.v38.transcripts.fa.gz GRCh38.primary_assembly.genome.fa.gz &gt; gentrome.fa.gz\n\nCreate decoys file for salmon:\ngrep \"&gt;\" &lt;(gunzip -c GRCh38.primary_assembly.genome.fa.gz) | cut -d \" \" -f 1 &gt; decoys.txt\nsed -i.bak -e 's/&gt;//g' decoys.txt\nFor more complicated steps include a script. e.g. creating a new genome index, subsetting BAM files, accessing data from NCBI, etc.\nPro-tip\nA simple way to build a data pipeline that is surprisingly robust is just to create scripts for each step and number them in the order that they should be executed.\n01_download.sh\n02_process.py\n03_makeFigures.R\nYou can also include a runner script that will execute all of the above. Or, for more consistent workflows, use a workflow manager like Nextflow, Snakemake, WDL, or good ole’ GNU Make",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#look-familiar",
    "href": "data-best-practices.html#look-familiar",
    "title": "‘Best’ practices for data projects",
    "section": "Look familiar?",
    "text": "Look familiar?",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#project-structure",
    "href": "data-best-practices.html#project-structure",
    "title": "‘Best’ practices for data projects",
    "section": "Project structure",
    "text": "Project structure\nOne of the most useful changes that you can make to your workflow is the create a consistent folder structure for all of your analyses and stick with it. Coming up with a consistent and generalizable structure can be challenging at first but some general guidelines are presented here and here\nFirst of all, when beginning a new project, you should have some way of naming your projects. One good way of naming projects is to each project a descriptive name and append the date the project was started. For example, brca_rnaseq_2023-11-09/ is better than rnaseq_data/. In six months when a collaborator wants their old BRCA data re-analyzed you’ll thank yourself for timestamping the project folder and giving it a descriptive name.\nMy personal structure for every project looks like:\nyyyymmdd_project-name/\n├── data\n├── doc\n├── README\n├── results\n│   ├── data-files\n│   ├── figures\n│   └── rds-files\n└── scripts\n\n\nPrefixing the project directory with the ISO date allows for easy sorting by date\na README text file is present at the top level of the directory with a short description about the project and any notes or updates\ndata/ should contain soft links to any raw data or the results of downloading data from an external source\ndoc/ contains metadata documents about the samples or other metadata information about the experiment\nresults/ contains only data generated within the project. It has sub-directories for figures/, data-files/ and rds-files/. If you have a longer or more complicated analysis then add sub-directories indicating which script generated the results.\nscripts/ contains all analysis scripts numbered in their order of execution. Synchronize the script names with the results they produce.\n\n\nA more complex example\nyyyymmdd_project-name/\n├── data\n│   ├── 00_fastq\n│   ├── 01_fastqc\n│   ├── 02_trimmed\n│   └── 03_quants\n├── delivered\n│   └── 2022-09-11_analysis-results.zip\n├── doc\n│   └── sample-metadata.txt\n├── project.Rproj\n├── README\n├── results\n│   ├── data-files\n│   │   ├── 04\n│   │   │   └── differential-expression.tsv.gz\n│   │   └── 05\n│   │       └── gsea-results.tsv.gz\n│   ├── figures\n│   │   ├── 04\n│   │   │   ├── ma-plots.png\n│   │   │   └── volcano-plots.png\n│   │   └── 05\n│   │       └── enrichment-plots.png\n│   └── rds-files\n│       └── 04\n│           └── SummarizedExperiment.rds\n└── scripts\n    ├── 01_fastqc.sh\n    ├── 02_trim.sh\n    ├── 03_quant.sh\n    ├── 04_differential_expression.Qmd\n    └── 05_gsea.Qmd\nPro-tip\nIf using Rstudio, include an .Rproj file at the top level of your directory. Doing this enables you to use the here package to reference data within your project in a relative fashion. For example, you can more easily save data with:\nplot_volcano(...)\nggsave(here(\"data\", \"results\", \"figures\", \"04\", \"volcano-plots.png\"))",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#project-structure-1",
    "href": "data-best-practices.html#project-structure-1",
    "title": "1  Some data best practices",
    "section": "3.3 Project structure",
    "text": "3.3 Project structure\nA fuller example might look more like:\nyyyymmdd_project-name/\n├── data\n│   ├── 00_fastq\n│   ├── 01_fastqc\n│   ├── 02_trimmed\n│   └── 03_quants\n├── delivered\n│   └── 2022-09-11_analysis-results.zip\n├── doc\n│   └── sample-metadata.txt\n├── project.Rproj\n├── README\n├── results\n│   ├── data-files\n│   │   ├── 04\n│   │   │   └── differential-expression.tsv.gz\n│   │   └── 05\n│   │       └── gsea-results.tsv.gz\n│   ├── figures\n│   │   ├── 04\n│   │   │   ├── ma-plots.png\n│   │   │   └── volcano-plots.png\n│   │   └── 05\n│   │       └── enrichment-plots.png\n│   └── rds-files\n│       └── 04\n│           └── SummarizedExperiment.rds\n└── scripts\n    ├── 01_fastqc.sh\n    ├── 02_trim.sh\n    ├── 03_quant.sh\n    ├── 04_differential_expression.Qmd\n    └── 05_gsea.Qmd\nPro-tip\nIf using Rstudio, include an .Rproj file at the top level of your directory. Doing this enables you to use the here package to reference data within your project in a relative fashion. For example, you can more easily save data with:\nplot_volcano(...)\nggsave(here(\"data\", \"results\", \"figures\", \"04\", \"volcano-plots.png\"))"
  },
  {
    "objectID": "data-best-practices.html#manual-version-control",
    "href": "data-best-practices.html#manual-version-control",
    "title": "‘Best’ practices for data projects",
    "section": "Manual version control",
    "text": "Manual version control\nVersion control refers to the practice of tracking changes in files and data over their lifetime. You should always track any changes made to your project over the entire life of the project. This can be done either manually or using a dedicated version control system. If doing this manually, add a file called “CHANGELOG.md” in your docs/ directory and add detailed notes in reverse chronological order.\nFor example:\n## 2016-04-08\n\n* Switched to cubic interpolation as default.\n* Moved question about family's TB history to end of questionnaire.\n\n## 2016-04-06\n\n* Added option for cubic interpolation.\n* Removed question about staph exposure (can be inferred from blood test results).\n\nIf you make a significant change to the project, copy the whole directory, date it, and store it such that it will no longer be modified. Copies of these old projects can be compressed and saved with tar + xz compression\ntar -cJvf old.20231109_myproject.tar.xz myproject/`",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#version-control-with-git",
    "href": "data-best-practices.html#version-control-with-git",
    "title": "‘Best’ practices for data projects",
    "section": "Version control with git",
    "text": "Version control with git\n\ngit is probably the de facto version control system in use today for tracking changes across software projects. You should strive to learn and use git to track your projects. Version control systems allow you to track all changes, comment on why changes were made, create parallel branches, and merge existing ones.\ngit is primarily used for source code files. Microsoft Office files and PDFs can be stored with Github but it’s hard to track changes. Rely on Microsoft’s “Track Changes” instead and save frequently.\nIt’s not necessary to version control raw data (back it up!) since it shouldn’t change. Likewise, backup intermediate data and version control the scripts that made it.\nFor a quick primer on Git and GitHub check out the book Happy Git with R or The Official GitHub Training Manual Anyone in the lab can join the coriell-research organization on Github and start tracking their code\nBe careful committing sensitive information to GitHub",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts",
    "title": "‘Best’ practices for data projects",
    "section": "Quick tips to improve your scripts",
    "text": "Quick tips to improve your scripts\n\nPlace a description at the top of every script\nThe description should indicate who the author is. When the code was created. A short description of what the expected inputs and outputs are along with how to use the code. You three months from now will appreciate it when you need to revisit your analysis\nFor example:\n#!/usr/bin/env python3\n# Gennaro Calendo\n# 2023-11-09\n# \n# This scripts performs background correction of all images in the \n#  user supplied directory\n#\n# Usage ./correct-bg.py --input images/ --out_dir out_directory\n#\nfrom image_correction import background_correct\n\nfor img in images:\n  img = background_correct(img)\n  save_image(img, \"out_directory/corrected.png\")\n  \n\n\nDecompose programs into functions\nFunctions make it easier to reason about your code, spot errors, and make changes. This also follows the Don’t Repeat Yourself principle aimed at reducing repetition by replacing it with abstractions that are more stable\nCompare this chunk of code that rescales values using a min-max function (0-1)\ndf &lt;- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\nto this function which does the same thing\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf &lt;- lapply(df, rescale01)\nWhich is easier to read? Which is easier to debug? Which is more efficient?\n\n\nGive functions and variables meaningful names\n\nPrograms are written for people and then computers\nUse variable and function names that are meaningful and correct\nKeep names consistent. Use either snake_case or camelCase but try not to mix both\n\nBad:\nlol &lt;- 1:100\n\nmydata &lt;- data.frame(x = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nf &lt;- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\nBetter:\nages &lt;- 1:100\n\nbioinfo_names &lt;- data.frame(Name = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nplotScatterPlot &lt;- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\n\n\nDo not control program flow with comments\n\nthis is error prone and makes it difficult or impossible to automate\nUse if/else statements instead\n\nBad:\n# Download the file\n#filename &lt;- \"data.tsv\"\n#url &lt;- \"http:://example.com/data.tsv\"\n#download.file(url, filename)\n\n# Read in to a data.frame\ndf &lt;- read.delim(\"data.tsv\", sep=\"\\t\")\nGood:\nfilename &lt;- \"data.tsv\"\nurl &lt;- \"http:://example.com/data.tsv\"\n\nif (!file.exists(filename)) {\n  download.file(url, filename)\n}\ndf &lt;- read.delim(filename)\n\n\nUse a consistent style\nPick a style guide and stick with it. If using R, the styler package can automatically clean up poorly formatted code. If using Python, black is a highly opinionated formatter that is pretty popular. Although, I think ruff is currently all the rage with the Pythonistas these days.\nBad:\nflights|&gt;group_by(dest)|&gt; summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |&gt; \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-&gt; flight_plot\nGood:\nflight_plot &lt;- flights |&gt; \n  group_by(dest) |&gt; \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |&gt; \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()\n\n\nDon’t use right hand assignment\nThis is R specific. I’ve seen this pop up with folks who are strong tidyverse adherents. I get it, that’s the direction of the piping operator. However, this right-hand assignment flies in the face of basically every other programming language, and since code is primarily read rather than executed, it’s much harder to scan a codebase and understand the variable assignment when the assignments can be anywhere in the pipe!\nDon’t do this\ndata |&gt; \n  select(...) |&gt; \n  filter(...) |&gt; \n  group_by(...) |&gt; \n  summarize(...) -&gt; by_group\nIt’s much easier to look down a script and see that by_group is created by all of the piped operations when assigned normally.\nby_group &lt;- data |&gt; \n  select(...) |&gt; \n  filter(...) |&gt; \n  group_by(...) |&gt; \n  summarize(...)",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-1",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-1",
    "title": "1  Some data best practices",
    "section": "5.2 Quick tips to improve your scripts",
    "text": "5.2 Quick tips to improve your scripts\n\n5.2.1 Give functions and variables meaningful names\n\nPrograms are written for people and then computers\nUse variable and function names that are meaningful and correct\nKeep names consistent. Use either snake_case or camelCase but try not to mix both\n\nBad:\nlol <- 1:100\n\nmydata <- data.frame(x = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nf <- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}\nBetter:\nages <- 1:100\n\nbioinfo_names <- data.frame(Name = c(\"Jozef\", \"Gennaro\", \"Matt\", \"Morgan\", \"Anthony\"))\n\nplotScatterPlot <- function(x, y, ...) {\n  plot(x = x, y = y, main = \"Scatter plot of x and y\", ...)\n}"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-2",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-2",
    "title": "1  Some data best practices",
    "section": "5.3 Quick tips to improve your scripts",
    "text": "5.3 Quick tips to improve your scripts\n\n5.3.1 Do not control program flow with comments\n\nthis is error prone and makes it difficult or impossible to automate\nUse if/else statements instead\n\nBad:\n# Download the file\n#filename <- \"data.tsv\"\n#url <- \"http:://example.com/data.tsv\"\n#download.file(url, filename)\n\n# Read in to a data.frame\ndf <- read.delim(\"data.tsv\", sep=\"\\t\")\nGood:\nfilename <- \"data.tsv\"\nurl <- \"http:://example.com/data.tsv\"\n\nif (!file.exists(filename)) {\n  download.file(url, filename)\n}\ndf <- read.delim(filename)"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-3",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-3",
    "title": "1  Some data best practices",
    "section": "5.4 Quick tips to improve your scripts",
    "text": "5.4 Quick tips to improve your scripts\n\n5.4.1 Use a consistent style\n\nPick a style guide and stick with it\nIf using R, the styler package can automatically cleanup poorly formatted code\nIf using Python, black is a highly opinionated formatter\nDon’t use right hand assignment\n\nBad:\nflights|>group_by(dest)|> summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |> \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-> flight_plot\nGood:\nflight_plot <- flights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()"
  },
  {
    "objectID": "data-best-practices.html#quick-tips-to-improve-your-scripts-4",
    "href": "data-best-practices.html#quick-tips-to-improve-your-scripts-4",
    "title": "1  Some data best practices",
    "section": "5.5 Quick tips to improve your scripts",
    "text": "5.5 Quick tips to improve your scripts\n\n5.5.1 Use a consistent style\n\nPick a style guide and stick with it\nIf using R, the styler package can automatically cleanup poorly formatted code\nIf using Python, black is a highly opinionated formatter\nDon’t use right hand assignment\n\nBad:\nflights|>group_by(dest)|> summarize(\ndistance=mean( distance),speed = mean(distance/air_time, na.rm= T)) |> \nggplot(aes(x= distance, y=speed))+geom_smooth(method = \"loess\",span = 0.5,se = FALSE,color = \"white\",linewidth =4)+geom_point()-> flight_plot\nGood:\nflight_plot <- flights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()"
  },
  {
    "objectID": "data-best-practices.html#a-typical-workflow",
    "href": "data-best-practices.html#a-typical-workflow",
    "title": "1  Some data best practices",
    "section": "6.1 A typical workflow",
    "text": "6.1 A typical workflow\n\nA common practice in academic writing is for the lead author to send successive versions of a manuscript to coauthors to collect feedback, which is returned as changes to the document, comments on the document, plain text in email, or a mix of all 3. This allows coauthors to use familiar tools but results in a lot of files to keep track of and a lot of tedious manual labor to merge comments to create the next master version.\n\nInstead of an email based workflow, we should aim to mirror best practices developed for data management and software development\nWe want to:\n\nEnsure text is accessible to yourself and others now and in the future by making a single accessible master document\nMake it easy to track and combine changes from multiple authors\nAvoid duplication and manual entry of references, figures, tables, etc.\nMake it easy to regenerate the final, most updated version and share with collaborators and submit\nMore important than the type of workflow is getting everyone to agree on the workflow chosen and how changes will be tracked"
  },
  {
    "objectID": "data-best-practices.html#workflow-1-single-online-master-document",
    "href": "data-best-practices.html#workflow-1-single-online-master-document",
    "title": "1  Some data best practices",
    "section": "6.2 Workflow 1: single online master document",
    "text": "6.2 Workflow 1: single online master document\n\nKeep a single online master document that all authors can collaborate on\nThe tool should be able to track changes, automatically manage references, track version changes.\nTwo candidates are Google Docs and Microsoft Office online\n\nHere’s a good blog post about using Google Docs for scientific writing\n\nOf course, some collaborators will insist on using familiar desktop based tools which will require saving these files in a versioned doc/ directory and manually merging the changes\n\nPandoc can be useful for these kinds of document merging"
  },
  {
    "objectID": "data-best-practices.html#workflow-2-text-documents-under-version-control",
    "href": "data-best-practices.html#workflow-2-text-documents-under-version-control",
    "title": "1  Some data best practices",
    "section": "6.3 Workflow 2: Text documents under version control",
    "text": "6.3 Workflow 2: Text documents under version control\n\nWrite papers in a text based format like \\(\\LaTeX\\) or markdown managing references in a .bib file and tracking changes via git\nThen convert these plain text documents to a pdf or Word doc for final submission with Pandoc\nMathematics, physics, and astronomy have been doing it this way for decades\nThis of course requires everyone to learn a typsetting language and be able to use version control tools…\n\nWhat is the difference between plain text and word processing?\nText editors?\nWhat does it mean to compile a document?\nBIBTex?\nGit/ Github?\n\n\nthis is all probably a bit too much"
  },
  {
    "objectID": "data-best-practices.html#summary-1",
    "href": "data-best-practices.html#summary-1",
    "title": "1  Some data best practices",
    "section": "7.1 Summary",
    "text": "7.1 Summary\n\nData management\n\nSave all raw data and don’t modify it\nKeep good backups and make sure they work. 3-2-1 rule\nUse consistent, meaningful filenames that make sense to computers and reflect their content or function\nCreate analysis friendly data\nWork with/ save data in the format that it is best suited to\n\nProject Organization\n\nUse a consistent, well-defined project structure across all projects\nGive each project a consistent and meaningful name\nUse the structure of the project to organize where files go\n\nTracking Changes\n\nKeep changes small and save changes frequently\nIf manually tracking changes do so in a logical location in a plain text document\nUse a version control system\n\nSoftware\n\nWrite a short description at the top of every script about what the script does and how to use it\nDecompose programs into functions. Don’t Repeat Yourself\nGive functions and variables meaningful names\nUse statements for control flow instead of comments\nUse a consistent style of coding. Use a code styler\n\nManuscripts\n\nPick a workflow that everyone agrees on and keep a single, active collaborative document using either Microsoft Online or Google Docs"
  },
  {
    "objectID": "data-best-practices.html#resources-1",
    "href": "data-best-practices.html#resources-1",
    "title": "1  Some data best practices",
    "section": "8.1 Resources",
    "text": "8.1 Resources\n\n8.1.1 Best practices\n\nGood Enough Practices in Scientific Computing: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510#sec004\nBackups: https://www.cisa.gov/sites/default/files/publications/data_backup_options.pdf\nFile naming: https://library.si.edu/sites/default/files/filenamingorganizing_20210609.pdf\nTidy data: https://www.jstatsoft.org/article/view/v059i10\nFrank Harrell’s R Workflow for Reproducible Analysis: https://hbiostat.org/rflow/\n\nFH blog is also really good for statistical concepts: https://www.fharrell.com/\n\n\n\n\n8.1.2 Bioinformatics\n\nBioinformatics Data Skills: https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/\nBioStars for bioinfo questions: https://www.biostars.org/\nBioconductor common workflows: https://bioconductor.org/packages/release/BiocViews.html#___Workflow\n\n\n\n8.1.3 Proficiency with computational tools\n\nMIT Missing Semester: https://missing.csail.mit.edu/\n\nReally, check this one out\n\n\n\n\n8.1.4 R\n\nR for Data Science: https://r4ds.hadley.nz/\nAdvanced R: https://adv-r.hadley.nz/\nfasteR (base): https://github.com/matloff/fasteR\nEfficient R Programming: https://bookdown.org/csgillespie/efficientR/\nR performance tips: https://peerj.com/preprints/26605.pdf\nR Inferno: https://www.burns-stat.com/documents/books/the-r-inferno/\nIntroduction to data science: https://rafalab.github.io/dsbook-part-1/\nAdvanced data science: https://rafalab.github.io/dsbook-part-2/"
  },
  {
    "objectID": "r-basics.html#introduction-to-programming-in-r",
    "href": "r-basics.html#introduction-to-programming-in-r",
    "title": "R programming basics",
    "section": "Introduction to programming in R",
    "text": "Introduction to programming in R\nThere are tons of great resources for learning R. R for Data Science is probably the most popular resource for new useRs to get up to speed with slicing and dicing data in R. The R for Data Science book, however, is taught from the perspective of the Tidyverse. The Tidyverse is an opinionated set of packages and functions that help users perform data manipulations primarily on data.frames. While these packages and functions can be great for experienced users by providing ergonomic and consistent interfaces for data.frame manipulation, it is my personal belief that new users should first learn the base language, especially if their goal is to perform bioinformatics analysis.\nBioinformatics tools rely heavily on subsetting and matrix manipulations. In my experience, users who start learning R using only function from the Tidyverse have a difficult time understanding matrix manipulations and subsetting operations common in bioinformatics workflows. This becomes especially important when using SummarizedExperiments - the backbone of many bioinformatics data structures in R.\nFor this reason, we’re going to focus on learning R from the ground up using functions that exist primarily in the base language. A great resource for learning base R quickly is Norm Matloff’s fasteR which can be found here."
  },
  {
    "objectID": "data-best-practices.html#large-files",
    "href": "data-best-practices.html#large-files",
    "title": "‘Best’ practices for data projects",
    "section": "Large files",
    "text": "Large files\nYou’ll probably be dealing with files on the order of 10s of GBs. You do not want to be copying these files from one place to another. This increases confusion and runs the risk of introducing errors. Instead avoid making copies of large local files or persistent databases and simply link to the files.\nYou can use use soft links. A powerful way of finding an linking files can be done with find\n# Link all fastq files to a local directory\nfind /path/to/fq/files -name \"*.fq.gz\") -exec ln -s {} . \\;\nIf using R, you can also sometimes specify a URL in place of a file path for certain functions.\n# Avoid downloading a large GTF file - reads GTF directly into memory\nurl &lt;- \"https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_44/gencode.v44.annotation.gtf.gz\"\ngtf &lt;- rtracklayer::import(url)",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#file-types-and-file-names",
    "href": "data-best-practices.html#file-types-and-file-names",
    "title": "‘Best’ practices for data projects",
    "section": "File types and file names",
    "text": "File types and file names\nAs a data scientist you’ll be dealing with a lot of files but have you ever considered what a file is? Files come in all shapes and formats. Some are very application specific and require specialized programs to open. For example, consider DICOM files that are used to store and manipulate radiology data. Luckily, in bioinformatics we tend to deal mainly with simple, plain text files, most often. Plain text files are typically designed to be both human and machine-readable. If you have the choice of saving any data, you should know that some formats will make your life easier. Certain file formats like TXT, CSV, TSV, JSON, and YAML are standard plain text file formats that are easy to share and easy to open and manipulate. Because of this, you should prefer to store your data in machine-readable formats. Avoid .xlsx files for storing data. Prefer TXT, CSV, TSV, JSON, YAML, and HDF5.\nIf you have very large text files then you can use compression utilities to save space. Most bioinformatics software is designed to work well with gzip compressed data. gzip is a relatively old compression format. You could also consider using xz as a means to compress your data - just know that xz compression is less supported across tools.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#file-naming",
    "href": "data-best-practices.html#file-naming",
    "title": "‘Best’ practices for data projects",
    "section": "File naming",
    "text": "File naming\nFile naming is important but often overlooked. You want your files to be named logically and communicate their contents. You also want your files to be named in a way that a computer can easily read. For example, spaces in filenames are a royal pain when manipulating files on the command line.\nTo ensure filenames are computer friendly, don’t use spaces in filenames. Use only letters, numbers, and “-” “_” and “.” as delimiters. For example:\n# Bad\ndata for jozef.txt\n\n# Okay\ndata-for-jozef.txt\n\n# Best\n2023-11-09_repetitive-element-counts.txt\nQuiz\nI have a directory with the following files:\na.txt\nb.txt\nc.txt\nfile with spaces in the name.txt\nsome other file.txt\nWhat does the following code return? (Expected: For each file in the directory print the filename)\nfor FILE in $(ls); do echo $FILE; done\nBash interprets every space as a new word!\na.txt\nb.txt\nc.txt\nfile\nwith\nspaces\nin\nthe\nname.txt\nsome\nother\nfile.txt\nPro-tip\nA simple alternative here is to use a text document with the basenames the files you want to loop over and then loop over the lines of the file instead.\nSAMPLES=sample-names.txt\n\nfor SAMPLE in $(cat $SAMPLES); do\n  doStuff ${SAMPLE}.txt;\ndone",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#best-practices",
    "href": "data-best-practices.html#best-practices",
    "title": "‘Best’ practices for data projects",
    "section": "Best practices",
    "text": "Best practices\n\nGood Enough Practices in Scientific Computing: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510#sec004\nBackups: https://www.cisa.gov/sites/default/files/publications/data_backup_options.pdf\nFile naming: https://library.si.edu/sites/default/files/filenamingorganizing_20210609.pdf\nTidy data: https://www.jstatsoft.org/article/view/v059i10\nFrank Harrell’s R Workflow for Reproducible Analysis: https://hbiostat.org/rflow/\n\nFH blog is also really good for statistical concepts: https://www.fharrell.com/",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#bioinformatics",
    "href": "data-best-practices.html#bioinformatics",
    "title": "‘Best’ practices for data projects",
    "section": "Bioinformatics",
    "text": "Bioinformatics\n\nBioinformatics Data Skills: https://www.oreilly.com/library/view/bioinformatics-data-skills/9781449367480/\nBioStars for bioinfo questions: https://www.biostars.org/\nBioconductor common workflows: https://bioconductor.org/packages/release/BiocViews.html#___Workflow",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#proficiency-with-computational-tools",
    "href": "data-best-practices.html#proficiency-with-computational-tools",
    "title": "‘Best’ practices for data projects",
    "section": "Proficiency with computational tools",
    "text": "Proficiency with computational tools\n\nMIT Missing Semester: https://missing.csail.mit.edu/\n\nReally, check this one out",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#r",
    "href": "data-best-practices.html#r",
    "title": "‘Best’ practices for data projects",
    "section": "R",
    "text": "R\n\nR for Data Science: https://r4ds.hadley.nz/\nAdvanced R: https://adv-r.hadley.nz/\nfasteR (base): https://github.com/matloff/fasteR\nEfficient R Programming: https://bookdown.org/csgillespie/efficientR/\nR performance tips: https://peerj.com/preprints/26605.pdf\nR Inferno: https://www.burns-stat.com/documents/books/the-r-inferno/\nIntroduction to data science: https://rafalab.github.io/dsbook-part-1/\nAdvanced data science: https://rafalab.github.io/dsbook-part-2/",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "data-best-practices.html#quick-note-todo",
    "href": "data-best-practices.html#quick-note-todo",
    "title": "1  Some data best practices",
    "section": "1.1 Quick note (TODO)",
    "text": "1.1 Quick note (TODO)\nCitations incoming. Please note that much of this document does not contain original material. References are listed at the end of the chapter (I still need to get my .bib files in order to give proper attribution)."
  },
  {
    "objectID": "r-basics.html#subsetting-in-r",
    "href": "r-basics.html#subsetting-in-r",
    "title": "R programming basics",
    "section": "Subsetting in R",
    "text": "Subsetting in R\nYou may have only ever encountered R from the perspective of the tidyverse. tidyverse functions provide useful abstractions for munging tidy data however, most genomics data is often best represented and operated on as matrices. Keeping your data in matrix format can provide many benefits as far as speed and code clarity, which in turn helps to ensure correctness. You can think of matrices as just fancy 2D versions of vectors. So what are vectors?\nVectors are the main building blocks of most R analyses. Whenever you use the c() function (‘concatenate’), like: x &lt;- c('a', 'b', 'c') you’re creating a vector. Vectors hold R objects and are the building block of more complex structures in R.\nNOTE: the following is heavily inspired by Norm Matloff’s excellent fasteR tutorial. Take a look there to get a brief and concise overview base R. You should also check out the first few chapters of Hadley Wickham’s amazing book Advanced R. The first edition contains some more information on base R.\n\nSubsetting vectors\nBelow, we’ll use the built-in R constant called LETTERS. The LETTERS vector is simply a ‘list’ of all uppercase letters in the Roman alphabet.\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\nWe can subset the vector by position. For example, to get the 3rd letter we use the [ operator and the position we want to extract.\n\nLETTERS[3]\n\n[1] \"C\"\n\n\nWe can also use a range of positions. The notation 3:7 is a shortcut that generates the numbers, 3, 4, 5, 6, 7.\n\nLETTERS[3:7]\n\n[1] \"C\" \"D\" \"E\" \"F\" \"G\"\n\n\nWe don’t have to select sequential elements either. We can extract elements by using another vector of positions.\n\nLETTERS[c(7, 5, 14, 14, 1, 18, 15)]\n\n[1] \"G\" \"E\" \"N\" \"N\" \"A\" \"R\" \"O\"\n\n\nVectors become really powerful when we start combining them with logical operations. R supports all of the usual logical and comparison operators you can expect from a programming language, &lt;, &gt;, ==, !=, &lt;=, &gt;=, %in%, & and |.\n\nmy_favorite_letters &lt;- c(\"A\", \"B\", \"C\")\n\n# See that this produces a logical vector of (TRUE/FALSE) values\n# TRUE when LETTERS is one of my_favorite_letters and FALSE otherwise\nLETTERS %in% my_favorite_letters\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE\n\n# We can use that same expression to filter the vector\nLETTERS[LETTERS %in% my_favorite_letters]\n\n[1] \"A\" \"B\" \"C\"\n\n\nThis same kind of subsetting works on vectors that contain numeric data as well. For example, we can filter the measurements of annual flow of water through the Nile river like so:\nNile is another built-in dataset\n\n# Any values strictly greater than 1200\nNile[Nile &gt; 1200]\n\n[1] 1210 1230 1370 1210 1250 1260 1220\n\n# Any even number - `%%` is the modulus operator\nNile[Nile %% 2 == 0]\n\n [1] 1120 1160 1210 1160 1160 1230 1370 1140 1110  994 1020  960 1180  958 1140\n[16] 1100 1210 1150 1250 1260 1220 1030 1100  774  840  874  694  940  916  692\n[31] 1020 1050  726  456  824  702 1120 1100  832  764  768  864  862  698  744\n[46]  796 1040  944  984  822 1010  676  846  812  742 1040  860  874  848  890\n[61]  744  838 1050  918  986 1020  906 1170  912  746  718  714  740\n\n\nAt this point it’s important to take a step back and appreciate what R is doing. Each of the comparison operators that we used above is vectorized. This means that the comparison is applied to all elements of the vector at one time. If you’re used to a programming language like Python this might seem foreign at first. In Python, you would have to write a list comprehension to filter observations from a list that meet a certain condition. For example, [x for x in Nile if x &gt; 1200]. However in R, most functions and operators are vectorized allowing us to do things like Nile &gt; 1200 and have the comparison applied to all of the elements of the vector automatically.\n\n\nSubsetting data.frames\nBut these are just one dimensional vectors. In R, we usually deal with data.frames (tibbles for you tidyverse folks) and matrices. Lucky for us, the subsetting operations we learned for vectors work the same way for data.frames and matrices.\nLet’s take a look at the built-in ToothGrowth dataset. The data consists of the length of odontoblasts in 60 guinea pigs receiving one of three levels of vitamin C by one of two delivery methods.\n\nhead(ToothGrowth)\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5\n\n\nThe dollar sign $ is used to extract an individual column from the data.frame, which is just a vector.\n\nhead(ToothGrowth$len)\n\n[1]  4.2 11.5  7.3  5.8  6.4 10.0\n\n\nWe can also use the [[ to get the same thing. Double-brackets come in handy when your columns are not valid R names since $ only works when columns are valid names.\n\nhead(ToothGrowth[[\"len\"]])\n\n[1]  4.2 11.5  7.3  5.8  6.4 10.0\n\n\nWhen subsetting a data.frame in base R, the general scheme is:\ndf[the rows you want, the columns you want]\nSo in order to get the 5th row of the first column we could do:\n\nToothGrowth[5, 1]\n\n[1] 6.4\n\n\nAgain, we can combine this kind of thinking to extract rows and columns matching logical conditions. For example, if we want to get all of the animals administered orange juice (‘OJ’)\n\nToothGrowth[ToothGrowth$supp == \"OJ\", ]\n\n    len supp dose\n31 15.2   OJ  0.5\n32 21.5   OJ  0.5\n33 17.6   OJ  0.5\n34  9.7   OJ  0.5\n35 14.5   OJ  0.5\n36 10.0   OJ  0.5\n37  8.2   OJ  0.5\n38  9.4   OJ  0.5\n39 16.5   OJ  0.5\n40  9.7   OJ  0.5\n41 19.7   OJ  1.0\n42 23.3   OJ  1.0\n43 23.6   OJ  1.0\n44 26.4   OJ  1.0\n45 20.0   OJ  1.0\n46 25.2   OJ  1.0\n47 25.8   OJ  1.0\n48 21.2   OJ  1.0\n49 14.5   OJ  1.0\n50 27.3   OJ  1.0\n51 25.5   OJ  2.0\n52 26.4   OJ  2.0\n53 22.4   OJ  2.0\n54 24.5   OJ  2.0\n55 24.8   OJ  2.0\n56 30.9   OJ  2.0\n57 26.4   OJ  2.0\n58 27.3   OJ  2.0\n59 29.4   OJ  2.0\n60 23.0   OJ  2.0\n\n\nWe can also combine logical statements. For example, to get all of the rows for animals administered orange juice and with odontoblast length (‘len’) less than 10.\n\nToothGrowth[ToothGrowth$supp == \"OJ\" & ToothGrowth$len &lt; 10, ]\n\n   len supp dose\n34 9.7   OJ  0.5\n37 8.2   OJ  0.5\n38 9.4   OJ  0.5\n40 9.7   OJ  0.5\n\n# We can also use the bracket notation to select rows and columns at the same time\n# Although this gets a little difficult to read\nToothGrowth[ToothGrowth$supp == \"OJ\" & ToothGrowth$len &lt; 10, c(\"len\", \"supp\")]\n\n   len supp\n34 9.7   OJ\n37 8.2   OJ\n38 9.4   OJ\n40 9.7   OJ\n\n\nIt gets annoying typing ToothGrowth every time we want to subset the data.frame. Base R has a very useful function called subset() that can help us type less. subset() essentially ‘looks inside’ the data.frame for the given columns and evaluates the expression without having to explicitly tell R where to find the columns. Think of it like dplyr::filter(), if you are familiar with that function.\n\nsubset(ToothGrowth, supp == \"OJ\" & len &lt; 10)\n\n   len supp dose\n34 9.7   OJ  0.5\n37 8.2   OJ  0.5\n38 9.4   OJ  0.5\n40 9.7   OJ  0.5\n\n\n\n\nSubsetting Lists\nAnother data structure to be aware of, which is used frequently, is the List. We’ve actually already encountered Lists above. data.frames are really just Lists where each vector contains the same data type and all List elements are the same length.\nWe can create a List in R using the list() function. Notice how each list element has a name and can contain a different type of data and number of data elements\n\nl &lt;- list(\n  element1 = c(1, 10, 12, 3, 6, 12, 13, 2, 5, 6, 3, 7),\n  element2 = c(\"a\", \"b\", \"c\"),\n  element3 = c(TRUE, TRUE, FALSE, FALSE, FALSE),\n  element4 = c(0.001, 0.05, 0.86, 1.098, 345.0)\n)\n\nLists can be tricky at first. To extract the data from a particular list element you can use the [[ or the $ (as in the case of data.frames above). Like vectors, you can use either the index or the name of the element you wish to extract.\n\nl[[1]]\n\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\nl[[\"element1\"]]\n\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\nl$element1\n\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\n\nWhat is returned if you only use the single bracket [?\n\nl[1]\n\n$element1\n [1]  1 10 12  3  6 12 13  2  5  6  3  7\n\n\nYou get another List, but now with a single element. This behavior might seem unintuitive at first, but it can be very useful for creating new lists.\n\nnumeric_l &lt;- l[c(1, 4)]\n\n\n\nSubsetting matrices\nMatrices behave much like data.frames but unlike data.frames matrices can only contain one type of data. This might sound like a limitation at first but you’ll soon come to realize that matrices are very powerful (and fast) to work with in R.\n\nset.seed(123)\n\n# Create some random data that looks like methylation values\n(m &lt;- matrix(\n  data = runif(6 * 10),\n  ncol = 6,\n  dimnames = list(\n    paste0(\"CpG.\", 1:10),\n    paste0(\"Sample\", 1:6)\n  )\n))\n\n         Sample1    Sample2   Sample3    Sample4   Sample5    Sample6\nCpG.1  0.2875775 0.95683335 0.8895393 0.96302423 0.1428000 0.04583117\nCpG.2  0.7883051 0.45333416 0.6928034 0.90229905 0.4145463 0.44220007\nCpG.3  0.4089769 0.67757064 0.6405068 0.69070528 0.4137243 0.79892485\nCpG.4  0.8830174 0.57263340 0.9942698 0.79546742 0.3688455 0.12189926\nCpG.5  0.9404673 0.10292468 0.6557058 0.02461368 0.1524447 0.56094798\nCpG.6  0.0455565 0.89982497 0.7085305 0.47779597 0.1388061 0.20653139\nCpG.7  0.5281055 0.24608773 0.5440660 0.75845954 0.2330341 0.12753165\nCpG.8  0.8924190 0.04205953 0.5941420 0.21640794 0.4659625 0.75330786\nCpG.9  0.5514350 0.32792072 0.2891597 0.31818101 0.2659726 0.89504536\nCpG.10 0.4566147 0.95450365 0.1471136 0.23162579 0.8578277 0.37446278\n\n\nIf we want to extract the value for CpG.3 for Sample3\n\nm[3, 3]\n\n[1] 0.6405068\n\n\nOr all values of CpG.3 for every sample\n\nm[3, ]\n\n  Sample1   Sample2   Sample3   Sample4   Sample5   Sample6 \n0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248 \n\n# Or refer to the row by it's name\nm[\"CpG.3\", ]\n\n  Sample1   Sample2   Sample3   Sample4   Sample5   Sample6 \n0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248 \n\n\nOr all CpGs for Sample3\n\nm[, 3]\n\n    CpG.1     CpG.2     CpG.3     CpG.4     CpG.5     CpG.6     CpG.7     CpG.8 \n0.8895393 0.6928034 0.6405068 0.9942698 0.6557058 0.7085305 0.5440660 0.5941420 \n    CpG.9    CpG.10 \n0.2891597 0.1471136 \n\n# Or refer to the column by it's name\nm[, \"Sample3\"]\n\n    CpG.1     CpG.2     CpG.3     CpG.4     CpG.5     CpG.6     CpG.7     CpG.8 \n0.8895393 0.6928034 0.6405068 0.9942698 0.6557058 0.7085305 0.5440660 0.5941420 \n    CpG.9    CpG.10 \n0.2891597 0.1471136 \n\n\nWe can also apply a mask to the entire matrix at once. For example, the following will mark any value that is greater than 0.5 with TRUE\n\nm &gt; 0.5\n\n       Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nCpG.1    FALSE    TRUE    TRUE    TRUE   FALSE   FALSE\nCpG.2     TRUE   FALSE    TRUE    TRUE   FALSE   FALSE\nCpG.3    FALSE    TRUE    TRUE    TRUE   FALSE    TRUE\nCpG.4     TRUE    TRUE    TRUE    TRUE   FALSE   FALSE\nCpG.5     TRUE   FALSE    TRUE   FALSE   FALSE    TRUE\nCpG.6    FALSE    TRUE    TRUE   FALSE   FALSE   FALSE\nCpG.7     TRUE   FALSE    TRUE    TRUE   FALSE   FALSE\nCpG.8     TRUE   FALSE    TRUE   FALSE   FALSE    TRUE\nCpG.9     TRUE   FALSE   FALSE   FALSE   FALSE    TRUE\nCpG.10   FALSE    TRUE   FALSE   FALSE    TRUE   FALSE\n\n\nWe can use this kind of masking to filter rows of the matrix using some very helpful base R functions that operate on matrices. For example, to get only those CpGs where 3 or more samples have a value &gt; 0.5 we can use the rowSums() like so:\n\nm[rowSums(m &gt; 0.5) &gt; 3, ]\n\n        Sample1   Sample2   Sample3   Sample4   Sample5   Sample6\nCpG.3 0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248\nCpG.4 0.8830174 0.5726334 0.9942698 0.7954674 0.3688455 0.1218993\n\n\nThis pattern is very common when dealing with sequencing data. Base R functions like rowSums() and colMeans() are specialized to operate over matrices and are the most efficient way to summarize matrix data. The R package matrixStats also contains highly optimized functions for operating on matrices.\nCompare the above to the tidy solution given the same matrix.\n\ntidyr::as_tibble(m, rownames = \"CpG\") |&gt;\n  tidyr::pivot_longer(!CpG, names_to = \"SampleName\", values_to = \"beta\") |&gt;\n  dplyr::group_by(CpG) |&gt;\n  dplyr::mutate(n = sum(beta &gt; 0.5)) |&gt;\n  dplyr::filter(n &gt; 3) |&gt;\n  tidyr::pivot_wider(id_cols = CpG, names_from = \"SampleName\", values_from = \"beta\") |&gt;\n  tibble::column_to_rownames(var = \"CpG\") |&gt;\n  data.matrix()\n\n        Sample1   Sample2   Sample3   Sample4   Sample5   Sample6\nCpG.3 0.4089769 0.6775706 0.6405068 0.6907053 0.4137243 0.7989248\nCpG.4 0.8830174 0.5726334 0.9942698 0.7954674 0.3688455 0.1218993\n\n\nThere’s probably some kind of tidy solution using across() that I’m missing but this is how most of the tidy code in the wild that I have seen looks",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#what-are-summarizedexperiments",
    "href": "r-basics.html#what-are-summarizedexperiments",
    "title": "R programming basics",
    "section": "What are SummarizedExperiments",
    "text": "What are SummarizedExperiments\nSummarizedExperiments are R objects meant for organizing and manipulating rectangular matrices that are typically produced by arrays or high-throughput sequencing. If you are doing any kind of analysis that requires associating feature-level data (RNA-seq gene counts, methylation array loci, ATAC-seq regions, etc.) with the genomic coordinates of those features and the sample-level metadata with which those features were measured, then you should be using a SummarizedExperiment to organize, manipulate, and store your results.\nPlease take a moment to read through the first 2 sections (at least) of the SummarizedExperiment vignette in order to familiarize yourself with what SummarizedExperiments are and their structure. I will demonstrate how you can use SummarizedExperiments below.\nFrom the SummarizedExperiment vignette:\n\nThe SummarizedExperiment object coordinates four main parts:\n\nassay(), assays(): A matrix-like or list of matrix-like objects of identical dimension\n\nmatrix-like: implements dim(), dimnames(), and 2-dimensional [, [<- methods.\nrows: genes, genomic coordinates, etc.\ncolumns: samples, cells, etc.\n\ncolData(): Annotations on each column, as a DataFrame.\n\nE.g., description of each sample\n\nrowData() and / or rowRanges(): Annotations on each row.\n\nrowRanges(): coordinates of gene / exons in transcripts / etc.\nrowData(): P-values and log-fold change of each gene after differential expression analysis.\n\nmetadata(): List of unstructured metadata describing the overall content of the object.\n\nIn order to better understand how they work, let’s construct a SummarizedExperiment from scratch.\n\nConstructing a SummarizedExperiment\nHopefully you’ll already be working with data that is in a SummarizedExperiment or some other class that derives from one. But just in case you don’t have data structured as a SummarizedExperiment it’s useful and instructive to understand how to create one from scratch.\nTo be most useful, a SummarizedExperiment should have at least:\n\nA matrix of data with features in rows and samples in columns\nA metadata data.frame with samples as rownames and columns describing their properties\n\nAnother really useful object to add to SummarizedExperiments is a GRanges object describing the genomic locations of each feature in the matrix. Adding this to the SummarizedExperiment creates what is called a RangedSummarizedExperiment that acts just like a regular SummarizedExperiment with some extra features.\nTo construct our basic SummarizedExperiment:\n\nWe’ll create a ‘counts’ matrix with gene IDs as rows and Samples in columns\nWe’ll add some metadata describing the Samples\nWe’ll add on GRanges() describing the genomic location of the genes\n\n\nConstruct the counts matrix\n\nsuppressPackageStartupMessages(library(SummarizedExperiment))\n\n\ncounts <- matrix(\n  data = rnbinom(n = 200 * 6, mu = 100, size = 1 / 0.5),\n  nrow = 200,\n  dimnames = list(paste0(\"Gene\", 1:200), paste0(\"Sample\", 1:6))\n)\n\n# Take a peek at what this looks like\ncounts[1:5, 1:5]\n\n      Sample1 Sample2 Sample3 Sample4 Sample5\nGene1     100     210     186      14      95\nGene2      74      17      62      48      27\nGene3     129      72     105     203      73\nGene4      73      80      84      81      59\nGene5      17     242      32      21      58\n\n\n\n\nConstruct the sample metadata\nIt is important that the sample metadata be either a data.frame or a DataFrame object because SummarizedExperiment requires the colData() to have rownames that match the colnames of the count matrix.\n\ncoldata <- data.frame(\n  SampleName = colnames(counts),\n  Treatment = gl(2, 3, labels = c(\"Control\", \"Treatment\")),\n  Age = sample.int(100, 6),\n  row.names = colnames(counts)\n)\n\n# Take a peek at what this looks like\ncoldata\n\n        SampleName Treatment Age\nSample1    Sample1   Control  38\nSample2    Sample2   Control  71\nSample3    Sample3   Control  30\nSample4    Sample4 Treatment  64\nSample5    Sample5 Treatment   8\nSample6    Sample6 Treatment  47\n\n\nNotice that all of the rownames of the metadata are in the same order as the colnames of the counts matrix. This is necessary.\n\n\nConstruct gene range annotations\nYou will usually have gene annotations or GRanges objects loaded from a GTF file or you may even create GRanges yourself by specifying the chromosome, start, end, and strand, information manually.\n\nrowranges <- GRanges(\n  rep(c(\"chr1\", \"chr2\"), c(50, 150)),\n  IRanges(floor(runif(200, 1e5, 1e6)), width = 100),\n  strand = sample(c(\"+\", \"-\"), 200, TRUE),\n  feature_id = sprintf(\"ID%03d\", 1:200),\n  gene_type = sample(c(\"protein_coding\", \"lncRNA\", \"repeat_element\"), 200, replace = TRUE)\n)\nnames(rowranges) <- rownames(counts)\n\n# Take a peek at what this looks like\nrowranges\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             <Rle>     <IRanges>  <Rle> | <character>    <character>\n    Gene1     chr1 954187-954286      - |       ID001 protein_coding\n    Gene2     chr1 552363-552462      + |       ID002 protein_coding\n    Gene3     chr1 303793-303892      - |       ID003         lncRNA\n    Gene4     chr1 875980-876079      + |       ID004 protein_coding\n    Gene5     chr1 427141-427240      - |       ID005 repeat_element\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 268871-268970      + |       ID196 repeat_element\n  Gene197     chr2 599914-600013      + |       ID197 repeat_element\n  Gene198     chr2 477464-477563      + |       ID198 repeat_element\n  Gene199     chr2 461424-461523      - |       ID199 repeat_element\n  Gene200     chr2 619939-620038      + |       ID200 protein_coding\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nConstruct the SummarizedExperiment object\nWith these pieces of information we’re ready to create a SummarizedExperiment object.\n\nse <- SummarizedExperiment(\n  assays = list(counts = counts),\n  colData = coldata,\n  rowRanges = rowranges\n)\n\n# Printing the object gives a summary of what's inside\nse\n\nclass: RangedSummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(2): feature_id gene_type\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(3): SampleName Treatment Age"
  },
  {
    "objectID": "r-basics.html#accessing-parts-of-the-summarizedexperiment-object",
    "href": "r-basics.html#accessing-parts-of-the-summarizedexperiment-object",
    "title": "R programming basics",
    "section": "Accessing parts of the SummarizedExperiment object",
    "text": "Accessing parts of the SummarizedExperiment object\nEvery part of the SummarizedExperiment object can be extracted with its accessor function. To extract a particular assay you can use the assay() function. To extract the column metadata you can use the colData() function. To extract the GRanges for the rows of the matrix you can use the rowRanges() function. The rowData() function also allows you to access row-level annotation information from data added to the rowData slot or by the mcols() of the rowRanges. This will be made more clear below.\n\nGetting the count matrix\n\nassay(se, \"counts\") |> head()\n\n      Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene1     100     210     186      14      95      43\nGene2      74      17      62      48      27      71\nGene3     129      72     105     203      73      63\nGene4      73      80      84      81      59     153\nGene5      17     242      32      21      58      40\nGene6      22      91      38      89     164      43\n\n\nTo see what assays are available you can use the assays() function\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\n\n\n\nGetting the column metadata\n\ncolData(se)\n\nDataFrame with 6 rows and 3 columns\n         SampleName Treatment       Age\n        <character>  <factor> <integer>\nSample1     Sample1 Control          38\nSample2     Sample2 Control          71\nSample3     Sample3 Control          30\nSample4     Sample4 Treatment        64\nSample5     Sample5 Treatment         8\nSample6     Sample6 Treatment        47\n\n\n\n\nGetting the rowRanges\n\nrowRanges(se)\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             <Rle>     <IRanges>  <Rle> | <character>    <character>\n    Gene1     chr1 954187-954286      - |       ID001 protein_coding\n    Gene2     chr1 552363-552462      + |       ID002 protein_coding\n    Gene3     chr1 303793-303892      - |       ID003         lncRNA\n    Gene4     chr1 875980-876079      + |       ID004 protein_coding\n    Gene5     chr1 427141-427240      - |       ID005 repeat_element\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 268871-268970      + |       ID196 repeat_element\n  Gene197     chr2 599914-600013      + |       ID197 repeat_element\n  Gene198     chr2 477464-477563      + |       ID198 repeat_element\n  Gene199     chr2 461424-461523      - |       ID199 repeat_element\n  Gene200     chr2 619939-620038      + |       ID200 protein_coding\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nGetting the rowData\nNote that rowData in this case is the same as mcols() of the rowRanges\n\nrowData(se)\n\nDataFrame with 200 rows and 2 columns\n         feature_id      gene_type\n        <character>    <character>\nGene1         ID001 protein_coding\nGene2         ID002 protein_coding\nGene3         ID003         lncRNA\nGene4         ID004 protein_coding\nGene5         ID005 repeat_element\n...             ...            ...\nGene196       ID196 repeat_element\nGene197       ID197 repeat_element\nGene198       ID198 repeat_element\nGene199       ID199 repeat_element\nGene200       ID200 protein_coding"
  },
  {
    "objectID": "r-basics.html#modifying-a-summarizedexperiment",
    "href": "r-basics.html#modifying-a-summarizedexperiment",
    "title": "R programming basics",
    "section": "Modifying a SummarizedExperiment",
    "text": "Modifying a SummarizedExperiment\nOnce you create a SummarizedExperiment you are not stuck with the information in that object. SummarizedExperiments allow you to add and modify the data within the object.\n\nAdding assays\nFor example, we may wish to calculate counts per million values for our counts matrix and add a new assay back into our SummarizedExperiment object.\n\n# Calculate counts per million\ncounts <- assay(se, \"counts\")\ncpm <- counts / colSums(counts) * 1e6\n\n# Add the CPM data as a new assay to our existing se object\nassay(se, \"cpm\") <- cpm\n\n# And if we wish to log-scale these values\nassay(se, \"logcounts\") <- log2(cpm)\n\n# Now there are three assays available\nassays(se)\n\nList of length 3\nnames(3): counts cpm logcounts\n\n\nNote: Instead of creating intermediate variables we could also directly use the assays like so:\n\nassay(se, \"cpm\") <- assay(se, \"counts\") / colSums(assay(se, \"counts\")) * 1e6\n\n\n\nAdding metadata\nSummarizedExperiment objects use the $ to get and set columns of the metadata contained in the colData slot. For example, to get all of the Ages we can use:\n\nse$Age\n\n[1] 38 71 30 64  8 47\n\n\nIf we want to add a new column we simply create the new column in the same way\n\nse$Batch <- factor(rep(c(\"A\", \"B\", \"C\"), 2))\n\n# Now you can se that a new 'Batch` column has been added to the colData\ncolData(se)\n\nDataFrame with 6 rows and 4 columns\n         SampleName Treatment       Age    Batch\n        <character>  <factor> <integer> <factor>\nSample1     Sample1 Control          38        A\nSample2     Sample2 Control          71        B\nSample3     Sample3 Control          30        C\nSample4     Sample4 Treatment        64        A\nSample5     Sample5 Treatment         8        B\nSample6     Sample6 Treatment        47        C\n\n\n\n\nAdding rowData\nWe can also modify the data which describes each feature in the matrix by adding columns to the rowData. For example, let’s create a new column called Keep if the gene is a protein_coding gene.\n\nrowData(se)$Keep <- rowData(se)$gene_type == \"protein_coding\"\n\nrowData(se)\n\nDataFrame with 200 rows and 3 columns\n         feature_id      gene_type      Keep\n        <character>    <character> <logical>\nGene1         ID001 protein_coding      TRUE\nGene2         ID002 protein_coding      TRUE\nGene3         ID003         lncRNA     FALSE\nGene4         ID004 protein_coding      TRUE\nGene5         ID005 repeat_element     FALSE\n...             ...            ...       ...\nGene196       ID196 repeat_element     FALSE\nGene197       ID197 repeat_element     FALSE\nGene198       ID198 repeat_element     FALSE\nGene199       ID199 repeat_element     FALSE\nGene200       ID200 protein_coding      TRUE"
  },
  {
    "objectID": "r-basics.html#subsetting-summarizedexperiment-objects",
    "href": "r-basics.html#subsetting-summarizedexperiment-objects",
    "title": "R programming basics",
    "section": "Subsetting SummarizedExperiment objects",
    "text": "Subsetting SummarizedExperiment objects\nSummarizedExperiments follow the basic idea of\nse[the rows you want, the columns you want]\nWith a SummarizedExperiment “the rows you want” corresponds to the features in the rows of the matrix/rowData and “the columns you want” corresponds to the metadata in colData\n\nSubsetting based on sample metadata\nFor example, if we want to select all of the data belonging only to samples in the Treatment group we can use the following:\n\n(trt <- se[, se$Treatment == \"Treatment\"])\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample4 Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim of the object changed from 6 to 3. This is because we have selected only the Samples from the original SummarizedExperiment object from the treatment group. The cool thing about SummarizedExperiments is that all of the assays have also been subsetted to reflect this selection!\nTake a look at the “logcounts” assay. It only contains Samples 4, 5, and 6.\n\nassay(trt, \"logcounts\") |> head()\n\n        Sample4  Sample5  Sample6\nGene1  9.681439 12.23369 11.09368\nGene2 11.306469 10.37294 11.82069\nGene3 13.329167 11.85724 11.85136\nGene4 11.957898 11.55359 12.97889\nGene5 10.059737 11.73207 10.98576\nGene6 12.146676 13.07906 11.04431\n\n\nOf course you can combine multiple conditions as well\n\nse[, se$Batch %in% c(\"B\", \"C\") & se$Age > 10]\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample2 Sample3 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\n\n\nSubsetting based on rows\nWe can also select certain features that we want to keep using row subsetting. For example to select only the first 50 rows:\n\nse[1:50, ]\n\nclass: RangedSummarizedExperiment \ndim: 50 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(50): Gene1 Gene2 ... Gene49 Gene50\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim changed from 200 to 50 reflecting the fact that we have only selected the first 50 rows.\nThis subsetting is very useful when combined with logical operators. Above we created a vector in rowData called “Keep” that contained TRUE if the corresponding row of the se object was a coding gene and FALSE otherwise. Let’s use this vector to subset our se object.\n\n(coding <- se[rowData(se)$Keep, ])\n\nclass: RangedSummarizedExperiment \ndim: 68 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(68): Gene1 Gene2 ... Gene193 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nAnd if we look at the resulting rowData we can see that it only contains the protein_coding features\n\nrowData(coding)\n\nDataFrame with 68 rows and 3 columns\n         feature_id      gene_type      Keep\n        <character>    <character> <logical>\nGene1         ID001 protein_coding      TRUE\nGene2         ID002 protein_coding      TRUE\nGene4         ID004 protein_coding      TRUE\nGene8         ID008 protein_coding      TRUE\nGene13        ID013 protein_coding      TRUE\n...             ...            ...       ...\nGene183       ID183 protein_coding      TRUE\nGene185       ID185 protein_coding      TRUE\nGene187       ID187 protein_coding      TRUE\nGene193       ID193 protein_coding      TRUE\nGene200       ID200 protein_coding      TRUE\n\n\nEach assay also reflects this operation\n\nassay(coding, \"cpm\") |> head()\n\n        Sample1    Sample2   Sample3    Sample4  Sample5   Sample6\nGene1  5865.103 10646.3878  9453.141   821.1144 4816.223  2185.404\nGene2  3904.601   834.9295  3158.753  2532.7142 1326.065  3617.281\nGene4  3585.286  4075.8101  4432.250  3978.1936 3005.910  8073.027\nGene8  1582.946 12180.1483  1069.900  3165.8928 2603.016   815.162\nGene13 5571.848  4512.0406  6556.211 12199.4135 9936.629 15704.411\nGene15 5019.011  3405.1637 15366.569  6235.7414 4421.630  3988.270\n\n\n\n\nSubsetting based on rowRanges\nA closely related row-wise subsetting operation can be used if you have a RangedSummarizedExperiment (a SummarizedExperiment with a rowRanges slot) that allows you to perform operations on a SummarizedExperiment object like you would a GRanges object.\nFor example, let’s say we only wanted to extract the features on Chromosome 2 only. Then we can use the GenomicRanges function subsetByOverlaps directly on our SummarizedExperiment object like so:\n\n# Region of interest\nroi <- GRanges(seqnames = \"chr2\", ranges = 1:1e7)\n\n# Subset the SE object for only features on chr2\n(chr2 <- subsetByOverlaps(se, roi))\n\nclass: RangedSummarizedExperiment \ndim: 150 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(150): Gene51 Gene52 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nYou can see again that the dim changed reflecting our selection. Again, all of the associated assays and rowData have also been subsetted reflecting this change as well.\n\nrowData(chr2)\n\nDataFrame with 150 rows and 3 columns\n         feature_id      gene_type      Keep\n        <character>    <character> <logical>\nGene51        ID051 protein_coding      TRUE\nGene52        ID052         lncRNA     FALSE\nGene53        ID053 repeat_element     FALSE\nGene54        ID054 repeat_element     FALSE\nGene55        ID055 repeat_element     FALSE\n...             ...            ...       ...\nGene196       ID196 repeat_element     FALSE\nGene197       ID197 repeat_element     FALSE\nGene198       ID198 repeat_element     FALSE\nGene199       ID199 repeat_element     FALSE\nGene200       ID200 protein_coding      TRUE\n\n\n\nassay(chr2, \"counts\") |> head()\n\n       Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene51      32      45     139     261     153      95\nGene52     113      80      80      61      72     131\nGene53     118      24      60     153       5     103\nGene54      20       1      35       4      89      49\nGene55     249      58      43     307      51      33\nGene56     215      96      28     198      50      81\n\n\n\nrowRanges(chr2)\n\nGRanges object with 150 ranges and 3 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type      Keep\n             <Rle>     <IRanges>  <Rle> | <character>    <character> <logical>\n   Gene51     chr2 839636-839735      + |       ID051 protein_coding      TRUE\n   Gene52     chr2 388487-388586      + |       ID052         lncRNA     FALSE\n   Gene53     chr2 357210-357309      + |       ID053 repeat_element     FALSE\n   Gene54     chr2 230565-230664      + |       ID054 repeat_element     FALSE\n   Gene55     chr2 491399-491498      + |       ID055 repeat_element     FALSE\n      ...      ...           ...    ... .         ...            ...       ...\n  Gene196     chr2 268871-268970      + |       ID196 repeat_element     FALSE\n  Gene197     chr2 599914-600013      + |       ID197 repeat_element     FALSE\n  Gene198     chr2 477464-477563      + |       ID198 repeat_element     FALSE\n  Gene199     chr2 461424-461523      - |       ID199 repeat_element     FALSE\n  Gene200     chr2 619939-620038      + |       ID200 protein_coding      TRUE\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\nThere’s also a few shortcuts on range operations using GRanges/SummarizedExperiments. See the help pages for %over, %within%, and %outside%. For example:\n\nall.equal(se[se %over% roi, ], subsetByOverlaps(se, roi))\n\n[1] TRUE\n\n\n\n\nCombining subsetting operations\nOf course you don’t have to perform one subsetting operation at a time. Like base R you can combine multiple expressions to subset a SummarizedExperiment object.\nFor example, to select only features labeled as repeat_elements and the Sample from ‘Batch’ A in the ‘Control’ group\n\n(selected <- se[\n  rowData(se)$gene_type == \"repeat_element\",\n  se$Treatment == \"Control\" &\n    se$Batch == \"A\"\n])\n\nclass: RangedSummarizedExperiment \ndim: 72 1 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(72): Gene5 Gene6 ... Gene198 Gene199\nrowData names(3): feature_id gene_type Keep\ncolnames(1): Sample1\ncolData names(4): SampleName Treatment Age Batch"
  },
  {
    "objectID": "r-basics.html#saving-a-summarizedexperiment",
    "href": "r-basics.html#saving-a-summarizedexperiment",
    "title": "R programming basics",
    "section": "Saving a SummarizedExperiment",
    "text": "Saving a SummarizedExperiment\nSince SummarizedExperiments keep basically all information about an experiment in one place, it is convenient to save the entire SummarizedExperiment object so that you can pick up an analysis where you left off or even to facilitate better sharing of data between collaborators.\nYou can save the entire SummarizedExperiment object with:\nsaveRDS(se, \"/path/to/se.rds\")\nAnd when you want to read the same object back into R for your next analysis you can do so with:\nse <- readRDS(\"/path/to/se.rds\")"
  },
  {
    "objectID": "r-basics.html#summarizedexperiments-in-the-real-world",
    "href": "r-basics.html#summarizedexperiments-in-the-real-world",
    "title": "R programming basics",
    "section": "SummarizedExperiments in the real world",
    "text": "SummarizedExperiments in the real world\nIf you’re working with any Bioconductor packages it’s likely that the object you’re working with either is a SummarizedExperiment or is inherited from one. For example, the DESeqDataSet from the DESeq2 package and BSseq objects from the bsseq package both inherit from a SummarizedExperiment and thus retain all of the same functionality as above. If you go to the SummarizedExperiment landing page and click “See More” under details you can see all of the packages that depend on SummarizedExperiment.\nAlso, many common methods are also implemented for SummarizedExperiment objects. For example, to simplify calculating counts-per-million above I could have simply used the edgeR::cpm() directly on the SummarizedExperiment object. Many functions in bioconductor packages know how to deal directly with SummarizedExperiments so you don’t ever have to take the trouble extracting components and performing tedious calculations yourself.\n\nassay(se, \"cpm\") <- edgeR::cpm(se)\n\nI also left out any discussion of the metadata() slot of the SummarizedExperiment. The metadata slot is simply a list of any R object that contains information about the experiment. The metadata in the metadata slots are not subjected to the same subsetting rules as the other slots. In practice this assay contains additional information about the experiment as a whole. For example, I typically store bootstrap alignments for each sample here.\nTo add something to the SummarizedExperiment metadata slot you can do:\n\nmetadata(se)$additional_info <- \"Experiment performed on 6 samples with three replicates each\"\n\nAnd to retrieve this:\n\nmetadata(se)$additional_info\n\n[1] \"Experiment performed on 6 samples with three replicates each\""
  },
  {
    "objectID": "r-basics.html#closing-thoughts",
    "href": "r-basics.html#closing-thoughts",
    "title": "R programming basics",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nHopefully this was enough information to get you started using SummarizedExperiments. There’s many things I left out such as different backings for storing out of memory data, a tidyverse interface to SummarizedExperiment objects, TreeSummarizedExperiments for microbiome data, MultiAssayExperiments for dealing with experiments containing multiomics data, and much more.\nPlease let me know your thoughts and if anything needs more clarification."
  },
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "R programming basics",
    "section": "",
    "text": "Introduction to programming in R\nThere are tons of great resources for learning R. R for Data Science is probably the most popular resource for new useRs to get up to speed with slicing and dicing data in R. The R for Data Science book, however, is taught from the perspective of the Tidyverse. The Tidyverse is an opinionated set of packages and functions that help users perform data manipulations primarily on data.frames. While these packages and functions can be great for experienced users by providing ergonomic and consistent interfaces for data.frame manipulation, it is my personal belief that new users should first learn the base language, especially if their goal is to perform bioinformatics analysis.\nBioinformatics tools rely heavily on subsetting and matrix manipulations. In my experience, users who start learning R using only function from the Tidyverse have a difficult time understanding matrix manipulations and subsetting operations common in bioinformatics workflows. This becomes especially important when using SummarizedExperiments - the backbone of many bioinformatics data structures in R.\nFor this reason, we’re going to focus on learning R from the ground up using functions that exist primarily in the base language. A great resource for learning base R quickly is Norm Matloff’s fasteR which can be found here.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "data-best-practices.html#create-analysis-friendly-data---tidy-data",
    "href": "data-best-practices.html#create-analysis-friendly-data---tidy-data",
    "title": "‘Best’ practices for data projects",
    "section": "Create analysis friendly data - tidy data",
    "text": "Create analysis friendly data - tidy data\nThe term tidy data was defined by Hadley Wickham to describe data which is amenable to downstream analysis. Most people are familiar with performing a quick and dirty data analysis in a program like Excel. You may have also used some of Excel’s fancy features for coloring cells, adding bold and underlines to text, and formatting cells with other decorations. All of this tends to just be extra fluff. If you format you data properly then it will be much easier to perform downstream analysis on and will not require the use of extra decorations. This is true even in Excel!\nTo conform to the requirements of being tidy, the data should follow some simple principles:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nDon’t cram two variables into one value. e.g. “male_treated”.\nDon’t embed units into data. e.g. “3.4kg”. Instead, put in column name e.g. “weight_kg” = 3.4\n\n\n\nOnce your data is in this format, it can easily be read into downstream programs like R, or parsed with command line text editing programs like sed.\nThe iris dataset in R provides a classic example of this format\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nPro-tip\ndata.frame like objects can be stored and retrieved efficiently using the Apache Arrow format instead of CSV files:\ndf &lt;- arrow::read_parquet(\"path/to/file.parquet\")\narrow::write_parquet(df, \"path/to/different_file.parquet\")\nsee R arrow for more details. There’s also the nanoparquet package which provides a light weight reader/writer for parquet files.",
    "crumbs": [
      "'Best' practices for data projects"
    ]
  },
  {
    "objectID": "linux.html",
    "href": "linux.html",
    "title": "Command line basics",
    "section": "",
    "text": "Accessing the terminal\nBasic proficiency with the Unix shell is essential for anyone who wants to start doing computational work outside of their laptop and Excel. Unix shells provide an interface for interacting with Unix-like (Mac OS, Linux, etc.) operating systems and a scripting language for controlling the system. The Unix philosopy is a set of software engineering norms and concepts that guide how the tools of the Unix shell interact with one another. Learning a few of these command line tools, and how they can be strung together into what are called “pipes”, is a powerful skill for developing quick and composable bioinformatics programs. Here, we’ll describe some essential commands to get you started using the command line.\nFirst, you’ll have to open the Terminal application. If you’re on Mac OS, the quickest way to access your terminal is: “command + space”, typing “terminal” and pressing Enter. On Windows, you’ll have to install Windows Subsystem for Linux which will allow you to interact with a (default) Ubuntu OS.\nOnce you’ve opened the terminal app, you’re ready to start typing commands at the command line.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#where-am-i",
    "href": "linux.html#where-am-i",
    "title": "Command line basics",
    "section": "Where am I?",
    "text": "Where am I?\nThe first command you should know is pwd. pwd will print your current working directory. This command is used to display where you are currently in the file system. For example, if I open a terminal window in my “Downloads” directory and type and hit Enter:\npwd \nIt will return\n/home/gennaro/Downloads\nindicating that I am in my “Downloads” directory.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#listing-files",
    "href": "linux.html#listing-files",
    "title": "Command line basics",
    "section": "Listing files",
    "text": "Listing files\nNow that I’m in my “Downloads” directory I want to see what files I’ve downloaded. To do this, I can use the ls command to list files in the directory.\nls\nwhich returns:\nBDNF-data.tsv  CORI_Candidate_SNP_draft_250528_clean.docx  differential-expression2.tsv\nYour “Downloads” directory will of course have different files. If I need to display more information about these files, such as the time that they were created or how large they are, I can supply the ls command with arguments.\nFor example\nls -lah\nReturns\ntotal 15M\ndrwxr-xr-x  2 gennaro gennaro 4.0K Jun  1 14:52 .\ndrwxr-x--- 51 gennaro gennaro 4.0K Jun  1 09:34 ..\n-rw-rw-r--  1 gennaro gennaro 6.8K May 30 18:00 BDNF-data.tsv\n-rw-rw-r--  1 gennaro gennaro 973K May 30 17:34 CORI_Candidate_SNP_draft_250528_clean.docx\n-rw-rw-r--  1 gennaro gennaro  14M May 30 12:54 differential-expression2.tsv\nWhich provides information about the file permissions, the file sizes, and when the files were created.\n\nLearning more about a command\nTo learn more about what arguments are available to any of the command line programs you run, you can use the man, or manual, command. This command will open the user manual for the given command.\nTry typing\nman ls\nto view all of the options available when listing files with ls.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#moving-around",
    "href": "linux.html#moving-around",
    "title": "Command line basics",
    "section": "Moving around",
    "text": "Moving around\nLet’s say I want to move from my “Downloads” directory to my “Documents” directory. The command I have to use is cd, short for “change directory”. We can use the cd command with the argument for the target directory we want to go to. For example, to move to my “Documents” directory\ncd /home/gennaro/Documents\n\nDirectory shortcuts\nThe shell has a few shortcuts that make moving around a little easier. Running cd without any arguments will bring you back into your home directory.\ncd\nIn Bash, there is an additional shortcut to specify the “/home” as well. You can use ~ in place of “/home”. For example, to move into my “Documents” folder I can use\ncd ~/Documents\ninstead of typing the full path. To go up one level in the directory you can use ... So to go from my Documents directory ‘up’ into my “/home” directory I can use\ncd ..\nFinally, to go back to the same directory that you were just in you can use\ncd -",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#creating-files",
    "href": "linux.html#creating-files",
    "title": "Command line basics",
    "section": "Creating files",
    "text": "Creating files\nYou can create files with the touch command. For example, to create an empty file in my “Downloads” directory called “A.txt” I can run\ntouch ~/Downloads/A.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#redirection",
    "href": "linux.html#redirection",
    "title": "Command line basics",
    "section": "Redirection",
    "text": "Redirection\nI’ll add some content to this file using the echo command. echo simply prints it’s arguments back out to the terminal. I’ll also use what is called redirection to append the results of the echo command into the text file.\nRedirection is a core concept in Unix pipes. It allows you to take the output from one program and use it as input to another program. In this example, I’ll take the output from echo and redirect it to the file “A.txt” that we just created.\necho \"This is a new line in the file\" &gt;&gt; ~/Downloads/A.txt\necho \"Here is another new line in the file\" &gt;&gt; ~/Downloads/A.txt\nThe &gt;&gt; took the output of the echo command and inserted it as a new line in “A.txt”. Importantly, &gt;&gt; appended these lines into “A.txt”. If I were instead to use &gt; like\necho \"This will replace the current contents of A.txt\" &gt; ~/Downloads/A.txt\n“A.txt” will be overwritten with the new contents. The final essential redirection operator is the pipe |. The pipe lets you take the output from one program and use it as input to another. I’ll show an example of this later.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#displaying-the-content-of-files",
    "href": "linux.html#displaying-the-content-of-files",
    "title": "Command line basics",
    "section": "Displaying the content of files",
    "text": "Displaying the content of files\nThe simplest way to display the contents of a file on the command line is by using the cat command. The cat command is actually designed to concatenate file together, but running it on a single file will print the entire contents of the file to the command line. For example, to print the contents of “A.txt”\ncat ~/Downloads/A.txt\nWill print\nThis will replace the current contents of A.txt\nto the console. If you have a lot of text that you would like to display cat can result in too much information being displayed on the screen. Instead, you can use the less command. less will print the contents of the file as pages on the screen. You can use the d key to scroll down a page, or the u key to scroll up a page.\nAnother way to display only some of the contents of a file is to use the head or tail commands. head -n10 will print the first 10 lines of a file, whereas tail -n10 can be used to print the last 10 lines of a file.",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#copying-files",
    "href": "linux.html#copying-files",
    "title": "Command line basics",
    "section": "Copying files",
    "text": "Copying files\nYou can copy a file using the cp command. For example, to copy the “A.txt” file into a new file “B.txt” I can use\ncp ~/Downloads/A.txt ~/Downloads/B.txt\nTo copy an entire directory you need to supply the -r, or recursive, argument to the cp command. For example, to create a copy of my Downloads directory inside of my Documents directory\ncp -r ~/Downloads ~/Documents/Downloads-copy",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#moving-and-renaming-files",
    "href": "linux.html#moving-and-renaming-files",
    "title": "Command line basics",
    "section": "Moving and renaming files",
    "text": "Moving and renaming files\nThe mv command can be used to move files and rename them. For example, to move the “A.txt” file into my Documents directory I can use\nmv A.txt ~/Documents\nIf I now want to change the name of that file I can also use the mv command. Now you need to specify the new file name instead of the location to move the file to\nmv ~/Documents/A.txt ~/Documents/C.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#making-new-directories",
    "href": "linux.html#making-new-directories",
    "title": "Command line basics",
    "section": "Making new directories",
    "text": "Making new directories\nTo make a new directory you can use the mkdir command. To make a new directory inside of my Downloads directory I can use\nmkdir ~/Downloads/textfiles\nBy default, the mkdir command doesn’t allow you to create nested directories. To enable this, set the mkdir -p flag. For example I can create a parent folder and subfolders using\nmkdir -p ~/Downloads/imagefiles/jpegs\n\nShell expansion\nAnother useful trick is to learn shell expansion. Shell expansion ‘expands’ the arguments. Shell expansion can be a shortcut when creating new project directories. For example\nmkdir -p data doc scripts results/{figures,data-files,rds-files}\nThe results/{figures,data-files,rds-files} expands this command into\nmkdir -p data doc scripts results/figures results/data-files results/rds-files\nWhich saves some typing. Shell expansion can also be used in other contexts. For example, I can create 260 empty text files using the following command\ntouch ~/Downloads/textfiles/{A..Z}{1..10}.txt\nAnother useful shell expansion is *. For example, if I needed to display the contents of each of the file we just created I could run\ncat ~/Downloads/textfiles/*.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#removing-files",
    "href": "linux.html#removing-files",
    "title": "Command line basics",
    "section": "Removing files",
    "text": "Removing files\nRemoving files on the command line can be done with the rm command. Unlike when using a GUI, when you remove files on the command line you cannot get them back so use rm wisely. To remove one of the empty files I just created I can use\nrm ~/Downloads/textfiles/A1.txt\nIf I want to remove the entire “textfiles” directory I can use the -r, or recursive flag with rm.\nrm -r ~/Downloads/textfiles\nBe careful when using rm. A simple space can mean removing entire file systems by mistake!",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#finding-files",
    "href": "linux.html#finding-files",
    "title": "Command line basics",
    "section": "Finding files",
    "text": "Finding files\nOne incredibly useful but often overlooked command line tools is find. find does exactly what you expect it to do, it finds files and folders. find has many arguments but the simplest usage is for finding files using a specific pattern. For example, to find all text (.txt) files in a particular directory and all of its subdirectories you can use:\nfind . -name \"*.txt\" -type f\nThis command says, “find any file (-type f) that has a name like ‘.txt’”. find is especially powerful when combined with the -exec argument. For example, to remove all .txt file in a directory you can use:\nfind . -name \"*.txt\" -type f -exec rm {} \\;",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#searching-the-contents-of-files",
    "href": "linux.html#searching-the-contents-of-files",
    "title": "Command line basics",
    "section": "Searching the contents of files",
    "text": "Searching the contents of files\ngrep is a tool that’s use to search the contents of files for specific text patterns. For example, if you wanted to find every line in a text file that contains the word “the” you could use:\ngrep \"the\" pg100.txt\ngrep also has many useful arguments. One of the most useful is that grep can return the count of the number of lines that are returned. For example, to count the number of lines in a text file that contain the word “the”:\ngrep -c \"the\" pg100.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#piping-commands",
    "href": "linux.html#piping-commands",
    "title": "Command line basics",
    "section": "Piping commands",
    "text": "Piping commands\nThe Unix pipe is what makes the command line so powerful. You can string together small programs to build up solutions to complex problems. The pipe allows you to take the output from one program and use it as input to another program directly.\nFor example, suppose we wanted count the top 10 most frequently used words across all of the works of Shakespeare\ncurl https://www.gutenberg.org/cache/epub/100/pg100.txt | \\\nsed 's/[^a-zA-Z ]/ /g' | \\\ntr 'A-Z ' 'a-z\\n' | \\\ngrep '[a-z]' | \\\nsort | \\\nuniq -c | \\\nsort -nr -k1 | \\\nhead -n10\n\ncurl downloads the text file from Project Gutenberg and streams it to stdout\nsed replaces all characters that are not spaces or letters, with spaces.\ntr changes all of the uppercase letters into lowercase and converts the spaces in the lines of text to newlines (each ‘word’ is now on a separate line)\ngrep includes only lines that contain at least one lowercase alphabetical character (removing any blank lines)\nsort sorts the list of ‘words’ into alphabetical order\nuniq counts the occurrences of each word\nsort sorts the occurrences numerically in descending order\nhead shows the top 10 lines",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#for-loops",
    "href": "linux.html#for-loops",
    "title": "Command line basics",
    "section": "For-loops",
    "text": "For-loops\nThe command line is also a scripting language and like any scripting language, it provides some basic control flow utilities. One of the more useful of these is the basic for loop. In Bash, the for-loop takes the form of a for each loop. the looping variable can be referred to in the loop by using the $ syntax. For example, to loop through\nfor F in *.txt; do sort $F | uniq -c | sort -nr -k1 | head -n1 &gt;&gt; top-words.txt; done",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#gnu-parallel",
    "href": "linux.html#gnu-parallel",
    "title": "Command line basics",
    "section": "GNU parallel",
    "text": "GNU parallel\nGNU parallel is a command line tool that takes away the need to use for-loops entirely. parallel is extremely powerful and feature filled. Importantly, it lets you run commands across multiple jobs. For example, instead of writing a for loop we can process the text files above using 8 jobs at once with parallel\nparallel --jobs 8 \"sort {} | uniq -c | sort -nr -k1 | head -n1 &gt;&gt; top-words.txt\" ::: *.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#downloading-files",
    "href": "linux.html#downloading-files",
    "title": "Command line basics",
    "section": "Downloading files",
    "text": "Downloading files\ncurl and wget are both command line utilities for downloading files from remote resources. curl will download and stream the results to your terminal by default. wget will save the result to a file by default.\ncurl https://www.gutenberg.org/cache/epub/100/pg100.txt\nwget https://www.gutenberg.org/cache/epub/100/pg100.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#compressing-and-uncompressing",
    "href": "linux.html#compressing-and-uncompressing",
    "title": "Command line basics",
    "section": "Compressing and uncompressing",
    "text": "Compressing and uncompressing\nBioinformatics and command line tools can generally work with compressed data. Data compression saves space which can be really beneficial when transferring files over the internet. gzip is an old but commonly used compression utility that is compatible with many command line utilities. To compress a file for example.\ngzip pg100.txt\nWill produce a compressed version of the “pg100.txt” file called “pg100.txt.gz”. Many Unix tools can work directly with gzipped files. For example,\nzcat pg100.txt.gz\nWill unzip and print the contents of the file to the terminal and zgrep can be used to directly search the contents of a gzipped file without the need to decompress the entire file\nzgrep -c \"the\" pg100.txt.gz\nTo decompress a file you use the ‘un’ version of the compression command, gunzip.\ngunzip somefile.txt.gz",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#replacing-file-contents",
    "href": "linux.html#replacing-file-contents",
    "title": "Command line basics",
    "section": "Replacing file contents",
    "text": "Replacing file contents\nsed 's/find/replace/' myfile.txt",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#editors",
    "href": "linux.html#editors",
    "title": "Command line basics",
    "section": "Editors",
    "text": "Editors\nYou’ll eventually need to edit some code or files from the terminal. Two options for code editing from the command line are vim and nano. vim can be more difficult to use for a beginner but is very powerful.\nvim\nOnce you’re in vim you can use the i key to enter “input” mode. “input” mode let’s you type in new characters. Once you’ve typed away, save your work and exit with esc + :wq. vim can be difficult to get used to which lead to the most famous of StackOverflow questions: nano provides a more user friendly interface.\nnano",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "linux.html#accessing-the-terminal",
    "href": "linux.html#accessing-the-terminal",
    "title": "Command line basics",
    "section": "Accessing the terminal",
    "text": "Accessing the terminal\nFirst, you’ll have to open the Terminal application. If you’re on Mac OS, the quickest way to access your terminal is: “command + space”, typing “terminal” and pressing Enter. On Windows, you’ll have to install Windows Subsystem for Linux which will allow you to interact with a (default) Ubuntu OS.\nOnce you’ve opened the terminal app, you’re ready to start typing commands at the command line."
  },
  {
    "objectID": "linux.html#resources",
    "href": "linux.html#resources",
    "title": "Command line basics",
    "section": "Resources",
    "text": "Resources\n\nTerminus is a fun game designed to get you comfortable navigating the command line\nOverTheWire is another game designed to tech you command line tools through the lense of a ‘hacker’\nvimtutor can be used to learn vim",
    "crumbs": [
      "Command line basics"
    ]
  },
  {
    "objectID": "summarized-experiments.html",
    "href": "summarized-experiments.html",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "",
    "text": "What are SummarizedExperiments\nSummarizedExperiments are R objects meant for organizing and manipulating rectangular matrices that are typically produced by arrays or high-throughput sequencing. If you are doing any kind of analysis that requires associating feature-level data (RNA-seq gene counts, methylation array loci, ATAC-seq regions, etc.) with the genomic coordinates of those features and the sample-level metadata with which those features were measured, then you should be using a SummarizedExperiment to organize, manipulate, and store your results.\nPlease take a moment to read through the first 2 sections (at least) of the SummarizedExperiment vignette in order to familiarize yourself with what SummarizedExperiments are and their structure. I will demonstrate how you can use SummarizedExperiments below.\nFrom the SummarizedExperiment vignette:\nThe SummarizedExperiment object coordinates four main parts:\nIn order to better understand how they work, let’s construct a SummarizedExperiment from scratch.",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#what-are-summarizedexperiments",
    "href": "summarized-experiments.html#what-are-summarizedexperiments",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "",
    "text": "assay(), assays(): A matrix-like or list of matrix-like objects of identical dimension\n\nmatrix-like: implements dim(), dimnames(), and 2-dimensional [, [&lt;- methods.\nrows: genes, genomic coordinates, etc.\ncolumns: samples, cells, etc.\n\ncolData(): Annotations on each column, as a DataFrame.\n\nE.g., description of each sample\n\nrowData() and / or rowRanges(): Annotations on each row.\n\nrowRanges(): coordinates of gene / exons in transcripts / etc.\nrowData(): P-values and log-fold change of each gene after differential expression analysis.\n\nmetadata(): List of unstructured metadata describing the overall content of the object.\n\n\n\nConstructing a SummarizedExperiment\nHopefully you’ll already be working with data that is in a SummarizedExperiment or some other class that derives from one. But just in case you don’t have data structured as a SummarizedExperiment it’s useful and instructive to understand how to create one from scratch.\nTo be most useful, a SummarizedExperiment should have at least:\n\nA matrix of data with features in rows and samples in columns\nA metadata data.frame with samples as rownames and columns describing their properties\n\nAnother really useful object to add to SummarizedExperiments is a GRanges object describing the genomic locations of each feature in the matrix. Adding this to the SummarizedExperiment creates what is called a RangedSummarizedExperiment that acts just like a regular SummarizedExperiment with some extra features.\nTo construct our basic SummarizedExperiment:\n\nWe’ll create a ‘counts’ matrix with gene IDs as rows and Samples in columns\nWe’ll add some metadata describing the Samples\nWe’ll add on GRanges() describing the genomic location of the genes\n\n\nConstruct the counts matrix\n\nsuppressPackageStartupMessages(library(SummarizedExperiment))\n\n\ncounts &lt;- matrix(\n  data = rnbinom(n = 200 * 6, mu = 100, size = 1 / 0.5),\n  nrow = 200,\n  dimnames = list(paste0(\"Gene\", 1:200), paste0(\"Sample\", 1:6))\n)\n\n# Take a peek at what this looks like\ncounts[1:5, 1:5]\n\n      Sample1 Sample2 Sample3 Sample4 Sample5\nGene1      55     106     305      51     136\nGene2     109      80      37      62     101\nGene3      58      50      83      78      96\nGene4     139      73     143      62      16\nGene5      60     159      26      50      86\n\n\n\n\nConstruct the sample metadata\nIt is important that the sample metadata be either a data.frame or a DataFrame object because SummarizedExperiment requires the colData() to have rownames that match the colnames of the count matrix.\n\ncoldata &lt;- data.frame(\n  SampleName = colnames(counts),\n  Treatment = gl(2, 3, labels = c(\"Control\", \"Treatment\")),\n  Age = sample.int(100, 6),\n  row.names = colnames(counts)\n)\n\n# Take a peek at what this looks like\ncoldata\n\n        SampleName Treatment Age\nSample1    Sample1   Control  42\nSample2    Sample2   Control  83\nSample3    Sample3   Control   6\nSample4    Sample4 Treatment  70\nSample5    Sample5 Treatment  95\nSample6    Sample6 Treatment  26\n\n\nNotice that all of the rownames of the metadata are in the same order as the colnames of the counts matrix. This is necessary.\n\n\nConstruct gene range annotations\nYou will usually have gene annotations or GRanges objects loaded from a GTF file or you may even create GRanges yourself by specifying the chromosome, start, end, and strand, information manually.\n\nrowranges &lt;- GRanges(\n  rep(c(\"chr1\", \"chr2\"), c(50, 150)),\n  IRanges(floor(runif(200, 1e5, 1e6)), width = 100),\n  strand = sample(c(\"+\", \"-\"), 200, TRUE),\n  feature_id = sprintf(\"ID%03d\", 1:200),\n  gene_type = sample(c(\"protein_coding\", \"lncRNA\", \"repeat_element\"), 200, replace = TRUE)\n)\nnames(rowranges) &lt;- rownames(counts)\n\n# Take a peek at what this looks like\nrowranges\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt;\n    Gene1     chr1 952383-952482      + |       ID001 protein_coding\n    Gene2     chr1 152251-152350      - |       ID002         lncRNA\n    Gene3     chr1 583313-583412      - |       ID003 protein_coding\n    Gene4     chr1 486004-486103      + |       ID004 repeat_element\n    Gene5     chr1 497793-497892      + |       ID005         lncRNA\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 515102-515201      + |       ID196 repeat_element\n  Gene197     chr2 459126-459225      + |       ID197 repeat_element\n  Gene198     chr2 630631-630730      + |       ID198 repeat_element\n  Gene199     chr2 943520-943619      - |       ID199         lncRNA\n  Gene200     chr2 666585-666684      - |       ID200 protein_coding\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nConstruct the SummarizedExperiment object\nWith these pieces of information we’re ready to create a SummarizedExperiment object.\n\nse &lt;- SummarizedExperiment(\n  assays = list(counts = counts),\n  colData = coldata,\n  rowRanges = rowranges\n)\n\n# Printing the object gives a summary of what's inside\nse\n\nclass: RangedSummarizedExperiment \ndim: 200 6 \nmetadata(0):\nassays(1): counts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(2): feature_id gene_type\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(3): SampleName Treatment Age",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#accessing-parts-of-the-summarizedexperiment-object",
    "href": "summarized-experiments.html#accessing-parts-of-the-summarizedexperiment-object",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Accessing parts of the SummarizedExperiment object",
    "text": "Accessing parts of the SummarizedExperiment object\nEvery part of the SummarizedExperiment object can be extracted with its accessor function. To extract a particular assay you can use the assay() function. To extract the column metadata you can use the colData() function. To extract the GRanges for the rows of the matrix you can use the rowRanges() function. The rowData() function also allows you to access row-level annotation information from data added to the rowData slot or by the mcols() of the rowRanges. This will be made more clear below.\n\nGetting the count matrix\n\nassay(se, \"counts\") |&gt; head()\n\n      Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene1      55     106     305      51     136      70\nGene2     109      80      37      62     101      72\nGene3      58      50      83      78      96     170\nGene4     139      73     143      62      16     163\nGene5      60     159      26      50      86       4\nGene6     243     103     199      50      34     123\n\n\nTo see what assays are available you can use the assays() function\n\nassays(se)\n\nList of length 1\nnames(1): counts\n\n\n\n\nGetting the column metadata\n\ncolData(se)\n\nDataFrame with 6 rows and 3 columns\n         SampleName Treatment       Age\n        &lt;character&gt;  &lt;factor&gt; &lt;integer&gt;\nSample1     Sample1 Control          42\nSample2     Sample2 Control          83\nSample3     Sample3 Control           6\nSample4     Sample4 Treatment        70\nSample5     Sample5 Treatment        95\nSample6     Sample6 Treatment        26\n\n\n\n\nGetting the rowRanges\n\nrowRanges(se)\n\nGRanges object with 200 ranges and 2 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt;\n    Gene1     chr1 952383-952482      + |       ID001 protein_coding\n    Gene2     chr1 152251-152350      - |       ID002         lncRNA\n    Gene3     chr1 583313-583412      - |       ID003 protein_coding\n    Gene4     chr1 486004-486103      + |       ID004 repeat_element\n    Gene5     chr1 497793-497892      + |       ID005         lncRNA\n      ...      ...           ...    ... .         ...            ...\n  Gene196     chr2 515102-515201      + |       ID196 repeat_element\n  Gene197     chr2 459126-459225      + |       ID197 repeat_element\n  Gene198     chr2 630631-630730      + |       ID198 repeat_element\n  Gene199     chr2 943520-943619      - |       ID199         lncRNA\n  Gene200     chr2 666585-666684      - |       ID200 protein_coding\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\n\n\nGetting the rowData\nNote that rowData in this case is the same as mcols() of the rowRanges\n\nrowData(se)\n\nDataFrame with 200 rows and 2 columns\n         feature_id      gene_type\n        &lt;character&gt;    &lt;character&gt;\nGene1         ID001 protein_coding\nGene2         ID002         lncRNA\nGene3         ID003 protein_coding\nGene4         ID004 repeat_element\nGene5         ID005         lncRNA\n...             ...            ...\nGene196       ID196 repeat_element\nGene197       ID197 repeat_element\nGene198       ID198 repeat_element\nGene199       ID199         lncRNA\nGene200       ID200 protein_coding",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#modifying-a-summarizedexperiment",
    "href": "summarized-experiments.html#modifying-a-summarizedexperiment",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Modifying a SummarizedExperiment",
    "text": "Modifying a SummarizedExperiment\nOnce you create a SummarizedExperiment you are not stuck with the information in that object. SummarizedExperiments allow you to add and modify the data within the object.\n\nAdding assays\nFor example, we may wish to calculate counts per million values for our counts matrix and add a new assay back into our SummarizedExperiment object.\n\n# Calculate counts per million\ncounts &lt;- assay(se, \"counts\")\ncpm &lt;- counts / colSums(counts) * 1e6\n\n# Add the CPM data as a new assay to our existing se object\nassay(se, \"cpm\") &lt;- cpm\n\n# And if we wish to log-scale these values\nassay(se, \"logcounts\") &lt;- log2(cpm)\n\n# Now there are three assays available\nassays(se)\n\nList of length 3\nnames(3): counts cpm logcounts\n\n\nNote: Instead of creating intermediate variables we could also directly use the assays like so:\n\nassay(se, \"cpm\") &lt;- assay(se, \"counts\") / colSums(assay(se, \"counts\")) * 1e6\n\n\n\nAdding metadata\nSummarizedExperiment objects use the $ to get and set columns of the metadata contained in the colData slot. For example, to get all of the Ages we can use:\n\nse$Age\n\n[1] 42 83  6 70 95 26\n\n\nIf we want to add a new column we simply create the new column in the same way\n\nse$Batch &lt;- factor(rep(c(\"A\", \"B\", \"C\"), 2))\n\n# Now you can se that a new 'Batch` column has been added to the colData\ncolData(se)\n\nDataFrame with 6 rows and 4 columns\n         SampleName Treatment       Age    Batch\n        &lt;character&gt;  &lt;factor&gt; &lt;integer&gt; &lt;factor&gt;\nSample1     Sample1 Control          42        A\nSample2     Sample2 Control          83        B\nSample3     Sample3 Control           6        C\nSample4     Sample4 Treatment        70        A\nSample5     Sample5 Treatment        95        B\nSample6     Sample6 Treatment        26        C\n\n\n\n\nAdding rowData\nWe can also modify the data which describes each feature in the matrix by adding columns to the rowData. For example, let’s create a new column called Keep if the gene is a protein_coding gene.\n\nrowData(se)$Keep &lt;- rowData(se)$gene_type == \"protein_coding\"\n\nrowData(se)\n\nDataFrame with 200 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene1         ID001 protein_coding      TRUE\nGene2         ID002         lncRNA     FALSE\nGene3         ID003 protein_coding      TRUE\nGene4         ID004 repeat_element     FALSE\nGene5         ID005         lncRNA     FALSE\n...             ...            ...       ...\nGene196       ID196 repeat_element     FALSE\nGene197       ID197 repeat_element     FALSE\nGene198       ID198 repeat_element     FALSE\nGene199       ID199         lncRNA     FALSE\nGene200       ID200 protein_coding      TRUE",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#subsetting-summarizedexperiment-objects",
    "href": "summarized-experiments.html#subsetting-summarizedexperiment-objects",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Subsetting SummarizedExperiment objects",
    "text": "Subsetting SummarizedExperiment objects\nSummarizedExperiments follow the basic idea of\nse[the rows you want, the columns you want]\nWith a SummarizedExperiment “the rows you want” corresponds to the features in the rows of the matrix/rowData and “the columns you want” corresponds to the metadata in colData\n\nSubsetting based on sample metadata\nFor example, if we want to select all of the data belonging only to samples in the Treatment group we can use the following:\n\n(trt &lt;- se[, se$Treatment == \"Treatment\"])\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample4 Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim of the object changed from 6 to 3. This is because we have selected only the Samples from the original SummarizedExperiment object from the treatment group. The cool thing about SummarizedExperiments is that all of the assays have also been subsetted to reflect this selection!\nTake a look at the “logcounts” assay. It only contains Samples 4, 5, and 6.\n\nassay(trt, \"logcounts\") |&gt; head()\n\n       Sample4  Sample5   Sample6\nGene1 11.27098 12.77363 11.793480\nGene2 11.64803 12.37114 11.834195\nGene3 11.97157 12.24916 13.007944\nGene4 11.66713  9.66427 13.042564\nGene5 11.30805 12.02482  7.686164\nGene6 11.30813 10.78130 12.655444\n\n\nOf course you can combine multiple conditions as well\n\nse[, se$Batch %in% c(\"B\", \"C\") & se$Age &gt; 10]\n\nclass: RangedSummarizedExperiment \ndim: 200 3 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(200): Gene1 Gene2 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(3): Sample2 Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\n\n\nSubsetting based on rows\nWe can also select certain features that we want to keep using row subsetting. For example to select only the first 50 rows:\n\nse[1:50, ]\n\nclass: RangedSummarizedExperiment \ndim: 50 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(50): Gene1 Gene2 ... Gene49 Gene50\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nNotice how the dim changed from 200 to 50 reflecting the fact that we have only selected the first 50 rows.\nThis subsetting is very useful when combined with logical operators. Above we created a vector in rowData called “Keep” that contained TRUE if the corresponding row of the se object was a coding gene and FALSE otherwise. Let’s use this vector to subset our se object.\n\n(coding &lt;- se[rowData(se)$Keep, ])\n\nclass: RangedSummarizedExperiment \ndim: 54 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(54): Gene1 Gene3 ... Gene192 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nAnd if we look at the resulting rowData we can see that it only contains the protein_coding features\n\nrowData(coding)\n\nDataFrame with 54 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene1         ID001 protein_coding      TRUE\nGene3         ID003 protein_coding      TRUE\nGene9         ID009 protein_coding      TRUE\nGene10        ID010 protein_coding      TRUE\nGene12        ID012 protein_coding      TRUE\n...             ...            ...       ...\nGene182       ID182 protein_coding      TRUE\nGene187       ID187 protein_coding      TRUE\nGene191       ID191 protein_coding      TRUE\nGene192       ID192 protein_coding      TRUE\nGene200       ID200 protein_coding      TRUE\n\n\nEach assay also reflects this operation\n\nassay(coding, \"cpm\") |&gt; head()\n\n        Sample1    Sample2   Sample3   Sample4    Sample5   Sample6\nGene1  2664.987  5457.7283 15466.531  2471.170  7002.3684  3549.696\nGene3  2986.304  2535.4970  4021.708  4016.064  4868.1542  8237.232\nGene9  3243.744 11561.8661  8527.958  2728.864 11916.8357  4845.431\nGene10 2202.990   507.1251  3261.038 11959.087   862.1127 14286.454\nGene12 7759.014  4865.6763  4248.623  4716.264  5486.8264  7710.464\nGene17 5273.834  4651.6135  6384.512  3194.726  4167.0705  2368.448\n\n\n\n\nSubsetting based on rowRanges\nA closely related row-wise subsetting operation can be used if you have a RangedSummarizedExperiment (a SummarizedExperiment with a rowRanges slot) that allows you to perform operations on a SummarizedExperiment object like you would a GRanges object.\nFor example, let’s say we only wanted to extract the features on Chromosome 2 only. Then we can use the GenomicRanges function subsetByOverlaps directly on our SummarizedExperiment object like so:\n\n# Region of interest\nroi &lt;- GRanges(seqnames = \"chr2\", ranges = 1:1e7)\n\n# Subset the SE object for only features on chr2\n(chr2 &lt;- subsetByOverlaps(se, roi))\n\nclass: RangedSummarizedExperiment \ndim: 150 6 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(150): Gene51 Gene52 ... Gene199 Gene200\nrowData names(3): feature_id gene_type Keep\ncolnames(6): Sample1 Sample2 ... Sample5 Sample6\ncolData names(4): SampleName Treatment Age Batch\n\n\nYou can see again that the dim changed reflecting our selection. Again, all of the associated assays and rowData have also been subsetted reflecting this change as well.\n\nrowData(chr2)\n\nDataFrame with 150 rows and 3 columns\n         feature_id      gene_type      Keep\n        &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\nGene51        ID051 repeat_element     FALSE\nGene52        ID052         lncRNA     FALSE\nGene53        ID053 protein_coding      TRUE\nGene54        ID054 repeat_element     FALSE\nGene55        ID055         lncRNA     FALSE\n...             ...            ...       ...\nGene196       ID196 repeat_element     FALSE\nGene197       ID197 repeat_element     FALSE\nGene198       ID198 repeat_element     FALSE\nGene199       ID199         lncRNA     FALSE\nGene200       ID200 protein_coding      TRUE\n\n\n\nassay(chr2, \"counts\") |&gt; head()\n\n       Sample1 Sample2 Sample3 Sample4 Sample5 Sample6\nGene51     176      95     273     120      87     185\nGene52     156      26      94     151      33      34\nGene53     228     131      13      42      95       5\nGene54      80     209     101      33     211     116\nGene55      69     250     116     134      81     149\nGene56      19      87      12      62      55     170\n\n\n\nrowRanges(chr2)\n\nGRanges object with 150 ranges and 3 metadata columns:\n          seqnames        ranges strand |  feature_id      gene_type      Keep\n             &lt;Rle&gt;     &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt;    &lt;character&gt; &lt;logical&gt;\n   Gene51     chr2 416048-416147      + |       ID051 repeat_element     FALSE\n   Gene52     chr2 435782-435881      - |       ID052         lncRNA     FALSE\n   Gene53     chr2 230798-230897      - |       ID053 protein_coding      TRUE\n   Gene54     chr2 340617-340716      - |       ID054 repeat_element     FALSE\n   Gene55     chr2 762324-762423      - |       ID055         lncRNA     FALSE\n      ...      ...           ...    ... .         ...            ...       ...\n  Gene196     chr2 515102-515201      + |       ID196 repeat_element     FALSE\n  Gene197     chr2 459126-459225      + |       ID197 repeat_element     FALSE\n  Gene198     chr2 630631-630730      + |       ID198 repeat_element     FALSE\n  Gene199     chr2 943520-943619      - |       ID199         lncRNA     FALSE\n  Gene200     chr2 666585-666684      - |       ID200 protein_coding      TRUE\n  -------\n  seqinfo: 2 sequences from an unspecified genome; no seqlengths\n\n\nThere’s also a few shortcuts on range operations using GRanges/SummarizedExperiments. See the help pages for %over, %within%, and %outside%. For example:\n\nall.equal(se[se %over% roi, ], subsetByOverlaps(se, roi))\n\n[1] TRUE\n\n\n\n\nCombining subsetting operations\nOf course you don’t have to perform one subsetting operation at a time. Like base R you can combine multiple expressions to subset a SummarizedExperiment object.\nFor example, to select only features labeled as repeat_elements and the Sample from ‘Batch’ A in the ‘Control’ group\n\n(selected &lt;- se[\n  rowData(se)$gene_type == \"repeat_element\",\n  se$Treatment == \"Control\" &\n    se$Batch == \"A\"\n])\n\nclass: RangedSummarizedExperiment \ndim: 71 1 \nmetadata(0):\nassays(3): counts cpm logcounts\nrownames(71): Gene4 Gene6 ... Gene197 Gene198\nrowData names(3): feature_id gene_type Keep\ncolnames(1): Sample1\ncolData names(4): SampleName Treatment Age Batch",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#saving-a-summarizedexperiment",
    "href": "summarized-experiments.html#saving-a-summarizedexperiment",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Saving a SummarizedExperiment",
    "text": "Saving a SummarizedExperiment\nSince SummarizedExperiments keep basically all information about an experiment in one place, it is convenient to save the entire SummarizedExperiment object so that you can pick up an analysis where you left off or even to facilitate better sharing of data between collaborators.\nYou can save the entire SummarizedExperiment object with:\nsaveRDS(se, \"/path/to/se.rds\")\nAnd when you want to read the same object back into R for your next analysis you can do so with:\nse &lt;- readRDS(\"/path/to/se.rds\")",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#summarizedexperiments-in-the-real-world",
    "href": "summarized-experiments.html#summarizedexperiments-in-the-real-world",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "SummarizedExperiments in the real world",
    "text": "SummarizedExperiments in the real world\nIf you’re working with any Bioconductor packages it’s likely that the object you’re working with either is a SummarizedExperiment or is inherited from one. For example, the DESeqDataSet from the DESeq2 package and BSseq objects from the bsseq package both inherit from a SummarizedExperiment and thus retain all of the same functionality as above. If you go to the SummarizedExperiment landing page and click “See More” under details you can see all of the packages that depend on SummarizedExperiment.\nAlso, many common methods are also implemented for SummarizedExperiment objects. For example, to simplify calculating counts-per-million above I could have simply used the edgeR::cpm() directly on the SummarizedExperiment object. Many functions in bioconductor packages know how to deal directly with SummarizedExperiments so you don’t ever have to take the trouble extracting components and performing tedious calculations yourself.\n\nassay(se, \"cpm\") &lt;- edgeR::cpm(se)\n\nI also left out any discussion of the metadata() slot of the SummarizedExperiment. The metadata slot is simply a list of any R object that contains information about the experiment. The metadata in the metadata slots are not subjected to the same subsetting rules as the other slots. In practice this assay contains additional information about the experiment as a whole. For example, I typically store bootstrap alignments for each sample here.\nTo add something to the SummarizedExperiment metadata slot you can do:\n\nmetadata(se)$additional_info &lt;- \"Experiment performed on 6 samples with three replicates each\"\n\nAnd to retrieve this:\n\nmetadata(se)$additional_info\n\n[1] \"Experiment performed on 6 samples with three replicates each\"",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "summarized-experiments.html#closing-thoughts",
    "href": "summarized-experiments.html#closing-thoughts",
    "title": "Practical introduction to SummarizedExperiments",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nHopefully this was enough information to get you started using SummarizedExperiments. There’s many things I left out such as different backings for storing out of memory data, a tidyverse interface to SummarizedExperiment objects, TreeSummarizedExperiments for microbiome data, MultiAssayExperiments for dealing with experiments containing multiomics data, and much more.\nPlease let me know your thoughts and if anything needs more clarification.",
    "crumbs": [
      "Practical introduction to SummarizedExperiments"
    ]
  },
  {
    "objectID": "r-basics.html#data-transformation",
    "href": "r-basics.html#data-transformation",
    "title": "R programming basics",
    "section": "Data transformation",
    "text": "Data transformation\nNow that we’ve got a handle on some different R data types, and how to slice and dice them, we can start learning basic data cleaning. We’ll focus on using data.frames since they’re the primary workhorse of data analysis in R. But remember, data.frames are just lists of vectors with some special rules, so many concepts you learn which apply to data.frames, also apply to vectors and lists.\n\nData import\nWe’ll use the built in penguins_raw dataset to learn some basic data cleaning. This dataset is built into R version 4.5 so you can just load it by running penguins_raw. However, to illustrate data import, we’ll read in the data from an external source.\nThe read.csv() function can read in comma-separated value files that are located either on your local machine or from remote sources if provided a URL.\n\nurl &lt;- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins_raw.csv\"\npenguins &lt;- read.csv(url)\n\n\n\nTaking a look at the data\nThe lines above read the data into data.frame that we called penguins. We can take a look at the first few rows of the penguins data.frame using the head() function.\n\nhead(penguins)\n\n  studyName Sample.Number                             Species Region    Island\n1   PAL0708             1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n2   PAL0708             2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n3   PAL0708             3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n4   PAL0708             4 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n5   PAL0708             5 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n6   PAL0708             6 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n               Stage Individual.ID Clutch.Completion   Date.Egg\n1 Adult, 1 Egg Stage          N1A1               Yes 2007-11-11\n2 Adult, 1 Egg Stage          N1A2               Yes 2007-11-11\n3 Adult, 1 Egg Stage          N2A1               Yes 2007-11-16\n4 Adult, 1 Egg Stage          N2A2               Yes 2007-11-16\n5 Adult, 1 Egg Stage          N3A1               Yes 2007-11-16\n6 Adult, 1 Egg Stage          N3A2               Yes 2007-11-16\n  Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.    Sex\n1               39.1              18.7                 181          3750   MALE\n2               39.5              17.4                 186          3800 FEMALE\n3               40.3              18.0                 195          3250 FEMALE\n4                 NA                NA                  NA            NA   &lt;NA&gt;\n5               36.7              19.3                 193          3450 FEMALE\n6               39.3              20.6                 190          3650   MALE\n  Delta.15.N..o.oo. Delta.13.C..o.oo.                       Comments\n1                NA                NA Not enough blood for isotopes.\n2           8.94956         -24.69454                           &lt;NA&gt;\n3           8.36821         -25.33302                           &lt;NA&gt;\n4                NA                NA             Adult not sampled.\n5           8.76651         -25.32426                           &lt;NA&gt;\n6           8.66496         -25.29805                           &lt;NA&gt;\n\n\nIf we want to get a general overview of the data, we can use the str() function.\n\nstr(penguins)\n\n'data.frame':   344 obs. of  17 variables:\n $ studyName          : chr  \"PAL0708\" \"PAL0708\" \"PAL0708\" \"PAL0708\" ...\n $ Sample.Number      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Species            : chr  \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" ...\n $ Region             : chr  \"Anvers\" \"Anvers\" \"Anvers\" \"Anvers\" ...\n $ Island             : chr  \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ Stage              : chr  \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" ...\n $ Individual.ID      : chr  \"N1A1\" \"N1A2\" \"N2A1\" \"N2A2\" ...\n $ Clutch.Completion  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Date.Egg           : chr  \"2007-11-11\" \"2007-11-11\" \"2007-11-16\" \"2007-11-16\" ...\n $ Culmen.Length..mm. : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ Culmen.Depth..mm.  : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ Flipper.Length..mm.: int  181 186 195 NA 193 190 181 195 193 190 ...\n $ Body.Mass..g.      : int  3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ Sex                : chr  \"MALE\" \"FEMALE\" \"FEMALE\" NA ...\n $ Delta.15.N..o.oo.  : num  NA 8.95 8.37 NA 8.77 ...\n $ Delta.13.C..o.oo.  : num  NA -24.7 -25.3 NA -25.3 ...\n $ Comments           : chr  \"Not enough blood for isotopes.\" NA NA \"Adult not sampled.\" ...\n\n\nThere are a few external packages that are also very useful for getting summaries of data.frames. Hmsic::describe() and skimr::skim() are two standouts.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#exploratory-data-analysis",
    "href": "r-basics.html#exploratory-data-analysis",
    "title": "R programming basics",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nNow that we’ve got a handle on some different R data types, and how to slice and dice them, we can start learning basic data cleaning and exploratory data analysis. We’ll focus on using data.frames since they’re the primary workhorse of data analysis in R. But remember, data.frames are just lists of vectors with some special rules, so many concepts you learn will apply to data.frames but also apply to vectors and lists.\n\nData import\nWe’ll use the built in penguins_raw dataset to learn some basic data cleaning. This dataset is built into R version 4.5 so you can just load it by running penguins_raw if you have that R version installed. However, to illustrate data import, we’ll read in the data from an external source.\nThe read.csv() function can read in comma-separated value files that are located either on your local machine or from remote sources if provided a URL.\n\nurl &lt;- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins_raw.csv\"\npenguins &lt;- read.csv(url)\n\nThe read.csv() function has many options for reading in data. If you want to learn about all of the options any particular R function has, you can prefix the function name with a ? like, ?read.csv() to bring up the help documentation.\n\n\nViewing data\nThe code above read the data into data.frame that we called penguins. We can take a look at the first few rows of the penguins data.frame using the head() function.\n\nhead(penguins)\n\n  studyName Sample.Number                             Species Region    Island\n1   PAL0708             1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n2   PAL0708             2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n3   PAL0708             3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n4   PAL0708             4 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n5   PAL0708             5 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n6   PAL0708             6 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n               Stage Individual.ID Clutch.Completion   Date.Egg\n1 Adult, 1 Egg Stage          N1A1               Yes 2007-11-11\n2 Adult, 1 Egg Stage          N1A2               Yes 2007-11-11\n3 Adult, 1 Egg Stage          N2A1               Yes 2007-11-16\n4 Adult, 1 Egg Stage          N2A2               Yes 2007-11-16\n5 Adult, 1 Egg Stage          N3A1               Yes 2007-11-16\n6 Adult, 1 Egg Stage          N3A2               Yes 2007-11-16\n  Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.    Sex\n1               39.1              18.7                 181          3750   MALE\n2               39.5              17.4                 186          3800 FEMALE\n3               40.3              18.0                 195          3250 FEMALE\n4                 NA                NA                  NA            NA   &lt;NA&gt;\n5               36.7              19.3                 193          3450 FEMALE\n6               39.3              20.6                 190          3650   MALE\n  Delta.15.N..o.oo. Delta.13.C..o.oo.                       Comments\n1                NA                NA Not enough blood for isotopes.\n2           8.94956         -24.69454                           &lt;NA&gt;\n3           8.36821         -25.33302                           &lt;NA&gt;\n4                NA                NA             Adult not sampled.\n5           8.76651         -25.32426                           &lt;NA&gt;\n6           8.66496         -25.29805                           &lt;NA&gt;\n\n\nIf we want to get a general overview of the data, we can use the str() function.\n\nstr(penguins)\n\n'data.frame':   344 obs. of  17 variables:\n $ studyName          : chr  \"PAL0708\" \"PAL0708\" \"PAL0708\" \"PAL0708\" ...\n $ Sample.Number      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Species            : chr  \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" ...\n $ Region             : chr  \"Anvers\" \"Anvers\" \"Anvers\" \"Anvers\" ...\n $ Island             : chr  \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ Stage              : chr  \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" ...\n $ Individual.ID      : chr  \"N1A1\" \"N1A2\" \"N2A1\" \"N2A2\" ...\n $ Clutch.Completion  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Date.Egg           : chr  \"2007-11-11\" \"2007-11-11\" \"2007-11-16\" \"2007-11-16\" ...\n $ Culmen.Length..mm. : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ Culmen.Depth..mm.  : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ Flipper.Length..mm.: int  181 186 195 NA 193 190 181 195 193 190 ...\n $ Body.Mass..g.      : int  3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ Sex                : chr  \"MALE\" \"FEMALE\" \"FEMALE\" NA ...\n $ Delta.15.N..o.oo.  : num  NA 8.95 8.37 NA 8.77 ...\n $ Delta.13.C..o.oo.  : num  NA -24.7 -25.3 NA -25.3 ...\n $ Comments           : chr  \"Not enough blood for isotopes.\" NA NA \"Adult not sampled.\" ...\n\n\nThere are a few external packages that are also very useful for getting summaries of data.frames. Hmsic::describe() and skimr::skim() are two standouts.\nOne of the most basic ways to get an idea of the data is to summarize each variable. There are a few functions we can use to get summaries of the data. The table() function will count the number of occurrences of each type in a vector.\nFor example, how many observations of each species of penguin are in the dataset?\n\ntable(penguins$Species)\n\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                      152 \nChinstrap penguin (Pygoscelis antarctica) \n                                       68 \n        Gentoo penguin (Pygoscelis papua) \n                                      124 \n\n\nR also provides the typical summary functions that you would expect from a statistical programming language such as mean(), median(), min(), and max(), and length().\nFor example, what is the mean flipper length?\n\nmean(penguins$Flipper.Length..mm.)\n\n[1] NA\n\n\nOn no! This returned NA but there is clearly data in this column. What happened? Missing data is commonly observed across all data domains. NAs simply represent unknown values in this context and it’s impossible to know how to take the mean of a value that’s known with a value that’s unknown. It is for this reason that many R functions have an argument called na.rm=. Setting na.rm=TRUE in these functions tells R to ignore the NA values.\n\nmean(penguins$Flipper.Length..mm., na.rm = TRUE)\n\n[1] 200.9152\n\n\nNow we can see that the mean flipper length is ~200 mm across all observations in the dataset. Another useful function is summary(). Running summary() on a numeric vector returns a lot of useful information.\n\nsummary(penguins$Flipper.Length..mm., na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nFinally, if you are using and IDE like Rstudio or Positron you can run the View() function on your data.frame. This will bring up an interactive data viewer.\n\n\nSplitting data\nYou may have noticed above that we computed the mean flipper length across all species of penguins. But do all species have the same mean? A common pattern in R is called “split-apply-combine”. This pattern means, split the data into groups you’re interested in, apply a function to each of those groups, and combine the results. One such function that performs this operation is called tapply(). tapply() will split the data by a given variable into groups and apply a function to each group.\nFor example, to find the mean flipper length for each species we could use\n\ntapply(penguins$Flipper.Length..mm., penguins$Species, mean, na.rm = TRUE)\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                 189.9536 \nChinstrap penguin (Pygoscelis antarctica) \n                                 195.8235 \n        Gentoo penguin (Pygoscelis papua) \n                                 217.1870 \n\n\nThe basic format of the tapply() function is\n\ntapply(\"data to split\", \"what to split by\", \"what to compute\")\n\nSplitting can also by applied directly to data.frames using the split() function. Let’s say we wanted to split the penguins data.frame into one data.frame for each species. We can use the split() function for this purpose.\n\nby_species &lt;- split(penguins, f = penguins$Species)\n\nThis function returns a list of data.frames, one for each species in the original data.frame. Use names(by_species) to see what each list element is named. To extract the first data.frame from this list, which contains Adelie penguin data only, we can subset the list of data.frames.\n\nadelie &lt;- by_species[[1]]\n\nPerforming operations on lists is such a common task in R that a function exists specifically to apply functions to list elements. This function is called lapply(). lapply() takes a list and a function and applies that function to each list element. The lapply() function returns the results as a list.\nFor example, to see how many rows are in each of the data.frames in the “by_species” list:\n\nlapply(by_species, nrow)\n\n$`Adelie Penguin (Pygoscelis adeliae)`\n[1] 152\n\n$`Chinstrap penguin (Pygoscelis antarctica)`\n[1] 68\n\n$`Gentoo penguin (Pygoscelis papua)`\n[1] 124\n\n\n\n\nPlotting data\nData cleaning and data visualization go hand-in-hand. To effectively clean data, you should be examining the changes you’re making in real time. Base R actually has very powerful graphics capabilities for quickly visualizing data. Packages like ggplot2 and lattice provide powerful alternatives to base R plots. We’ll cover ggplot2 later. For now, base R plotting can provide all we need for exploratory analyses.\nOne of the most useful plots for numeric data is a histogram. Histograms bin the data and plot how many occurrences of a particular bin are present. This plot allows you to get an idea of the numeric summary of a variable. To plot a histogram of a numeric variable we can use the hist() function.\n\nhist(penguins$Flipper.Length..mm.)\n\n\n\n\n\n\n\n\nThe histogram has a few arguments we can use to adjust the plot. Use ?hist() to see the full list. Below, we can adjust the axes to be more informative and modify the number of bins we’re computing.\n\nhist(penguins$Flipper.Length..mm., \n     breaks = 30,\n     main = \"Flipper Length of All Palmer Penguins\", \n     xlab = \"Flipper Length (mm)\",\n     ylab = \"Count\"\n     )\n\n\n\n\n\n\n\n\nThe distribution appears to be bimodal. Is this because there are different species present in this plot? We can check by applying the hist() function to each of the groups using the list of data.frames from above.\n\n# Adelie penguins \nhist(by_species[[1]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Adelie\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n# Chinstrap penguins\nhist(by_species[[2]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Chinstrap\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n# Gentoo penguins\nhist(by_species[[3]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Gentoo\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n\nHistograms are useful for plotting the distribution of a single numeric variable. Often, we wish to see how two variables are related. The plot() function provides a way to create a scatter plot of two variables against each other on the same plot. For example, to plot the culmen length vs the culmen depth:\n\nplot(x = penguins$Culmen.Length..mm., \n     y = penguins$Culmen.Depth..mm.,\n     xlab = \"Culmen Length (mm)\",\n     ylab = \"Culmen Depth (mm)\",\n     main = \"Relationship between culmen length and depth\"\n     )\n\n\n\n\n\n\n\n\nAgain, these data points seem to be split by the different species of penguins present in the dataset. We can color these points using an additional variable in the call to the plot() function.\n\nplot(x = penguins$Culmen.Length..mm., \n     y = penguins$Culmen.Depth..mm.,\n     col = factor(penguins$Species),\n     xlab = \"Culmen Length (mm)\",\n     ylab = \"Culmen Depth (mm)\",\n     main = \"Relationship between culmen length and depth\"\n     )\n\n\n\n\n\n\n\n\nYou may have noticed that we did something special to that vector of species. We wrapped it in the factor() function. In R, factors are used when we have categorical values. R uses factors to represent the different levels of each category. It may not seem important now, but factors are very useful for statistical analysis. We’ll build on this topic shortly.\nThe plot() function is very versatile. You can use it to create line plots as well, to show trends of variables over time. Here is a contrived example of using plot() to create a simple line chart.\n\nplot(\n  x = adelie$Sample.Number, \n  y = adelie$Body.Mass..g., \n  type = \"l\", \n  main = \"Body mass vs sample number in Adelie penguins\",\n  xlab = \"Sample Number\",\n  ylab = \"Body Mass (g)\"\n  )\n\n\n\n\n\n\n\n\nScatter plots are useful for displaying two numeric values against each other. However you’ll often want to compare numeric values across categorical values. One such plot for doing this is called a boxplot. Boxplots show the median and interquartile (IQR) range for a set of data. The ‘whiskers’ of the boxplot display the 1.5 x IQR of the data. We can plot a boxplot of the flipper length for each species using the boxplot() function.\n\nboxplot(\n  Flipper.Length..mm. ~ Species, \n  data = penguins, \n  main = \"Flipper length by species\"\n  )\n\n\n\n\n\n\n\n\nThe boxplot() function is a little funny because it uses formula notation. Many functions in R accept formula notation as input. In this case, the formula notation means “plot the flipper length as a function of the species type”. You may have also noticed that we gave this function the penguins data.frame as the value for the data= argument. This allowed the boxplot function to use the column names without us having to explicitly say that they came from the penguins data.frame.\nIf there’s not too much data, boxplots can actually hide a lot of information. In this case, another option is to use a stripchart. A stripchart shows every data point on the plot instead of a depiction of the distribution as in the boxplot.\n\nstripchart(\n  Flipper.Length..mm. ~ Species, \n  data = penguins, \n  method = \"jitter\",\n  pch = 1,\n  main = \"Flipper length by species\",\n  vertical = TRUE\n  )\n\n\n\n\n\n\n\n\nBar charts are another common way that people show summaries of data. To display a barplot of data, we can use the barplot() function. To create a barplot of the mean flipper length for each species we first need to summarize the flipper length for each group and then supply these summaries to the barplot() function.\n\nspecies_means &lt;- tapply(penguins$Flipper.Length..mm., penguins$Species, mean, na.rm = TRUE)\nbarplot(species_means, main = \"Mean flipper length by species\")\n\n\n\n\n\n\n\n\nThere are many ways to modify these default plot types and make the charts look great. Take the time to look into some of base R graphing capabilities. A really good tutorial for learning about base R plotting can be found here.\n\n\nFiltering data\nWe’ve already learned how to subset a data.frame but let’s just take a step back and quickly revisit subsetting data.frames. To subset a data.frame means to select rows of a data.frame based on some condition that you’re interested in. This subsetting can be accomplished using the [ operator and setting conditions by specifying df[the rows we want, the cols we want] syntax.\nFor example, in the above plots it looked like the Gentoo penguins typically had the largest flippers. It looked like a value &gt; 205 roughly separated the Gentoo penguins from the others. How could we select the rows where the flipper length is greater than or equal to 205 and count the species types?\n\nbig_flippers &lt;- penguins[penguins$Flipper.Length..mm. &gt;= 205, ]\ntable(big_flippers$Species)\n\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                        3 \nChinstrap penguin (Pygoscelis antarctica) \n                                        8 \n        Gentoo penguin (Pygoscelis papua) \n                                      122 \n\n\nWe can combine conditions as well to select on multiple conditions. For example, let’s select the female penguins with flippers &gt;= 205 mm.\n\nhead(penguins[penguins$Sex == \"FEMALE\" & penguins$Flipper.Length..mm. &gt;= 205, ])\n\n    studyName Sample.Number                           Species Region Island\nNA       &lt;NA&gt;            NA                              &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n153   PAL0708             1 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n155   PAL0708             3 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n158   PAL0708             6 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n159   PAL0708             7 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n161   PAL0708             9 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n                 Stage Individual.ID Clutch.Completion   Date.Egg\nNA                &lt;NA&gt;          &lt;NA&gt;              &lt;NA&gt;       &lt;NA&gt;\n153 Adult, 1 Egg Stage         N31A1               Yes 2007-11-27\n155 Adult, 1 Egg Stage         N32A1               Yes 2007-11-27\n158 Adult, 1 Egg Stage         N33A2               Yes 2007-11-18\n159 Adult, 1 Egg Stage         N34A1               Yes 2007-11-27\n161 Adult, 1 Egg Stage         N35A1               Yes 2007-11-27\n    Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.\nNA                  NA                NA                  NA            NA\n153               46.1              13.2                 211          4500\n155               48.7              14.1                 210          4450\n158               46.5              13.5                 210          4550\n159               45.4              14.6                 211          4800\n161               43.3              13.4                 209          4400\n       Sex Delta.15.N..o.oo. Delta.13.C..o.oo. Comments\nNA    &lt;NA&gt;                NA                NA     &lt;NA&gt;\n153 FEMALE           7.99300         -25.51390     &lt;NA&gt;\n155 FEMALE           8.14705         -25.46172     &lt;NA&gt;\n158 FEMALE           7.99530         -25.32829     &lt;NA&gt;\n159 FEMALE           8.24515         -25.46782     &lt;NA&gt;\n161 FEMALE           8.13643         -25.32176     &lt;NA&gt;",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#data-wrangling",
    "href": "r-basics.html#data-wrangling",
    "title": "R programming basics",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nNow that we’ve got a handle on some different R data types, and how to slice and dice them, we can start learning basic data cleaning and exploratory data analysis. We’ll focus on using data.frames since they’re the primary workhorse of data analysis in R. But remember, data.frames are just lists of vectors with some special rules, so many concepts you learn will apply to data.frames but also apply to vectors and lists.\n\nData import\nWe’ll use the built in penguins_raw dataset to learn some basic data cleaning. This dataset is built into R version 4.5 so you can just load it by running penguins_raw if you have that R version installed. However, to illustrate data import, we’ll read in the data from an external source.\nThe read.csv() function can read in comma-separated value files that are located either on your local machine or from remote sources if provided a URL.\n\nurl &lt;- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins_raw.csv\"\npenguins &lt;- read.csv(url)\n\nThe read.csv() function has many options for reading in data. If you want to learn about all of the options any particular R function has, you can prefix the function name with a ? like, ?read.csv() to bring up the help documentation.\n\n\nViewing data\nThe code above read the data into data.frame that we called penguins. We can take a look at the first few rows of the penguins data.frame using the head() function.\n\nhead(penguins)\n\n  studyName Sample.Number                             Species Region    Island\n1   PAL0708             1 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n2   PAL0708             2 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n3   PAL0708             3 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n4   PAL0708             4 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n5   PAL0708             5 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n6   PAL0708             6 Adelie Penguin (Pygoscelis adeliae) Anvers Torgersen\n               Stage Individual.ID Clutch.Completion   Date.Egg\n1 Adult, 1 Egg Stage          N1A1               Yes 2007-11-11\n2 Adult, 1 Egg Stage          N1A2               Yes 2007-11-11\n3 Adult, 1 Egg Stage          N2A1               Yes 2007-11-16\n4 Adult, 1 Egg Stage          N2A2               Yes 2007-11-16\n5 Adult, 1 Egg Stage          N3A1               Yes 2007-11-16\n6 Adult, 1 Egg Stage          N3A2               Yes 2007-11-16\n  Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.    Sex\n1               39.1              18.7                 181          3750   MALE\n2               39.5              17.4                 186          3800 FEMALE\n3               40.3              18.0                 195          3250 FEMALE\n4                 NA                NA                  NA            NA   &lt;NA&gt;\n5               36.7              19.3                 193          3450 FEMALE\n6               39.3              20.6                 190          3650   MALE\n  Delta.15.N..o.oo. Delta.13.C..o.oo.                       Comments\n1                NA                NA Not enough blood for isotopes.\n2           8.94956         -24.69454                           &lt;NA&gt;\n3           8.36821         -25.33302                           &lt;NA&gt;\n4                NA                NA             Adult not sampled.\n5           8.76651         -25.32426                           &lt;NA&gt;\n6           8.66496         -25.29805                           &lt;NA&gt;\n\n\nIf we want to get a general overview of the data, we can use the str() function.\n\nstr(penguins)\n\n'data.frame':   344 obs. of  17 variables:\n $ studyName          : chr  \"PAL0708\" \"PAL0708\" \"PAL0708\" \"PAL0708\" ...\n $ Sample.Number      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Species            : chr  \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" \"Adelie Penguin (Pygoscelis adeliae)\" ...\n $ Region             : chr  \"Anvers\" \"Anvers\" \"Anvers\" \"Anvers\" ...\n $ Island             : chr  \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ Stage              : chr  \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" \"Adult, 1 Egg Stage\" ...\n $ Individual.ID      : chr  \"N1A1\" \"N1A2\" \"N2A1\" \"N2A2\" ...\n $ Clutch.Completion  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Date.Egg           : chr  \"2007-11-11\" \"2007-11-11\" \"2007-11-16\" \"2007-11-16\" ...\n $ Culmen.Length..mm. : num  39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ Culmen.Depth..mm.  : num  18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ Flipper.Length..mm.: int  181 186 195 NA 193 190 181 195 193 190 ...\n $ Body.Mass..g.      : int  3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ Sex                : chr  \"MALE\" \"FEMALE\" \"FEMALE\" NA ...\n $ Delta.15.N..o.oo.  : num  NA 8.95 8.37 NA 8.77 ...\n $ Delta.13.C..o.oo.  : num  NA -24.7 -25.3 NA -25.3 ...\n $ Comments           : chr  \"Not enough blood for isotopes.\" NA NA \"Adult not sampled.\" ...\n\n\nThere are a few external packages that are also very useful for getting summaries of data.frames. Hmsic::describe() and skimr::skim() are two standouts.\nOne of the most basic ways to get an idea of the data is to summarize each variable. There are a few functions we can use to get summaries of the data. The table() function will count the number of occurrences of each type in a vector.\nFor example, how many observations of each species of penguin are in the dataset?\n\ntable(penguins$Species)\n\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                      152 \nChinstrap penguin (Pygoscelis antarctica) \n                                       68 \n        Gentoo penguin (Pygoscelis papua) \n                                      124 \n\n\nR also provides the typical summary functions that you would expect from a statistical programming language such as mean(), median(), min(), and max(), and length().\nFor example, what is the mean flipper length?\n\nmean(penguins$Flipper.Length..mm.)\n\n[1] NA\n\n\nOn no! This returned NA but there is clearly data in this column. What happened? Missing data is commonly observed across all data domains. NAs simply represent unknown values in this context and it’s impossible to know how to take the mean of a value that’s known with a value that’s unknown. It is for this reason that many R functions have an argument called na.rm=. Setting na.rm=TRUE in these functions tells R to ignore the NA values.\n\nmean(penguins$Flipper.Length..mm., na.rm = TRUE)\n\n[1] 200.9152\n\n\nNow we can see that the mean flipper length is ~200 mm across all observations in the dataset. Another useful function is summary(). Running summary() on a numeric vector returns a lot of useful information.\n\nsummary(penguins$Flipper.Length..mm., na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  172.0   190.0   197.0   200.9   213.0   231.0       2 \n\n\nFinally, if you are using and IDE like Rstudio or Positron you can run the View() function on your data.frame. This will bring up an interactive data viewer.\n\n\nSplitting data\nYou may have noticed above that we computed the mean flipper length across all species of penguins. But do all species have the same mean? A common pattern in R is called “split-apply-combine”. This pattern means, split the data into groups you’re interested in, apply a function to each of those groups, and combine the results. One such function that performs this operation is called tapply(). tapply() will split the data by a given variable into groups and apply a function to each group.\nFor example, to find the mean flipper length for each species we could use\n\ntapply(penguins$Flipper.Length..mm., penguins$Species, mean, na.rm = TRUE)\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                 189.9536 \nChinstrap penguin (Pygoscelis antarctica) \n                                 195.8235 \n        Gentoo penguin (Pygoscelis papua) \n                                 217.1870 \n\n\nThe basic format of the tapply() function is\n\ntapply(\"data to split\", \"what to split by\", \"what to compute\")\n\nSplitting can also by applied directly to data.frames using the split() function. Let’s say we wanted to split the penguins data.frame into one data.frame for each species. We can use the split() function for this purpose.\n\nby_species &lt;- split(penguins, f = penguins$Species)\n\nThis function returns a list of data.frames, one for each species in the original data.frame. Use names(by_species) to see what each list element is named. To extract the first data.frame from this list, which contains Adelie penguin data only, we can subset the list of data.frames.\n\nadelie &lt;- by_species[[1]]\n\nPerforming operations on lists is such a common task in R that a function exists specifically to apply functions to list elements. This function is called lapply(). lapply() takes a list and a function and applies that function to each list element. The lapply() function returns the results as a list. We’ll learn more about this later in the functional programming section.\nFor example, to see how many rows are in each of the data.frames in the “by_species” list:\n\nlapply(by_species, nrow)\n\n$`Adelie Penguin (Pygoscelis adeliae)`\n[1] 152\n\n$`Chinstrap penguin (Pygoscelis antarctica)`\n[1] 68\n\n$`Gentoo penguin (Pygoscelis papua)`\n[1] 124\n\n\n\n\nPlotting data\nData cleaning and data visualization go hand-in-hand. To effectively clean data, you should be examining the changes you’re making in real time. Base R actually has very powerful graphics capabilities for quickly visualizing data. Packages like ggplot2 and lattice provide powerful alternatives to base R plots. We’ll cover ggplot2 later. For now, base R plotting can provide all we need for exploratory analyses.\nOne of the most useful plots for numeric data is a histogram. Histograms bin the data and plot how many occurrences of a particular bin are present. This plot allows you to get an idea of the numeric summary of a variable. To plot a histogram of a numeric variable we can use the hist() function.\n\nhist(penguins$Flipper.Length..mm.)\n\n\n\n\n\n\n\n\nThe histogram has a few arguments we can use to adjust the plot. Use ?hist() to see the full list. Below, we can adjust the axes to be more informative and modify the number of bins we’re computing.\n\nhist(penguins$Flipper.Length..mm., \n     breaks = 30,\n     main = \"Flipper Length of All Palmer Penguins\", \n     xlab = \"Flipper Length (mm)\",\n     ylab = \"Count\"\n     )\n\n\n\n\n\n\n\n\nThe distribution appears to be bimodal. Is this because there are different species present in this plot? We can check by applying the hist() function to each of the groups using the list of data.frames from above.\n\n# Adelie penguins \nhist(by_species[[1]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Adelie\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n# Chinstrap penguins\nhist(by_species[[2]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Chinstrap\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n# Gentoo penguins\nhist(by_species[[3]]$Flipper.Length..mm., \n     breaks = 20, \n     main = \"Gentoo\", \n     xlab = \"Flipper Length (mm)\")\n\n\n\n\n\n\n\n\nHistograms are useful for plotting the distribution of a single numeric variable. Often, we wish to see how two variables are related. The plot() function provides a way to create a scatter plot of two variables against each other on the same plot. For example, to plot the culmen length vs the culmen depth:\n\nplot(x = penguins$Culmen.Length..mm., \n     y = penguins$Culmen.Depth..mm.,\n     xlab = \"Culmen Length (mm)\",\n     ylab = \"Culmen Depth (mm)\",\n     main = \"Relationship between culmen length and depth\"\n     )\n\n\n\n\n\n\n\n\nAgain, these data points seem to be split by the different species of penguins present in the dataset. We can color these points using an additional variable in the call to the plot() function.\n\nplot(x = penguins$Culmen.Length..mm., \n     y = penguins$Culmen.Depth..mm.,\n     col = factor(penguins$Species),\n     xlab = \"Culmen Length (mm)\",\n     ylab = \"Culmen Depth (mm)\",\n     main = \"Relationship between culmen length and depth\"\n     )\n\n\n\n\n\n\n\n\nYou may have noticed that we did something special to that vector of species. We wrapped it in the factor() function. In R, factors are used when we have categorical values. R uses factors to represent the different levels of each category. It may not seem important now, but factors are very useful for statistical analysis. We’ll build on this topic shortly.\nThe plot() function is very versatile. You can use it to create line plots as well, to show trends of variables over time. Here is a contrived example of using plot() to create a simple line chart.\n\nplot(\n  x = adelie$Sample.Number, \n  y = adelie$Body.Mass..g., \n  type = \"l\", \n  main = \"Body mass vs sample number in Adelie penguins\",\n  xlab = \"Sample Number\",\n  ylab = \"Body Mass (g)\"\n  )\n\n\n\n\n\n\n\n\nScatter plots are useful for displaying two numeric values against each other. However you’ll often want to compare numeric values across categorical values. One such plot for doing this is called a boxplot. Boxplots show the median and interquartile (IQR) range for a set of data. The ‘whiskers’ of the boxplot display the 1.5 x IQR of the data. We can plot a boxplot of the flipper length for each species using the boxplot() function.\n\nboxplot(\n  Flipper.Length..mm. ~ Species, \n  data = penguins, \n  main = \"Flipper length by species\"\n  )\n\n\n\n\n\n\n\n\nThe boxplot() function is a little funny because it uses formula notation. Many functions in R accept formula notation as input. In this case, the formula notation means “plot the flipper length as a function of the species type”. You may have also noticed that we gave this function the penguins data.frame as the value for the data= argument. This allowed the boxplot function to use the column names without us having to explicitly say that they came from the penguins data.frame.\nIf there’s not too much data, boxplots can actually hide a lot of information. In this case, another option is to use a stripchart. A stripchart shows every data point on the plot instead of a depiction of the distribution as in the boxplot.\n\nstripchart(\n  Flipper.Length..mm. ~ Species, \n  data = penguins, \n  method = \"jitter\",\n  pch = 1,\n  main = \"Flipper length by species\",\n  vertical = TRUE\n  )\n\n\n\n\n\n\n\n\nBar charts are another common way that people show summaries of data. To display a barplot of data, we can use the barplot() function. To create a barplot of the mean flipper length for each species we first need to summarize the flipper length for each group and then supply these summaries to the barplot() function.\n\nspecies_means &lt;- tapply(penguins$Flipper.Length..mm., penguins$Species, mean, na.rm = TRUE)\nbarplot(species_means, main = \"Mean flipper length by species\")\n\n\n\n\n\n\n\n\nThere are many ways to modify these default plot types and make the charts look great. Take the time to look into some of base R graphing capabilities. A really good tutorial for learning about base R plotting can be found here. Also take a look at ?pairs(), ?dotchart(), and ?coplot() for some other useful base R graphics functions for creating quick plots.\n\n\nFiltering data\nWe’ve already learned how to subset a data.frame but let’s just take a step back and quickly revisit subsetting data.frames. To subset a data.frame means to select rows of a data.frame based on some condition that you’re interested in. This subsetting can be accomplished using the [ operator and setting conditions by specifying df[the rows we want, the cols we want] syntax.\nFor example, in the above plots it looked like the Gentoo penguins typically had the largest flippers. It looked like a value &gt; 205 roughly separated the Gentoo penguins from the others. How could we select the rows where the flipper length is greater than or equal to 205 and count the species types?\n\nbig_flippers &lt;- penguins[penguins$Flipper.Length..mm. &gt;= 205, ]\ntable(big_flippers$Species)\n\n\n      Adelie Penguin (Pygoscelis adeliae) \n                                        3 \nChinstrap penguin (Pygoscelis antarctica) \n                                        8 \n        Gentoo penguin (Pygoscelis papua) \n                                      122 \n\n\nWe can combine conditions as well to select on multiple conditions. For example, let’s select the female penguins with flippers &gt;= 205 mm.\n\nhead(penguins[penguins$Sex == \"FEMALE\" & penguins$Flipper.Length..mm. &gt;= 205, ])\n\n    studyName Sample.Number                           Species Region Island\nNA       &lt;NA&gt;            NA                              &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;\n153   PAL0708             1 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n155   PAL0708             3 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n158   PAL0708             6 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n159   PAL0708             7 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n161   PAL0708             9 Gentoo penguin (Pygoscelis papua) Anvers Biscoe\n                 Stage Individual.ID Clutch.Completion   Date.Egg\nNA                &lt;NA&gt;          &lt;NA&gt;              &lt;NA&gt;       &lt;NA&gt;\n153 Adult, 1 Egg Stage         N31A1               Yes 2007-11-27\n155 Adult, 1 Egg Stage         N32A1               Yes 2007-11-27\n158 Adult, 1 Egg Stage         N33A2               Yes 2007-11-18\n159 Adult, 1 Egg Stage         N34A1               Yes 2007-11-27\n161 Adult, 1 Egg Stage         N35A1               Yes 2007-11-27\n    Culmen.Length..mm. Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.\nNA                  NA                NA                  NA            NA\n153               46.1              13.2                 211          4500\n155               48.7              14.1                 210          4450\n158               46.5              13.5                 210          4550\n159               45.4              14.6                 211          4800\n161               43.3              13.4                 209          4400\n       Sex Delta.15.N..o.oo. Delta.13.C..o.oo. Comments\nNA    &lt;NA&gt;                NA                NA     &lt;NA&gt;\n153 FEMALE           7.99300         -25.51390     &lt;NA&gt;\n155 FEMALE           8.14705         -25.46172     &lt;NA&gt;\n158 FEMALE           7.99530         -25.32829     &lt;NA&gt;\n159 FEMALE           8.24515         -25.46782     &lt;NA&gt;\n161 FEMALE           8.13643         -25.32176     &lt;NA&gt;\n\n\nAnother important aspect of base R data.frames is that they can utilize rownames(). Using rownames() is uncommon in workflows from the tidyverse or even data.table. However, rownames() can be very useful when dealing with bioinformatics data. One R package that utilizes rownames() extensively is the SummarizedExperiment. SummarizedExperiments use rownames and colnames to ensure that data stays coordinated during an analysis.\nYou can view and set the rownames() on a data.frame using the rownames() function.\n\n# View the current rownames\nhead(rownames(penguins))\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n# Set the rownames based on a new value - we'll go over what paste() does shortly\nrownames(penguins) &lt;- paste(\"row_number\", 1:nrow(penguins), sep = \".\")\n\n# View the new rownames\nhead(rownames(penguins))\n\n[1] \"row_number.1\" \"row_number.2\" \"row_number.3\" \"row_number.4\" \"row_number.5\"\n[6] \"row_number.6\"\n\n\nAbove, we filtered by a column in the data.frame. We can also filter by selecting rownames.\n\npenguins[c(\"row_number.1\", \"row_number.5\"), ]\n\n             studyName Sample.Number                             Species Region\nrow_number.1   PAL0708             1 Adelie Penguin (Pygoscelis adeliae) Anvers\nrow_number.5   PAL0708             5 Adelie Penguin (Pygoscelis adeliae) Anvers\n                Island              Stage Individual.ID Clutch.Completion\nrow_number.1 Torgersen Adult, 1 Egg Stage          N1A1               Yes\nrow_number.5 Torgersen Adult, 1 Egg Stage          N3A1               Yes\n               Date.Egg Culmen.Length..mm. Culmen.Depth..mm.\nrow_number.1 2007-11-11               39.1              18.7\nrow_number.5 2007-11-16               36.7              19.3\n             Flipper.Length..mm. Body.Mass..g.    Sex Delta.15.N..o.oo.\nrow_number.1                 181          3750   MALE                NA\nrow_number.5                 193          3450 FEMALE           8.76651\n             Delta.13.C..o.oo.                       Comments\nrow_number.1                NA Not enough blood for isotopes.\nrow_number.5         -25.32426                           &lt;NA&gt;\n\n\n\n\nReordering data\nWe can also change the order of the rows in a data.frame. This is similar to subsetting but instead rearranges the data based on some condition. To reorder the data.frame we can use the order() function.\nFor example, we can order the data by decreasing flipper length\n\nhead(penguins[order(penguins$Flipper.Length..mm., decreasing = TRUE), ])\n\n               studyName Sample.Number                           Species Region\nrow_number.216   PAL0809            64 Gentoo penguin (Pygoscelis papua) Anvers\nrow_number.154   PAL0708             2 Gentoo penguin (Pygoscelis papua) Anvers\nrow_number.186   PAL0708            34 Gentoo penguin (Pygoscelis papua) Anvers\nrow_number.218   PAL0809            66 Gentoo penguin (Pygoscelis papua) Anvers\nrow_number.228   PAL0809            76 Gentoo penguin (Pygoscelis papua) Anvers\nrow_number.242   PAL0910            90 Gentoo penguin (Pygoscelis papua) Anvers\n               Island              Stage Individual.ID Clutch.Completion\nrow_number.216 Biscoe Adult, 1 Egg Stage         N19A2               Yes\nrow_number.154 Biscoe Adult, 1 Egg Stage         N31A2               Yes\nrow_number.186 Biscoe Adult, 1 Egg Stage         N56A2               Yes\nrow_number.218 Biscoe Adult, 1 Egg Stage         N20A2               Yes\nrow_number.228 Biscoe Adult, 1 Egg Stage         N56A2               Yes\nrow_number.242 Biscoe Adult, 1 Egg Stage         N14A2               Yes\n                 Date.Egg Culmen.Length..mm. Culmen.Depth..mm.\nrow_number.216 2008-11-13               54.3              15.7\nrow_number.154 2007-11-27               50.0              16.3\nrow_number.186 2007-12-03               59.6              17.0\nrow_number.218 2008-11-04               49.8              16.8\nrow_number.228 2008-11-06               48.6              16.0\nrow_number.242 2009-11-25               52.1              17.0\n               Flipper.Length..mm. Body.Mass..g.  Sex Delta.15.N..o.oo.\nrow_number.216                 231          5650 MALE           8.49662\nrow_number.154                 230          5700 MALE           8.14756\nrow_number.186                 230          6050 MALE           7.76843\nrow_number.218                 230          5700 MALE           8.47067\nrow_number.228                 230          5800 MALE           8.59640\nrow_number.242                 230          5550 MALE           8.27595\n               Delta.13.C..o.oo. Comments\nrow_number.216         -26.84166     &lt;NA&gt;\nrow_number.154         -25.39369     &lt;NA&gt;\nrow_number.186         -25.68210     &lt;NA&gt;\nrow_number.218         -26.69166     &lt;NA&gt;\nrow_number.228         -26.71199     &lt;NA&gt;\nrow_number.242         -26.11657     &lt;NA&gt;\n\n\nR also provides the sort() function. See if you can understand the difference between order() and sort().\nSimilar to filtering above, we can also reorder by the rownames().\n\nhead(penguins[order(rownames(penguins)), ])\n\n               studyName Sample.Number                             Species\nrow_number.1     PAL0708             1 Adelie Penguin (Pygoscelis adeliae)\nrow_number.10    PAL0708            10 Adelie Penguin (Pygoscelis adeliae)\nrow_number.100   PAL0809           100 Adelie Penguin (Pygoscelis adeliae)\nrow_number.101   PAL0910           101 Adelie Penguin (Pygoscelis adeliae)\nrow_number.102   PAL0910           102 Adelie Penguin (Pygoscelis adeliae)\nrow_number.103   PAL0910           103 Adelie Penguin (Pygoscelis adeliae)\n               Region    Island              Stage Individual.ID\nrow_number.1   Anvers Torgersen Adult, 1 Egg Stage          N1A1\nrow_number.10  Anvers Torgersen Adult, 1 Egg Stage          N5A2\nrow_number.100 Anvers     Dream Adult, 1 Egg Stage         N50A2\nrow_number.101 Anvers    Biscoe Adult, 1 Egg Stage         N47A1\nrow_number.102 Anvers    Biscoe Adult, 1 Egg Stage         N47A2\nrow_number.103 Anvers    Biscoe Adult, 1 Egg Stage         N49A1\n               Clutch.Completion   Date.Egg Culmen.Length..mm.\nrow_number.1                 Yes 2007-11-11               39.1\nrow_number.10                Yes 2007-11-09               42.0\nrow_number.100               Yes 2008-11-10               43.2\nrow_number.101               Yes 2009-11-09               35.0\nrow_number.102               Yes 2009-11-09               41.0\nrow_number.103               Yes 2009-11-15               37.7\n               Culmen.Depth..mm. Flipper.Length..mm. Body.Mass..g.    Sex\nrow_number.1                18.7                 181          3750   MALE\nrow_number.10               20.2                 190          4250   &lt;NA&gt;\nrow_number.100              18.5                 192          4100   MALE\nrow_number.101              17.9                 192          3725 FEMALE\nrow_number.102              20.0                 203          4725   MALE\nrow_number.103              16.0                 183          3075 FEMALE\n               Delta.15.N..o.oo. Delta.13.C..o.oo.\nrow_number.1                  NA                NA\nrow_number.10            9.13362         -25.09368\nrow_number.100           8.97025         -26.03679\nrow_number.101           8.84451         -26.28055\nrow_number.102           9.01079         -26.38085\nrow_number.103           9.21510         -26.22530\n                                           Comments\nrow_number.1         Not enough blood for isotopes.\nrow_number.10  No blood sample obtained for sexing.\nrow_number.100                                 &lt;NA&gt;\nrow_number.101                                 &lt;NA&gt;\nrow_number.102                                 &lt;NA&gt;\nrow_number.103                                 &lt;NA&gt;\n\n\nCan you figure out why the data.frame was sorted this way?\n\n\nSelecting columns of data\nSimilarly to how we were able to select rows of data from the data.frame, we can also select columns of the data.frame. First, to see what columns are in our data.frame we can use the colnames() function.\n\ncolnames(penguins)\n\n [1] \"studyName\"           \"Sample.Number\"       \"Species\"            \n [4] \"Region\"              \"Island\"              \"Stage\"              \n [7] \"Individual.ID\"       \"Clutch.Completion\"   \"Date.Egg\"           \n[10] \"Culmen.Length..mm.\"  \"Culmen.Depth..mm.\"   \"Flipper.Length..mm.\"\n[13] \"Body.Mass..g.\"       \"Sex\"                 \"Delta.15.N..o.oo.\"  \n[16] \"Delta.13.C..o.oo.\"   \"Comments\"           \n\n\nTo select columns, we can either specify the columns we want by name\n\nhead(penguins[, c(\"studyName\", \"Species\")])\n\n             studyName                             Species\nrow_number.1   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.2   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.3   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.4   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.5   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.6   PAL0708 Adelie Penguin (Pygoscelis adeliae)\n\n\nOr by the column index\n\nhead(penguins[, c(1, 3)])\n\n             studyName                             Species\nrow_number.1   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.2   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.3   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.4   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.5   PAL0708 Adelie Penguin (Pygoscelis adeliae)\nrow_number.6   PAL0708 Adelie Penguin (Pygoscelis adeliae)\n\n\nIf we want to rename a column, we can use the colnames() function to reassign the column name with a new name. First, we can get the index of the column we want to change. Then we can reassign that column with the new name.\n\n# This returns the index of the colnames() that matches \"Species\"\nwhich_col_to_change &lt;- which(colnames(penguins) == \"Species\")\n\n# Rename \"Species\" to \"species\"\ncolnames(penguins)[which_col_to_change] &lt;- \"species\"\n\nOne tricky aspect of base R subsetting on data.frames has to do with selecting a single column of data. If you select a single column of data from a data.frame what you get back is a vector. To ensure that you return a data.frame when selecting a single column, add the drop=FALSE argument when selecting a single column.\n\n# Returns a vector\nclass(penguins[, \"species\"])\n\n[1] \"character\"\n\n# Returns a data.frame\nclass(penguins[, \"species\", drop = FALSE])\n\n[1] \"data.frame\"\n\n\n\n\nModifying columns of data\nAdding or removing columns of data to a data.frame is as simple as specifying the name of the column you want to add along with the data that you want to add.\nFor example, if I wanted to convert the flipper length from mm to m I could multiply each value of flipper length by 0.001.\n\npenguins$flipper_length_m &lt;- penguins$Flipper.Length..mm. * 0.001\n\nThe above data transformation works because of R’s vectorization rule described above. What if I wanted to define a new variable based on a logical condition? For example, what if I wanted to create a new column based on whether or not a penguin was and Adelie or any other kind? A useful function for performing this action is ifelse(). ifelse() evaluates a logical condition and returns a value based on whether the condition evaluates to TRUE or FALSE.\n\npenguins$is_adelie &lt;- ifelse(\n  penguins$species == \"Adelie Penguin (Pygoscelis adeliae)\", \n  \"Adelie\", \n  \"Other\"\n  )\n\nIf I want to remove a column from the data.frame I can set the value of the column to NULL\n\npenguins$Flipper.Length..mm. &lt;- NULL",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#string-operations",
    "href": "r-basics.html#string-operations",
    "title": "R programming basics",
    "section": "String operations",
    "text": "String operations\nString operations are an important part of data cleaning. Base R supports many functions for transforming strings. Again, these string functions are typically vectorized meaning that they can easily be used to modify columns of data.frames.\nThe “species” column of the penguins data.frame is annoying to work with. We can simplify this column by creating simpler names for each species by excluding the scientific name and the word ‘penguin’ (we know they’re all penguins…)\nOne base R function we can use to find and replace text is called gsub(). gsub() takes what is called a regular expression to find text inside of a string and then replaces the text it finds with the text you supply. Regular expressions can become incredibly complex. With this complexity come a lot of power. It would be impossible to teach regular expression in this tutorial. Taking some time to understand the basics of regular expressions can go a long way during data cleaning.\n\n# Replace a space followed by the word 'penguins' and any other character\npenguins$species_clean &lt;- gsub(\n  pattern = \" penguin.*\", \n  replacement = \"\", \n  x = penguins$species, \n  ignore.case = TRUE\n  )\n\n# Show the counts for these new categories\ntable(penguins$species_clean)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nAnother base R function that is useful for extracting text is the substr() function. substr() extracts sub-strings from the supplied text based on the index of the text. For example, to create an even simpler representation of species we could extract only the first letter from the “species” column.\n\npenguins$species_simple &lt;- substr(penguins$species, start = 1, stop = 1)\ntable(penguins$species_simple)\n\n\n  A   C   G \n152  68 124 \n\n\nA slightly more complicated string operation is extracting text from a string. This involves two functions; one to find a match in the text and another to extract it. Let’s say we wanted to extract the word ‘penguin’ from each of the original species strings. We can do this with a combination of regmatches() and regexpr().\n\npenguins$only_penguin &lt;- regmatches(\n  x = penguins$species, \n  m = regexpr(pattern = \"penguin\", \n              text = penguins$species, \n              ignore.case = TRUE)\n  )\n\nR also supports basic operations on strings like making all characters upper or lowercase. For example, to ensure only_penguin contains ‘penguin’ (and not ‘Penguin’) we can convert the string to lowercase\n\n# lower case\nhead(tolower(penguins$only_penguin))\n\n[1] \"penguin\" \"penguin\" \"penguin\" \"penguin\" \"penguin\" \"penguin\"\n\n# or upper case\nhead(toupper(penguins$only_penguin))\n\n[1] \"PENGUIN\" \"PENGUIN\" \"PENGUIN\" \"PENGUIN\" \"PENGUIN\" \"PENGUIN\"\n\n\nAnother slightly complicated string operation is string splitting. String splitting in base R can be accomplished using the strsplit() function. This function is a little tricky because it returns a list. Here, we split the “species” column on every space character. Using the results from the strsplit() function most efficiently involves some advanced R skills.\n\nsplit_species &lt;- strsplit(\n  x = penguins$species, \n  split = \" \"\n  )\n\n# strsplit returns a list of character vectors split on every space\nhead(split_species)\n\n[[1]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n[[2]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n[[3]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n[[4]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n[[5]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n[[6]]\n[1] \"Adelie\"      \"Penguin\"     \"(Pygoscelis\" \"adeliae)\"   \n\n# To extract the species from this list, we can use a fancy trick\n# don't worry about what's happening here right now\npenguins$extracted_species &lt;- sapply(split_species, `[[`, 1)",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#useful-functions",
    "href": "r-basics.html#useful-functions",
    "title": "R programming basics",
    "section": "Useful functions",
    "text": "Useful functions",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#tips-and-tricks",
    "href": "r-basics.html#tips-and-tricks",
    "title": "R programming basics",
    "section": "Tips and tricks",
    "text": "Tips and tricks",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#functions",
    "href": "r-basics.html#functions",
    "title": "R programming basics",
    "section": "Functions",
    "text": "Functions\nEventually you’ll want to create your own functions to do stuff. Functions can be defined in R using the following syntax\n\nf &lt;- function(args) {\n  expr\n}\n\nTo make this a little more concrete, let’s define a function that classifies a penguin based on it’s flipper size. Our function will take in a flipper size and a threshold value for determining whether or not the penguin is “large” or “small” based on this flipper size. The function will return the resulting size designation.\n\ndetermine_size &lt;- function(flipper_size, threshold = 0.2) {\n  if (flipper_size &gt;= threshold) {\n    size_category &lt;- \"large\"\n  } else {\n    size_category &lt;- \"small\"\n  }\n  \n  return(size_category)\n}\n\n# Is a 0.5 meter flipper length large or small?\ndetermine_size(flipper_size = 0.5)\n\n[1] \"large\"",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#functional-programming",
    "href": "r-basics.html#functional-programming",
    "title": "R programming basics",
    "section": "Functional programming",
    "text": "Functional programming\nFor-loops in R are very simple to understand but most R programmers use functions in the apply() family. These functions supply abstractions over common for-loops that make applying functions over lists much easier in R.\nThe most commonly used functional programming paradigm in R is the use of the lapply() function. The lapply() function takes a list as input, applies a function to each element of that list, and returns the results as a list. For example, above we split the penguins data.frame into a list of smaller data.frames for each species. Instead of repeating the code to create a histogram from each data.frame, let’s define a function that plots a histogram using a data.frame and then apply that function to every data.frame in our “by_species” list of data.frames\n\n# Define a function for plotting a histogram of flipper lengths\nplot_histogram &lt;- function(df) {\n  hist(df[, \"Flipper.Length..mm.\"], breaks=20)\n}\n\n# Apply this function to every data.frame\n# invisible() is used to hide the output from the hist() function\ninvisible(\n  lapply(X = by_species, FUN = plot_histogram)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR includes some other specialized functions for functional programming. Be sure to explore apply() for computing over rows/columns of matrices, sapply()and vapply() for applying functions over lists and returning vectors, mapply() for supplying multiple arguments to a list.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#control-flow",
    "href": "r-basics.html#control-flow",
    "title": "R programming basics",
    "section": "Control flow",
    "text": "Control flow\nR contains all of the typical control flow you expect from a programming language.\nFor-loops are somewhat under-utilized by many R users but can often be the most clear for new (and experienced) users. one common for-loop idion in R is to use a for-loop with the seq_along() function. seq_along() generates an index of the given vector that you can use to loop over.\n\nfor (i in seq_along(x)) {\n  doSomething(x[i])\n}\n\nOther looping operations in base R include while and repeat.\nConditional statements can be tested using if statements. These follow a familiar syntax\n\nx &lt;- 10\nif (x == 10) {\n  print(\"the value of x is 10\")\n} else {\n  print(\"the value of x is something else\")\n}\n\n[1] \"the value of x is 10\"\n\n\nIf statements in R are not vectorized. Use ifelse() to perform a vectorized if statement as demonstrated above.\nIf statements are often combined with for-loops.\n\n# Make some space to save the result\nresult &lt;- vector(\"character\", length = 10)\n\n# Determine if the number is even or odd\nfor (i in 1:10) {\n  if (i %% 2 == 0) {\n    result[i] &lt;- \"even\"\n  } else {\n    result[i] &lt;- \"odd\"\n  }\n}\n\nresult\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\nR also contains a switch() statement which can be very useful when programming new functions.",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "r-basics.html#resources",
    "href": "r-basics.html#resources",
    "title": "R programming basics",
    "section": "Resources",
    "text": "Resources\nThis only scratches the surface of programming in R. Check out these resources for more information.\n\nfasteR\nR for Data Science\nAdvanced R 2nd Edition\nAdvanced R 1st Edition\nThe R Inferno",
    "crumbs": [
      "R programming basics"
    ]
  },
  {
    "objectID": "data-viz.html#plotting-with-ggplot2",
    "href": "data-viz.html#plotting-with-ggplot2",
    "title": "Effective data visualizations",
    "section": "",
    "text": "Data: The dataset you want to visualize.\nAesthetics (aes): How columns in your data map to visual properties of the graphic. For example, mapping a ‘temperature’ column to the y-axis, a ‘time’ column to the x-axis, and a ‘city’ column to color.\nGeometries (geoms): The visual elements used to represent the data, such as points, lines, bars, histograms, etc.\nScales: Control how the aesthetic mappings translate to visual output (e.g., how data values are converted to colors, sizes, or positions on an axis).\nStatistics (stats): Transformations of the data that are performed before plotting (e.g., calculating a histogram, smoothing a line).\nCoordinates: The coordinate system used for the plot (e.g., Cartesian, polar).\nFaceting: Creating multiple subplots (a “trellis” or “lattice” plot) based on subsets of the data.\nTheme: Non-data elements like background, grid lines, and font choices.",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#resources",
    "href": "data-viz.html#resources",
    "title": "Effective data visualizations",
    "section": "Resources",
    "text": "Resources\n\nLearning ggplot\nggplot2 Book\nModern Data Visualization with R\nR for Data Science Data Viz chapter\nFundamentals of Data Visualization\nThe Visual Display of Quantitative Information\nR Graph Gallery",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#line-graphs",
    "href": "data-viz.html#line-graphs",
    "title": "Effective data visualizations",
    "section": "Line graphs",
    "text": "Line graphs\nLine graphs are meant to emphasize change in the y-variable over the x-variable. When designing line graphs:\n\nTake advantage the range of the data. Data should take up about 3/4 of the y-axis. ggplot has pretty good defaults for this automatically.\nChoose line weights that do not overshadow points (if present)\nDashed lines can be hard to read. Use contrasting colors instead\nIf presenting two graphs next to each other be sure to match the axes ranges\n\n\nggplot(economics, aes(date, unemploy)) + \n  geom_line() +\n  labs(\n    title = \"Unemployed Individuals Over Time\",\n    x = \"Year\", \n    y = \"Thousands of Persons\"\n    ) +\n  scale_y_continuous(labels = scales::number_format(big.mark = \",\")) +\n  theme_clean()\n\n\n\n\n\n\n\n\nWe can also explore adding multiple colors to the plot to compare values. Another useful technique is to add the legend to the plot area.\n\necon_2 &lt;- economics_long[!economics_long$variable %in% c(\"pce\", \"pop\"), ]\n\nggplot(econ_2, aes(date, value01, colour = variable)) +\n  geom_line() +\n  labs(\n    x = \"Year\", \n    y = \"Variable\",\n    color = NULL\n    ) +\n  scale_color_brewer(                             # Add better colors and labels\n    palette = \"Set1\", \n    breaks = c(\"psavert\", \"uempmed\", \"unemploy\"),\n    labels = c(\"Savings\", \"Duration\", \"Unemployed\")\n    ) +\n  scale_y_continuous(labels = scales::number_format(big.mark = \",\")) +\n  guides(color = guide_legend(position = \"inside\")) +  # add the legend inside the plot\n  theme_clean() +\n  theme(\n    legend.position.inside = c(0.5, 0.85),      # specify where to put the legend\n    legend.text = element_text(size = 12)\n    )",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#bar-graphs",
    "href": "data-viz.html#bar-graphs",
    "title": "Effective data visualizations",
    "section": "Bar graphs",
    "text": "Bar graphs\nPeople generally use bar graphs to display a mean and some variation around the mean. Filled bar plots can also be used to show proportions.\n\nIf there are relatively few data points, consider showing all points and a crossbar for the mean and error instead. For example, as a beeswarm plot. If there are many points, consider a boxplot.\nIn most cases, it’s usually best to start the y-axis at 0 if the data has a natural 0. The exception is for charts where 0 does not indicate ‘nothing’ but rather exists on a continuum, for example, temperature in Fahrenheit.\nDon’t have bars with too thick of outlines\nAvoid bars that are too thin. Aim for about 1/3 width of the bar as space between bars\nPlace extra space between categories if showing bars next to each other\nIf showing statistical information try not to overwhelm the data. Use subtle thin lines for comparisons and asterisks for significance.\n\nActivity: Make this example plot better.\n\ndf &lt;- data.frame(trt = c(\"a\", \"b\", \"c\"), outcome = c(2.3, 1.9, 3.2))\n\nggplot(df, aes(trt, outcome)) +\n  geom_col() +\n  theme_clean()",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#histograms",
    "href": "data-viz.html#histograms",
    "title": "Effective data visualizations",
    "section": "Histograms",
    "text": "Histograms\nHistograms are used to show distributions of data with their relative frequencies.\n\nThe number of bins influences the interpretation of the data. Play around with the number of bins to ensure the best display\nDon’t assign different colors to different bins - it doesn’t add any information\nDon’t add spaces between bins, as in a bar plot\nIf displaying two datasets, you can either overlay each with a different fill and some transparency or split into two histograms with the same y-axis.\n\nActivity: Make this example plot better.\n\nggplot(diamonds, aes(carat)) +\n  geom_histogram() +\n  theme_clean()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#scatter-plots",
    "href": "data-viz.html#scatter-plots",
    "title": "Effective data visualizations",
    "section": "Scatter plots",
    "text": "Scatter plots\nWe already showed a basic example of creating a scatter plot above. You can use that as a starting point for generating scatter plots. However, one common issue when designing scatter plots is overplotting, or, showing so many points that the data is cluttered. Below is an example of overplotting.\n\nggplot(diamonds, aes(carat, price)) +\n  geom_point() +\n  theme_clean()\n\n\n\n\n\n\n\n\nOne technique to overcome overplotting is to add transparency to the points\n\nggplot(diamonds, aes(carat, price)) +\n  geom_point(alpha = 0.05) +\n  theme_clean()\n\n\n\n\n\n\n\n\nAnother is to change the point type. Here, we plot each point as a single dot\n\nggplot(diamonds, aes(carat, price)) +\n  geom_point(shape = \".\") +\n  theme_clean()\n\n\n\n\n\n\n\n\nAnd another is random subsampling.\n\nrandom_rows &lt;- sample.int(nrow(diamonds), size = 500)\n\nggplot(diamonds[random_rows, ], aes(carat, price)) +\n  geom_point() +\n  theme_clean()\n\n\n\n\n\n\n\n\nThe density of the points could also be summarized. For example, explore geom_hex() or geom_density2d() geoms.\nLet’s make a final version of this plot by cleaning up the background, axes, and titles. We’ll add a trendline to emphasize the relationship and we’ll also use a function to transform the y-axis to dollar format.\n\nggplot(diamonds[random_rows, ], aes(carat, price)) +\n  geom_point() +\n  geom_smooth(se = FALSE, color = \"red\") +   # Adds a smooth trendline\n  labs(\n    title = \"Larger Diamonds are More Expensive\",    \n    x  = \"Carat\",\n    y = \"Price ($)\") +\n  scale_y_continuous(             # Formats the y-axis labels as dollar amounts\n    labels = scales::dollar_format()\n    )  +\n  theme_clean()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#colors",
    "href": "data-viz.html#colors",
    "title": "Effective data visualizations",
    "section": "Colors",
    "text": "Colors",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#axes-and-legends",
    "href": "data-viz.html#axes-and-legends",
    "title": "Effective data visualizations",
    "section": "Axes and legends",
    "text": "Axes and legends",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#image-file-formats",
    "href": "data-viz.html#image-file-formats",
    "title": "Effective data visualizations",
    "section": "Image file formats",
    "text": "Image file formats\n\nSave your images in .pdf format or .svg format. These formats are called vector graphics. Vector graphics are infinitely scalable. They never lose resolution no matter the zoom level.\nVector graphics don’t play nicely with Word documents. If you need to share images that will be used in Word docs, save your image as a high-quality .png. .png files are raster graphics. Raster based images store actual pixel values so they cannot be infinitely zoomed. They lose quality at high magnification. Consider at least a dpi of 300 when saving .pngs.\nVector graphics are also editable using a program like Inkscape. Often, you may need to add some custom color or annotations using an illustrator program. Vector graphics allow you to do this.",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "Effective data visualizations",
    "section": "",
    "text": "Plotting with ggplot2\nData visualization is one of the most important skills to develop as a data scientist. We use graphs, instead of tables for instance, to clearly communicate patterns, trends, and comparisons in data in a way that is inherently more interesting and informative than numbers in a grid.\nEdward Tufte’s, “The Visual Display of Quantitative Information”, is probably the most famous text in all of data visualization (I’m guessing). In this text, Tufte outlines a theory of graphics and provides extensive detail regarding techniques for displaying data that maximizes clarity, precision, and efficiency. Tufte describes graphical excellence by the following points:\nThese points describe how to make good visualizations. However, some visualizations are clearly better than others. Take a look at this presentation by Andrew Gelmen and Antony Unwin. Here, they describe what makes some visualizations effective vs. what makes others ineffective and distracting. I think the main conclusion comes down to design vs decoration. Effective data visualizations are intentionally designed to communicate a point in the clearest possible way. Ineffective visualizations often contain clutter in the form of decorations, unnecessary colors, patterns, lines, and unintuitive use of shapes and sizes.\nOutside of the design choices that go into creating effective visualizations, there are also other expectations required of graphs. The data points should be labelled so that the viewer immediately knows what they’re looking at. The axes should be labelled in a way that’s easy to read and do not distort the message. If using colors, the color palette should augment the visualization in a way that enhances the display of information and not only ‘looks pretty’.\nThere are probably thousands (millions?, infinite?) of types of visualizations in use today. However, in scientific communication, there are basically 5 types of graphs that are most commonly used; line graphs, bar graphs, histograms, scatter plots, and pie charts. Most of these have persisted in the literature (with the exception of pie charts) because of their ability to clearly and quickly display visual information about quantitative data. Many other forms of these basic charts exist primarily as variations on a theme but the core display of information remains the same.\nWe’ll use the ggplot2 R package to start creating effective visualizations. ggplot2 works in a way that can feel a little strange at first, especially if you’re used to creating plots in Python with matplotlib, for example. ggplot2 implements ideas from Leland Wilkinson’s, “Grammar of Graphics”. ggplot2, breaks down a graphic into several key components that can be combined in a layered fashion. These core components typically include:\nThe power of this approach lies in its declarative nature. You specify what you want to plot by defining these components, rather than detailing how to draw each element step-by-step (like in matplotlib). This makes it easier to build complex visualizations and to switch between different representations of the same data with minimal changes to the code.",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#general-design-guidelines-for-plots",
    "href": "data-viz.html#general-design-guidelines-for-plots",
    "title": "Effective data visualizations",
    "section": "General design guidelines for plots",
    "text": "General design guidelines for plots\n\nDon’t use a special color for the background color. Use the same color as your presentation background. Generally, either white or black backgrounds are best\nAxes colors should be high contrast relative to the background. This means black axes on white backgrounds and white axes on black backgrounds.\nUse tick marks that have whole-number intervals. e.g. don’t use axis ticks that are 1.33, 2.75, 3.38, …. Instead, use numbers that people use naturally when counting, e.g. 1, 2, 3 or 2, 4, 6, etc.\nAxes need to be legible. The default font size is often too small. At least 12 point font.\nAxes labels should clearly communicate what is being plotted on each axis. Always show units.\nIf showing a plot title, use the title to tell the user what the conclusion is, not simply describing the data on each axis.\nIf using multiple colors, they should contrast well with each other. Warm colors (reds, yellows) can be used to emphasize selected data.\nKeep colors consistent across graphs. If you use the color “red” to represent treated samples in one plot, use the color “red” in all other plots where the treated samples are present.\nUse colors to communicate. Hues are not emotionally neutral. A good color palette can affect the audience’s perception of the figure (just like a bad one can).\nOnly use gridlines if you need them, most of the time they’re not needed.\nIf using gridlines, be judicious. Their color should be subtle and not obscure the data. Only use grids that matter for the data.\nChoose fonts that are easy to read. sans serif fonts are best. Helvetica or Arial are good default choices\nWhen drawing error bars, don’t make them so wide/large as to obscure the data",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#a-basic-ggplot",
    "href": "data-viz.html#a-basic-ggplot",
    "title": "Effective data visualizations",
    "section": "A basic ggplot",
    "text": "A basic ggplot\nggplot() constructs a plot from data, what it calls aesthetic mappings, and layers. Aesthetic mappings describe how variables in the data are mapped to visual properties (aesthetics) of geoms. geoms then determine how the data is displayed. The other parts of the ggplot object have been handled automatically (i.e. scales, stats, coordinates, and theme). These, however, can be modified to enhance the plot. Check out the ggplot2 homepage for an overview or the ggplot2 book for details.\nThe code below demonstrates the most basic way of creating a plot with ggplot2.\n\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis plot is okay but can be improved. Let’s improve this plot by removing the grey background and gridlines, increasing the font size of the axis ticks, improving the axis labels, and creating an informative title.\n\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg)) +\n  geom_point(size = 2) +        # increase the size of the points\n  labs(                         # labs() can be used to modify axis label text\n    title = \"Fuel Efficiency Decreases with Increasing Weight\",    \n    x  = \"Weight (1000 lbs)\",\n    y = \"Fuel Efficiency (mpg)\") +\n  theme_classic() +             # Removes grey background and gridlines\n  theme(                        # Adjust the plot and axes titles, and text\n    plot.title = element_text(size = 18, face = \"bold\", color = \"black\"),\n    axis.title = element_text(size = 14, color = \"black\"),\n    axis.text = element_text(size = 12, color = \"black\")\n  )\n\n\n\n\n\n\n\n\n\nSaving themes\nIf you’re going to be using the same theme elements often, it can be helpful to save those as a new custom theme. You don’t have to understand exactly how this function works right now - simply modify the arguments to the theme() function and figure this out as you improve\n\ntheme_clean &lt;- function(...) {\n  ggplot2::theme_classic(...) %+replace%\n  ggplot2::theme(\n      text = ggplot2::element_text(family = \"Helvetica\"),\n      plot.title = ggplot2::element_text(size = 18, face = \"bold\", color = \"black\", hjust = 0),\n      axis.title = ggplot2::element_text(size = 14, color = \"black\"),\n      axis.text = ggplot2::element_text(size = 12, color = \"black\")\n    )\n}\n\n# The new theme can be applied to other plots\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg)) +\n  geom_point() +        \n  theme_clean()\n\n\n\n\n\n\n\n\nThis plot is contains the same information but more quickly and clearly communicates the message by just modifying the design. What other element could be added to this plot to make the trend more apparent?",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "data-viz.html#saving-images",
    "href": "data-viz.html#saving-images",
    "title": "Effective data visualizations",
    "section": "Saving images",
    "text": "Saving images\nUsing ggplot2 you can save images with the ggsave() function. ggsave() can automatically detect the image format by the file extension. The ggsave() function works by saving the last plot created.\n\nggplot(data, aes(x, y)) + \n  geom_point()\n\nggsave(\"my-pretty-plot.pdf\", width = 8, height = 6)\n\nIf using base R, you typically open the graphics device (png(), pdf(), jpeg(), etc.) first, depending on the file format you want to save, then create the plot, and close the plot device.\n\npdf(\"another-pretty-plot.pdf\", width = 8, height = 6)\nplot(x, y)\ndev.off()",
    "crumbs": [
      "Effective data visualizations"
    ]
  },
  {
    "objectID": "intro-to-bioinfo.html",
    "href": "intro-to-bioinfo.html",
    "title": "Introduction to Bioinformatics",
    "section": "",
    "text": "Downloading FASTQ files from SRA\nWelcome! In this tutorial, we’ll walk through the common steps of an RNA‑seq analysis pipeline.\nFirst, we need to obtain the raw sequencing reads (FASTQ files) from the Sequence Read Archive (SRA). We’ll use prefetch to download data and fasterq-dump to extract FASTQ files.",
    "crumbs": [
      "Introduction to Bioinformatics"
    ]
  },
  {
    "objectID": "intro-to-bioinfo.html#downloading-fastq-files-from-sra",
    "href": "intro-to-bioinfo.html#downloading-fastq-files-from-sra",
    "title": "Introduction to Bioinformatics",
    "section": "",
    "text": "#!/usr/bin/env bash\n#\n# Obtain fastq files from SRA using prefetch + fasterq-dump\n#\n# ----------------------------------------------\nSAMPLES=sra-names.txt # A text file listing SRR ID's from SRA Run Selector\nOUT=/path/to/put/00_fastq\n\nmkdir -p $OUT\n\nfor SAMPLE in $(cat $SAMPLES)\ndo\n    prefetch $SAMPLE\n    fasterq-dump $SAMPLE\ndone",
    "crumbs": [
      "Introduction to Bioinformatics"
    ]
  },
  {
    "objectID": "intro-to-bioinfo.html#downloading-metadata-for-the-fastq-files-from-sra",
    "href": "intro-to-bioinfo.html#downloading-metadata-for-the-fastq-files-from-sra",
    "title": "Introduction to Bioinformatics",
    "section": "Downloading metadata for the FASTQ files from SRA",
    "text": "Downloading metadata for the FASTQ files from SRA\nNext, it’s important to gather metadata (like sample descriptions) associated with our sequencing runs. We’ll query the SRA database by project ID and save the run information.\n#!/usr/bin/env bash\n#\n# Obtain metadata for fastq files from SRA using esearch + efetch\n#\n# ----------------------------------------------\nPRJ=PRJNA229998 # SRA project ID from SRA Run Selector\nOUT=/docs/metadata\n\nmkdir -p $OUT\n\nesearch -db sra -query $PRJ | efetch -format runinfo &gt; ${OUT}/RunInfo.csv",
    "crumbs": [
      "Introduction to Bioinformatics"
    ]
  },
  {
    "objectID": "intro-to-bioinfo.html#trimming-and-filtering-reads",
    "href": "intro-to-bioinfo.html#trimming-and-filtering-reads",
    "title": "Introduction to Bioinformatics",
    "section": "Trimming and filtering reads",
    "text": "Trimming and filtering reads\nWe should remove adapters and low‑quality bases using fastp. This improves downstream alignment.\n#!/usr/bin/env bash\n#\n# Run fastp on the raw fastq files\n#\n# ----------------------------------------------------------------------------\nset -Eeou pipefail\n\nFQ=/path/to/00_fastq       # Directory containing raw fastq files\nSAMPLES=sample-names.txt   # A text file listing basenames of fastq files\nOUT=/path/to/put/01_fastp  # Where to save the fastp results\nTHREADS=8\n\nmkdir -p $OUT\n\nfor SAMPLE in $(cat $SAMPLES)\ndo\n    fastp -i $FQ/${SAMPLE}_R1.fq.gz \\\n          -I $FQ/${SAMPLE}_R2.fq.gz \\\n          -o $OUT/${SAMPLE}.trimmed.1.fq.gz \\\n          -O $OUT/${SAMPLE}.trimmed.2.fq.gz \\\n          -h $OUT/${SAMPLE}.fastp.html \\\n          -j $OUT/${SAMPLE}.fastp.json \\\n          -w $THREADS\ndone",
    "crumbs": [
      "Introduction to Bioinformatics"
    ]
  },
  {
    "objectID": "intro-to-bioinfo.html#aligning-reads",
    "href": "intro-to-bioinfo.html#aligning-reads",
    "title": "Introduction to Bioinformatics",
    "section": "Aligning reads",
    "text": "Aligning reads\nWe’ll use STAR to perform splice-aware alignment, which is ideal for RNA‑seq data.\n#!/usr/bin/env bash\n#\n# Align reads with STAR\n#\n# ----------------------------------------------------------------------------\nset -Eeou pipefail\n\nSAMPLES=sample-names.txt   # Same sample-names.txt file as above\nFQ=/path/to/01_fastp       # Directory containing the fastp output\nOUT=/path/to/03_STAR_outs  # Where to save the STAR results\nIDX=/path/to/STAR-idx      # Index used by STAR for alignment\nTHREADS=24\n\nmkdir -p $OUT\n\nfor SAMPLE in $(cat $SAMPLES)\ndo\n  STAR --runThreadN $THREADS \\\n       --genomeDir $IDX \\\n       --readFilesIn ${FQ}/${SAMPLE}.trimmed.1.fq.gz ${FQ}/${SAMPLE}.trimmed.2.fq.gz \\\n       --readFilesCommand zcat \\\n       --outFilterType BySJout \\\n       --outFileNamePrefix ${OUT}/${SAMPLE}_ \\\n       --alignSJoverhangMin 8 \\\n       --alignSJDBoverhangMin 1 \\\n       --outFilterMismatchNmax 999 \\\n       --outFilterMismatchNoverReadLmax 0.04 \\\n       --alignIntronMin 20 \\\n       --alignIntronMax 1000000 \\\n       --alignMatesGapMax 1000000 \\\n       --outMultimapperOrder Random \\\n       --outSAMtype BAM SortedByCoordinate \\\n       --quantMode - # Do not perform any quantifications\ndone",
    "crumbs": [
      "Introduction to Bioinformatics"
    ]
  },
  {
    "objectID": "intro-to-bioinfo.html#counting-reads",
    "href": "intro-to-bioinfo.html#counting-reads",
    "title": "Introduction to Bioinformatics",
    "section": "Counting reads",
    "text": "Counting reads\nWe’ll use summarizeOverlaps in R to count reads overlapping exonic features.\n\nlibrary(\"GenomicFeatures\")\n\n# Load in the genome txdb\ngtf_file &lt;- \"/path/to/gtf_file.gtf.gz\"\ntxdb &lt;- makeTxDbFromGFF(gtf_file, format=\"gtf\")\nexonsByGene &lt;- exonsBy(txdb, by=\"gene\", use.names = FALSE)\n\n# Read in sra metadata and save it to a DataFrame called sampleTable\nsampleTable &lt;- DataFrame(read.csv(\"./doc/metadata/RunInfo.csv\"))\nrownames(sampleTable) &lt;- sampleTable$Run\n\n# Load in alignment files\nfls &lt;- fls &lt;- list.files(\n  \"/path/to/02_STAR_outs\",\n  full.names = TRUE,\n  pattern = \"*.bam$\"\n)\n\nlibrary(\"Rsamtools\")\nbam_fls &lt;- BamFileList(fls)\n\n# Extract sample IDs from file names\nnames(fls) &lt;- stringr::str_match(\n  fls,\n  \"SRR[0-9]+\"\n)[,1]\n\n# Only keep observations that you have in your experiment\nsampleTable &lt;- sampleTable[names(fls),]\n\n# Quantify reads using GenomicAlignments package\nlibrary(\"BiocParallel\")\nregister(MulticoreParam(workers=8))\nlibrary(\"GenomicAlignments\")\n\nse &lt;- summarizeOverlaps(\n    features = exonsByGene,\n    reads = bam_fls,\n    mod = \"Union\",\n    singleEnd = FALSE,\n    ignore.strand = TRUE,\n    fragments = TRUE\n)\n\ncolData(se) &lt;- DataFrame(sampleTable)\n\n# Save your output\ndir.create(\"/path/to/03_RNAseq_counts\", showWarnings = FALSE)\nsaveRDS(se, file = \"/path/to/03_RNAseq_counts/se.rds\")",
    "crumbs": [
      "Introduction to Bioinformatics"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html",
    "href": "differential-expression-analysis.html",
    "title": "Differential Expression Analysis",
    "section": "",
    "text": "Library QC\nWelcome! In this tutorial, we’ll walk through the common steps of differential expression analysis.\nBefore we perform differential expression analysis it is important to explore the samples’ library distributions in order to ensure good quality before downstream analysis. There are several diagnostic plots we can use for this purpose implemented in the coriell package. However, first we must remove any features that have too low of counts for meaningful differential expression analysis. This can be achieved using edgeR::filterByExpr().\nIf you’d like to use your data from the previous module please follow accordingly\nlibrary(edgeR)\n\n# Load in SummarizedExperiment from previous module\nse &lt;- readRDS(\"/path/to/03_RNAseq_counts/se.rds\")\n\n# Convert to a DGEList to be consistent with above steps\ny &lt;- SE2DGEList(airway)\nFor this module I will be using the airway dataset\n# Load the SummarizedExperiment object\ndata(airway)\n\n# Set the group levels\nairway$group &lt;- factor(airway$dex, levels = c(\"untrt\", \"trt\"))\n\n# Convert to a DGEList to be consistent with above steps\ny &lt;- SE2DGEList(airway)\n\n# Determine which genes have enough counts to keep around\nkeep &lt;- filterByExpr(y)\n\n# Remove the unexpressed genes\ny &lt;- y[keep,,keep.lib.sizes = FALSE]\nAt this stage it is often wise to perform library QC on the library size normalized counts. This will give us an idea about potential global expression differences and potential outliers before introducing normalization factors. We can use edgeR to generate log2 counts-per-million values for the retained genes.\nlogcounts &lt;- cpm(y, log = TRUE)",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#relative-log-expression-boxplots",
    "href": "differential-expression-analysis.html#relative-log-expression-boxplots",
    "title": "Differential Expression Analysis",
    "section": "Relative log expression boxplots",
    "text": "Relative log expression boxplots\nThe first diagnostic plot we can look at is a plot of the relative log expression values. RLE plots are good diagnostic tools for evaluating unwanted variation in libraries.\n\nlibrary(ggplot2)\nlibrary(coriell)\n\n\nplot_boxplot(logcounts, metadata = y$samples, fillBy = \"group\",\n             rle = TRUE, outliers = FALSE) +\n  labs(title = \"Relative Log Expression\",\n       x = NULL,\n       y = \"RLE\",\n       color = \"Treatment Group\")",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#library-density-plots",
    "href": "differential-expression-analysis.html#library-density-plots",
    "title": "Differential Expression Analysis",
    "section": "Library density plots",
    "text": "Library density plots\nLibrary density plots show the density of reads corresponding to a particular magnitude of counts. Shifts of these curves should align with group differences and generally samples from the same group should have overlapping density curves\n\nplot_density(logcounts, metadata = y$samples, colBy = \"group\") +\n  labs(title = \"Library Densities\",\n       x = \"logCPM\",\n       y = \"Density\",\n       color = \"Treatment Group\")",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#sample-vs-sample-distances",
    "href": "differential-expression-analysis.html#sample-vs-sample-distances",
    "title": "Differential Expression Analysis",
    "section": "Sample vs Sample Distances",
    "text": "Sample vs Sample Distances\nWe can also calculate the euclidean distance between all pairs of samples and display this on a heatmap. Again, samples from the same group should show smaller distances than sample pairs from differing groups.\n\nplot_dist(logcounts, metadata = y$samples[, \"group\", drop = FALSE])",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#parallel-coordinates-plot",
    "href": "differential-expression-analysis.html#parallel-coordinates-plot",
    "title": "Differential Expression Analysis",
    "section": "Parallel coordinates plot",
    "text": "Parallel coordinates plot\nParallel coordinates plots are useful for giving you an idea of how the most variable genes change between treatment groups. These plots show the expression of each gene as a line on the y-axis traced between samples on the x-axis.\n\nplot_parallel(logcounts, y$samples, colBy = \"group\", \n              removeVar = 0.9, alpha = 0.05) +\n  labs(title = \"10% Most Variable Genes\",\n       x = \"Sample\",\n       y = \"logCPM\",\n       color = \"Treatment Group\")",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#correlations-between-samples",
    "href": "differential-expression-analysis.html#correlations-between-samples",
    "title": "Differential Expression Analysis",
    "section": "Correlations between samples",
    "text": "Correlations between samples\nWe can also plot the pairwise correlations between all samples. These plots can be useful for identifying technical replicates that deviate from the group\n\nplot_cor_pairs(logcounts, cex_labels = 1)",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#pca",
    "href": "differential-expression-analysis.html#pca",
    "title": "Differential Expression Analysis",
    "section": "PCA",
    "text": "PCA\nPrincipal components analysis is an unsupervised method for reducing the dimensionality of a dataset while maintaining its fundamental structure. PCA biplots can be used to examine sample groupings following PCA. These biplots can reveal overall patterns of expression as well as potential problematic samples prior to downstream analysis. For simple analyses we expect to see the ‘main’ effect primarily along the first component.\nI like to use the PCAtools package for quickly computing and plotting principal components. For more complicated experiments I have also found UMAP (see coriell::UMAP()) to be useful for dimensionality reduction (although using UMAP is not without its problems for biologists).\n\nlibrary(PCAtools)\n\n\n# Perform PCA on the 20% most variable genes\n# Center and scale the variable after selecting most variable\npca_result &lt;- pca(\n  logcounts, \n  metadata = y$samples, \n  center = TRUE, \n  scale = TRUE, \n  removeVar = 0.8\n  )\n\n# Show the PCA biplot\nbiplot(\n  pca_result, \n  colby = \"group\", \n  hline = 0, \n  vline = 0, \n  hlineType = 2, \n  vlineType = 2, \n  legendPosition = \"bottom\",\n  title = \"PCA\",\n  caption = \"20% Most Variable Features\"\n  )",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#assessing-global-scaling-normalization-assumptions",
    "href": "differential-expression-analysis.html#assessing-global-scaling-normalization-assumptions",
    "title": "Differential Expression Analysis",
    "section": "Assessing global scaling normalization assumptions",
    "text": "Assessing global scaling normalization assumptions\nMost downstream differential expression testing methods apply a global scaling normalization factor to each library prior to DE testing. Applying these normalization factors when there are global expression differences can lead to spurious results. In typical experiments this is usually not a problem but when dealing with cancer or epigenetic drug treatment this can actually lead to many problems if not identified.\nIn order to identify potential violations of global scaling normalization I use the quantro R package. quantro uses two data driven approaches to assess the appropriateness of global scaling normalization. The first involves testing if the medians of the distributions differ between groups. These differences could indicate technical or real biological variation. The second test assesses the ratio of between group variability to within group variability using a permutation test similar to an ANOVA. If this value is large, it suggests global adjustment methods might not be appropriate.\n\nlibrary(quantro)\n\n\n# Initialize multiple (8) cores for permutation testing\ndoParallel::registerDoParallel(cores = 8)\n\n# Compute the qstat on the filtered libraries\nqtest &lt;- quantro(y$counts, groupFactor = y$samples$group, B = 500)\n\nNow we can assess the results. We can use anova() to test for differences in medians across groups. Here, they do not significantly differ.\n\nanova(qtest)\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: objectMedians\n#&gt;             Df  Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; groupFactor  1  1984.5  1984.5  0.3813 0.5596\n#&gt; Residuals    6 31225.5  5204.3\n\nWe can also plot the results of the permutation test to see the between:within group ratios. Again, there are no large differences in this dataset suggesting that global scaling normalization such as TMM is appropriate.\n\nquantroPlot(qtest)",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#differential-expression-testing-with-edger",
    "href": "differential-expression-analysis.html#differential-expression-testing-with-edger",
    "title": "Differential Expression Analysis",
    "section": "Differential expression testing with edgeR",
    "text": "Differential expression testing with edgeR\nAfter removing lowly expressed features and checking the assumptions of normalization we can perform downstream differential expression testing with edgeR. The edgeR manual contains a detailed explanation of all steps involved in differential expression testing.\nIn short, we need to specify the experimental design, estimate normalization factors, fit the models, and perform DE testing.",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#creating-the-experimental-design",
    "href": "differential-expression-analysis.html#creating-the-experimental-design",
    "title": "Differential Expression Analysis",
    "section": "Creating the experimental design",
    "text": "Creating the experimental design\nMaybe the most important step in DE analysis is properly constructing a design matrix. The details of design matrices are outside of the scope of this tutorial but a good overview can be found here. Generally, your samples will fall nicely into several well defined groups, facilitating the use of a design matrix without an intercept e.g. design ~ model.matrix(~0 + group, …). This kind of design matrix makes it relatively simple to construct contrasts that describe exactly what pairs of groups you want to compare.\nSince this example experiment is simply comparing treatments to control samples we can model the differences in means by using a model with an intercept where the intercept is the mean of the control samples and the 2nd coefficient represents the differences in the treatment group.\n\n# Model with intercept\ndesign &lt;- model.matrix(~group, data = y$samples)\n\nWe can make an equivalent model and test without an intercept like so:\n\n# A means model\ndesign_no_intercept &lt;- model.matrix(~0 + group, data = y$samples)\n\n# Construct contrasts to test the difference in means between the groups\ncm &lt;- makeContrasts(\n  Treatment_vs_Control = grouptrt - groupuntrt,\n  levels = design_no_intercept\n)\n\nThe choice of which design is up to you. I typically use whatever is clearer for the experiment at hand. In this case, that is the model with an intercept.",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#estimating-normalization-factors",
    "href": "differential-expression-analysis.html#estimating-normalization-factors",
    "title": "Differential Expression Analysis",
    "section": "Estimating normalization factors",
    "text": "Estimating normalization factors\nWe use edgeR to calculate trimmed mean of the M-value (TMM) normalization factors for each library.\n\n# Estimate TMM normalization factors\ny &lt;- normLibSizes(y)\n\nWe can check the normalization by creating MA plots for each library. The bulk of the data should be centered on zero without any obvious differences in the logFC as a function of average abundance.\n\npar(mfrow = c(2, 4))\nfor (i in 1:ncol(y)) {\n  plotMD(cpm(y, log = TRUE), column = i)\n  abline(h = 0, lty = 2, col = \"red2\")\n}",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#what-to-do-if-global-scaling-normalization-is-violated",
    "href": "differential-expression-analysis.html#what-to-do-if-global-scaling-normalization-is-violated",
    "title": "Differential Expression Analysis",
    "section": "What to do if global scaling normalization is violated?",
    "text": "What to do if global scaling normalization is violated?\nAbove I described testing for violations of global scaling normalization. So what should we do if these assumptions are violated and we don’t have a good set of control genes or spike-ins etc.?\nIf we believe that the differences we are observing are due to true biological phenomena (this is a big assumption) then we can try to apply a method such as smooth quantile normalization to the data using the qsmooth package.\nBelow I will show how to apply qsmooth to our filtered counts and then calculate offsets to be used in downstream DE analysis with edgeR. Please note this is not a benchmarked or ‘official’ workflow just a method that I have implemented based on reading forums and github issues.\n\nlibrary(qsmooth)\n\n\n# Compute the smooth quantile factors \nqs &lt;- qsmooth(y$counts, group_factor = y$samples$group)\n\n# Extract the qsmooth transformed data\nqsd &lt;- qsmoothData(qs)\n\n# Calculate offsets to be used by edgeR in place of norm.factors\n# Offsets are on the natural log scale. Add a small offset to avoid\n# taking logs of zero \noffset &lt;- log(y$counts + 0.1) - log(qsd + 0.1)\n\n# Scale the offsets for internal usage by the DGEList object\n# Now the object is ready for downstream analysis\ny &lt;- scaleOffset(y, offset = offset)\n\n# To create logCPM values with the new norm factors use\nlcpm &lt;- cpm(y, offset = y$offset, log = TRUE)",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#fit-the-model",
    "href": "differential-expression-analysis.html#fit-the-model",
    "title": "Differential Expression Analysis",
    "section": "Fit the model",
    "text": "Fit the model\nNew in edgeR 4.0 is the ability to estimate dispersions while performing the model fitting step. I typically tend to ‘robustify’ the fit to outliers. Below I will perform dispersion estimation in legacy mode so that we can use competitive gene set testing later. If we want to use the new workflow we can use the following:\n\n# edgeR 4.0 workflow\nfit &lt;- glmQLFit(y, design, legacy = FALSE, robust = TRUE)",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#test-for-differential-expression",
    "href": "differential-expression-analysis.html#test-for-differential-expression",
    "title": "Differential Expression Analysis",
    "section": "Test for differential expression",
    "text": "Test for differential expression\nNow that the models have been fit we can test for differential expression.\n\n# Test the treatment vs control condition\nqlf &lt;- glmQLFTest(fit, coef = 2)\n\nOften it is more biologically relevant to give more weight to higher fold changes. This can be achieved using glmTreat(). NOTE do not use glmQLFTest() and then filter by fold-change - you destroy the FDR correction!\nWhen testing against a fold-change we can use relatively modest values since the fold-change must exceed this threshold before being considered for significance. Values such as log2(1.2) or log2(1.5) work well in practice.\n\ntrt_vs_control_fc &lt;- glmTreat(fit, coef = 2, lfc = log2(1.2))\n\nIn any case, the results of the differential expression test can be extracted to a data.frame for downstream plotting with coriell::edger_to_df(). This function simply returns a data.frame of all results from the differential expression object in the same order as y. (i.e. topTags(…, n=Inf, sort.by=“none”))\n\nde_result &lt;- edger_to_df(qlf)",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  },
  {
    "objectID": "differential-expression-analysis.html#plotting-de-results",
    "href": "differential-expression-analysis.html#plotting-de-results",
    "title": "Differential Expression Analysis",
    "section": "Plotting DE results",
    "text": "Plotting DE results\nThe two most common plots for differential expression analysis results are the volcano plot and the MA plot. Volcano plots display the negative log10 of the significance value on the y-axis vs the log2 fold-change on the x-axis. MA plots show the average expression of the gene on the x-axis vs the log2 fold-change of the gene on the y-axis. The coriell package includes functions for producing both.\n\nlibrary(patchwork)\n\n\n# Create a volcano plot of the results\nv &lt;- plot_volcano(de_result, fdr = 0.05) \n\n# Create and MA plot of the results\nm &lt;- plot_md(de_result, fdr = 0.05) \n\n# Patch both plots together\n(v | m) + \n  plot_annotation(title = \"Treatment vs. Control\") &\n  theme_coriell()\n\n\nThe coriell package also has a function for quickly producing heatmaps with nice defaults for RNA-seq. Sometimes it’s useful to show the heatmaps of the DE genes.\n\n# Compute logCPM values after normalization\nlcpm &lt;- cpm(y, log = TRUE)\n\n# Determine which of the genes in the result were differentially expressed\nis_de &lt;- de_result$FDR &lt; 0.05\n\n# Produce a heatmap from the DE genes\nquickmap(\n  x = lcpm[is_de, ], \n  metadata = y$samples[, \"group\", drop = FALSE],\n  main = \"Differentially Expressed Genes\"\n  )",
    "crumbs": [
      "Differential Expression Analysis"
    ]
  }
]